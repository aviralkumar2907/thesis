\documentclass[../thesis.tex]{subfiles}

\usepackage{wrapfig}
\usepackage{cite}
% \usepackage{natbib}
% \usepackage[round]{natbib}

\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usetikzlibrary{positioning}
\usepackage{mathtools}
\usepackage{fleqn, tabularx}
\usepackage{multirow}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
% \newtheorem{corollary}{Corollary}[proposition]
\newtheorem{definition1}{Definition}[section]



\begin{document}

% \blfootnote{This chapter is based on \cite{fu2019diagnosing}, published at ICML 2019, which is joint work with Justin Fu, Matthew Soh, and Sergey Levine.}

% \input{chapters/bear/text/abstract.tex}

% \input{chapters/bear/text/introduction.tex}
% \input{chapters/bear/text/related.tex}
% \input{chapters/bear/text/background.tex}
% \input{chapters/bear/text/method.tex}
% \input{chapters/bear/text/experiments.tex}
% \input{chapters/bear/text/conclusions.tex}

\vspace{-0.4cm}
\begin{AIbox}{\large{\textbf{Abstract}}}
\vspace{4mm}
In this chapter, we utilize offline RL to the problem of designing hardware accelerators, i.e., chips that are tailored towards improving the efficiency of running certain software applications. For instance, TPU accelerators aim to optimize the performance of machine learning models. While a paradigm shift towards specializing hardware is already starting to show promising results, designers still need to spend considerable manual effort and perform large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, a designer would need to typically start from scratch every time the set of target applications or design constraints change in such a ``simulation-driven'' workflow. An alternative paradigm is to use an offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing active simulations. This is different from supervised learning: instead of predicting a performance metric associated with an accelerator, we wish to find a new accelerator that attains  near-optimal metrics, much in the same way where we wish to find a near-optimal policy in offline RL. In this problem, our offline RL based approach, dubbed \primemethodname, not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes.
% Our approach learns a conservative estimate of the desired reward function, utilizes infeasible points and optimizes the design against this estimate without any additional simulator queries during optimization.
\primemethodname\ architects accelerators for both single and multiple applications, improving performance upon state-of-the-art simulation-driven methods by about 1.54$\times$ and 1.20$\times$, while reducing the total simulation time by 93\% and 99\%, respectively.
% In addition, \primemethodname\ architects accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26$\times$.
\vspace{2mm}
\end{AIbox}


\input{chapters/prime/body/01_intro}
\input{chapters/prime/body/02_background}
\input{chapters/prime/body/03_problem}
\input{chapters/prime/body/04_method}
\input{chapters/prime/body/05_related}
\input{chapters/prime/body/06_ablation}
\input{chapters/prime/body/07_discussion}


\end{document}