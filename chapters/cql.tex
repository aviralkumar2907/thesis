\documentclass[../thesis.tex]{subfiles}

\usepackage{wrapfig}
\usepackage{cite}
% \usepackage{natbib}
% \usepackage[round]{natbib}

\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usetikzlibrary{positioning}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
% \newtheorem{corollary}{Corollary}[proposition]
\newtheorem{definition1}{Definition}[section]



% \include{chapters/cql/defs}


\begin{document}


% \blfootnote{This chapter is based on \cite{kumar2020conservative}, published at NeurIPS 2020, which is joint work with Aurick Zhou, George Tucker, and Sergey Levine.}

\vspace{-0.4cm}
\begin{AIbox}{\large{\textbf{Abstract}}}
\vspace{4mm}
In Chapters~\ref{chapter:diagnosing} and \ref{chapter:bear}, we discussed how standard off-policy RL methods can fail in offline RL and how restricting action selection can provide for a first approach to tackle distributional shift in offline RL. While this approach does attain promising empirical performance, methods based on this principle tend to be restrictive, constraining the learned policy to the behavioral policy even when the Q-function on out-of-distribution actions are not erroneously large to inhibit effective policy learning.                 
% Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. 
In this chapter, we present a paradigm called \emph{conservative value function estimation}, which aims to address these limitations by learning a value-function such that the expected value of a policy under this value-function lower-bounds its true value. We discuss two concrete variants of this paradigm: a model-free variant, called conservative Q-learning (CQL)~\citep{kumar2020conservative}, and a model-based variant, called conservative offline model-based policy optimization (COMBO)~\citep{yu2021conservative}. Both of these variants can be implemented by augmenting the standard temporal-difference error objective with a Q-value regularizer, on top of model-based and model-free Q-learning and actor-critic algorithms. We will also theoretically show that this paradigm produces conservative value functions, that lower bound the ground-truth value of the current policy, and policies, that enjoy safe policy improvement.
\vspace{2mm}
% CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with safe improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of standard Q-learning and actor-critic implementations. 
% On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.
\end{AIbox}

% \blfootnote{Our follow-up on CQL for vision-based robotic manipulation can be found here: \url{https://arxiv.org/abs/2010.14500}.}

\vspace{-0.2cm}
\section{Model-Free Conservative Value Function Estimation}
\label{sec:cql_section}
\vspace{-0.2cm}

\input{chapters/cql/introduction.tex}
% \input{chapters/cql/background.tex}
\input{chapters/cql/method_new.tex}
\input{chapters/cql/method_part2}
\input{chapters/cql/related.tex}
\input{chapters/cql/experiments_shortened}

\vspace{-0.2cm}
\section{Model-Based Conservative Value Function Estimation}
\label{sec:combo_section}
\vspace{-0.2cm}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/combo/teaser_combo.png}
    \vspace*{-0.5cm}
    \caption{\footnotesize{COMBO learns a conservative value function by utilizing both the offline dataset as well as simulated data from the model. Crucially, COMBO does not require uncertainty quantification, and the value function learned by COMBO is a tighter lower-bound of the true value compared to CQL. This enables COMBO to steer the agent towards higher value states compared to CQL, which may steer towards sub-optimal states as illustrated in the figure.}}
    \vspace*{-0.6cm}
    \label{fig:combo_teaser}
\end{figure}

In Section~\ref{sec:cql_section}, we discussed conservative Q-learning, a model-free algorithm that learns conservative value functions on offline datasets. An alternative way to use offline data is to first attempt to estimate the dynamics of the underlying task, followed by training control policies via planning or explicit policy optimization. Empirically, in the standard RL problem setting, model-based RL algorithms have achieved better generalization~\citep{sutton1991dyna,janner2019mbpo}, especially in multi-task settings~\citep{yu2020mopo}. Specifically, one performant recipe for model-based RL is to utilize an estimated or learned dynamics model to generate data for a downstream model-free control pipeline, such as one based on Q-learning~\citep{janner2019mbpo}. 

Despite appealing properties of model-based RL, existing model-based methods still suffer from similar problems in the offline setting: any learned model of the environment dynamics is bound to make erroneous predictions on states or actions that are unlikely to appear within the support of the training dataset, and inevitably, downstream policy optimization will exploit these errors to produce policies that appear performant under the learned model, but fail to act reliably when deployed. To address this issue, existing offline model-based RL algorithms~\cite{kidambi2020morel, yu2020mopo} rely on precise quantifying uncertainty in predictions made by the learned dynamics model. However, as we show in Section~\ref{sec:uq}, obtaining calibrated uncertainty estimates can be extremely difficult, especially for complex datasets with deep network models.  
% A natural next step is to extend this approach to use dynamics models. By generating data, model-based reinforcement learning algorithms can achieve better generalization~\citep{sutton1991dyna,janner2019mbpo} and have demonstrated the ability to solve new tasks using the offline dataset~\cite{yu2020mopo}. 
% Most existing model-based algorithms learn a pessimistic dynamics model, which in turn induces a conservative estimate of the value function. 
% By generating data, model-based algorithms can achieve better generalization and e.g. have demonstrated the ability to solve new tasks using the offline dataset~\cite{yu2020mopo}.
% However, such algorithms rely crucially 
% Furthermore, these methods do not adapt the uncertainty estimates as the policy and value function change over the course of learning.
%%SL.1.31: The significance of this is not entirely clear (but I agree this is a true statement, and it's a nice observation, just that most readers won't get why this is important without more explanation)

To alleviate the need for calibrated uncertainty estimation, in this section, we develop a new model-based offline RL algorithm, building on conservative value estimation paradigm. Our approach, conservative offline model-based policy optimization (COMBO), retains the benefits of model-based algorithms while removing the reliance on uncertainty estimation. COMBO first learns a dynamics model using the offline dataset. Subsequently, it employs an actor-critic method where the value function is learned using both the offline dataset as well as synthetically generated data from the model. Crucially, this value function is trained via a variant of conservative optimization, that penalizes the value function on state-action tuples that are visited by the policy by simulating the learned model, but are not in the support of the offline dataset.
Using the theoretical tools from Section~\ref{sec:cql_section}, we theoretically show that for any policy, the Q-function learned with COMBO is a lower bound of the true Q-function, making it a good surrogate for policy optimization. 
We also theoretically show that the Q-function learned by COMBO represents a tighter lower bound of the true Q-function when the model bias is low compared to prior model-free algorithms like CQL~\cite{kumar2020conservative}.
Thus, as a consequence of optimizing a tighter lower bound, COMBO has the potential to learn higher rewarding policies compared to prior model-free algorithms. This is illustrated in Figure~\ref{fig:combo_teaser}.

Finally, in our experiments, we find that COMBO matches or exceeds the state-of-the-art results in commonly studied benchmark tasks for offline RL. Specifically, COMBO achieves the highest score in $9$ out of $12$ continuous control domains from the D4RL~\cite{fu2020d4rl} benchmark suite, while the next best algorithm achieves the highest score in only $3$ out of the $12$ domains. We also show that COMBO achieves the best performance in tasks that require out-of-distribution generalization and outperforms previous offline model-based RL methods on this image-based robotic manipulation task.%%CF: Need to also mention the generalization results and the vision results!
%%TY.2.3: added a sentence on the generalization and vision results. Rafael, feel free to also add the vision-based locomotion task to the sentence if we are going to include the walker results. 

%% Now adding material from COMBO paper
\input{chapters/combo/prelim}
\input{chapters/combo/method}
\input{chapters/combo/experiments}

\vspace{-0.2cm}
\section{Discussion and Limitations}
\vspace{-0.2cm}
% summary
In this chapter, we presented an algorithmic framework for estimating value functions in offline RL that lower bound the true policy value function. Optimizing the policy against such as a lower bound value function results in policies that provably improve over the behavioral policy, from the perspective of robust or safe policy improvement. Learning this sort of a conservative value function removes the need to use policy constraints, thereby alleviating the need to estimate the support of the behavioral policy in high-dimensional action spaces. Empirically, we demonstrate that our practical approaches, CQL and COMBO outperform prior offline RL methods on a wide range of offline RL benchmark tasks, including complex control tasks and tasks with raw image observations. Externally of work covered in this thesis, CQL has been utilized in several real-world applications: \textbf{(1)} for optimizing decisions in notification systems~\citep{prabhakar2022multi}, and \textbf{(2)} for identifying dead end in patient treatment in healthcare~\citep{killian2023risk}.  

Although conservative approaches have been applied in various downstream applications, there are still several unresolved questions that need to be addressed. Firstly, like any machine learning method, the performance of offline RL methods heavily relies on the values assigned to specific hyperparameters. These include the choice of the model architecture used to represent the Q-function and, more importantly, the hyperparameter $\alpha$ in CQL (and $\beta$ in COMBO), which governs the strength of the conservatism regularizer. In general, finding a universal recipe for tuning these hyperparameters is a challenging task. However, it is possible to develop strategies to select these hyperparameters under certain assumptions and conditions specific to the underlying offline RL problem. For instance, our follow-up work presents an empirical approach to tuning this hyperparameter in the context of robotic applications~\citep{kumar2021workflow}.

From a theoretical standpoint, while we show that conservative methods learn lower bounds on the Q-function in tabular, linear, and a subset of non-linear function approximation cases, a rigorous theoretical analysis of CQL with deep neural nets is yet to be conducted. As we discuss in Chapter~\ref{chapter:scaling}, the implicit regularization effects of non-linear neural net models may interact poorly with value-based RL, and understanding its consequences for conservative methods remains an open question.

% Even though conservative approaches have been utilized in several downstream applications, there are several remaining questions that need to be tackled. First, similarly to any form of machine learning, the performance of offline RL methods crucially depends on the values assigned to several hyperparameters such as the model architecture used to represent the Q-function, and, perhaps more importantly, the hyperparameter $\alpha$ in CQL (and $\beta$ in COMBO, respectively), which controls the strength of the conservatism regularizer. In general, developing a single recipe for tuning these hyperparameters is intractable in the worst case. Nonetheless, it is possible to develop strategies to enable selecting these hyperparameters under certain assumptions and conditions on the underlying offline RL problem. We present an empirical approach to tuning this hyperparameter in the context of robotic applications in Chapter ?? and in \citet{kumar2022workflow}.      

% Theoretically, while we are able to prove that conservative methods learns lower bounds on the Q-function in the tabular, linear, and a subset of non-linear function approximation cases, a rigorous theoretical analysis of CQL with deep neural nets, is left for future work. As we will discuss in Chapter ??, the implicit regularization effects of non-linear function approximators may interface poorly with value-based RL and understanding its consequences for conservative methods is an open area. 
% Additionally, offline RL methods are liable to suffer from overfitting in the same way as standard supervised methods, so another important challenge for future work is to devise simple and effective early stopping methods, analogous to validation error in supervised learning.


%%AK: fix this to include both CQL and COMBO paper people
\section*{Acknowledgements and Funding}
We thank Mohammad Norouzi, Oleh Rybkin, Anton Raichuk, Avi Singh, Vitchyr Pong and anonymous reviewers from the Robotic AI and Learning Lab at UC Berkeley and NeurIPS for their feedback on an earlier version of this paper. We thank Rishabh Agarwal for help with the Atari QR-DQN/REM codebase and for sharing baseline results. This research was funded by the DARPA Assured Autonomy program, and compute support from Google and Amazon.


\end{document}
