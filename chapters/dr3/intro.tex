\vspace{-0.2cm}
\section{Introduction}
\vspace{-0.2cm}

Despite being massively over-parameterized, deep neural networks with billions of parameters still learn representations that generalize well when trained with supervised learning. This is surprising considering the classical notions of overfitting when provided with many parameters. A widely-held consensus is that deep networks find simple solutions that generalize due to various implicit regularization effects~\citep{blanc2020implicit, woodworth2020kernel, arora2018optimization, gunasekar2017implicit, wei2019regularization, li2019towards}. Does this mean that large, over-parameterized networks will also perform well for offline RL, scaling effectively as the number of parameters is increased?

In the first part of this chapter (Section~\ref{sec:dr3_section}), we show that, the very same implicit regularization effects can lead to the emergence of poor representations when training overparameterized deep network value functions via offline RL. This often results in the inability to improve performance with larger models and, in particular, unreliable optimization behavior over the course of offline RL training. While there are already some empirical studies showing that value function training via offline temporal-difference (TD) learning leads to emergence of poor representations~\citep{kumar2021implicit}, our goal in this chapter is to understand the cause in a simpler theoretical model and develop a potential solution. Building on the theoretical framework developed by \citet{blanc2020implicit,damian2021label}, we characterize the implicit regularizer that arises when training deep value functions with offline TD learning. The form of this implicit regularizer implies that offline TD-learning would ``co-adapt'' the learned feature representations at state-action tuples that appear on either side of a Bellman backup.

Practically, we illustrate that this theoretically-predicted co-adaptation phenomenon results in a higher dot product of the features of consecutive state-action tuples learned by the Q-value network~(\Secref{app:problem_more}). Training runs that exhibit feature co-adaptation typically converge to poorly performing solutions. Even when Q-values are not overestimated, prolonged training in offline RL can result in performance degradation as feature co-adaptation increases. To mitigate this co-adaptation issue, which arises as a result of implicit regularization, we propose an \emph{explicit regularizer} that we call \drmethodname~(\Secref{sec:dr3_method}). While exactly estimating the effects of the theoretically derived implicit regularizer is computationally difficult, \drmethodname\ provides a simple and tractable approximation that mitigates the issues discussed above. In practice, \drmethodname\ amounts to regularizing the features at consecutive state-action pairs to be dissimilar in terms of their dot-product similarity. Empirically, we find that \drmethodname\ prevents previously noted pathologies such as feature rank collapse~\citep{kumar2021implicit}, gives methods that train for longer and improves performance relative to the base offline RL method on benchmark problems.

Building on this approach, in the second part of this chapter (Section~\ref{sec:scaled_section}), we perform a large-scale empirical study that aims to train a large model via offline conservative Q-learning. Specifically, we train a single policy to play 40 Atari games~\citep{bellemare2013arcade}, similar to \citet{lee2022multi}, and evaluate performance when the training dataset contains expert trajectories \emph{and} when the data is sub-optimal. This problem is especially challenging due to the diversity in dynamics, rewards, visuals, and agent embodiments across different games. Furthermore, the sub-optimal data setting requires the learning algorithm to ``stitch together'' useful segments of sub-optimal trajectories to perform well.
Our approach, \textbf{Scaled Q-learning} (Scaled QL), incorporates a variant of the \drmethodname\ technique, which normalizes features instead of using regularization to attain the same benefit without needing a hyperparameter, in conjunction with carefully chosen neural network architectural design decisions. Scaled QL can train up to 80 million parameter ResNet~\citep{resnet} models entirely via offline RL, and the performance of our approach follows a power-law relationship between model capacity and performance, similar to various scaling studies in supervised and unsupervised learning.
In terms of absolute performance, scaled Q-learning learns policies that attain more than 100\% human-level performance on most of the 40 games, about \textbf{2x} better than prior supervised learning~(SL) approaches for learning from sub-optimal offline data (51\% human-level performance).    

% Building on this approach, in the second part of this chapter (Section~\ref{sec:scaled_section}), we perform a large-scale empirical study that aims to train a large model via offline conservative Q-learning. Specifically, we train a single policy to play 40 Atari games~\citep{bellemare2013arcade}, similarly to \citet{lee2022multi}, and evaluate performance when the training dataset contains expert trajectories \emph{and} when the data is sub-optimal. This problem is especially challenging because of the diversity pertaining to dynamics, reward, visuals, and agent embodiments across different games. Furthermore, the sub-optimal data setting requires the learning algorithm to ``stitch together'' useful segments of sub-optimal trajectories to perform well. Our approach, \textbf{scaled Q-learning}, incorporates a variant of the \drmethodname\ technique which normalizes features instead of regularization to attain the same benefit without needing a hyperparameter, in conjunction with careful neural network architectural design decisions. Scaled Q-learning (Scaled QL) is able to train upto 80 million parameter ResNet~\citep{he2016resnet} models entirely via offline RL and the performance of our approach follows a power-law relationship between model capacity and performance, similar to various scaling studies in supervised and unsupervised learning. In terms of absolute performance, scaled Q-learning learns policies that attain more than 100\% human-level performance on most of the 40 games, about \textbf{2x} better than prior supervised learning~(SL) approaches for learning from sub-optimal offline data (51\% human-level performance).    

% Our first contribution is the derivation of the implicit regularizer that arises when training deep net value functions via TD learning, and an empirical demonstration that it manifests as \emph{feature co-adaptation} in the offline deep RL setting.
% %, which results in highly similar feature representations for state-action tuples at consecutive time steps. 
% Feature co-adaptation accounts at least in part for some of the challenges of offline deep RL, including degradation of performance with prolonged training. Second, we propose a simple and effective \emph{explicit} regularizer for offline value-based RL, \drmethodname, which minimizes the feature similarity between state-action pairs appearing in a bootstrapping update. \drmethodname\ is inspired by the theoretical derivation of the implicit regularizer, it alleviates co-adaptation and can be easily combined with modern offline RL methods, such as REM~\citep{agarwal2019optimistic}, CQL~\citep{kumar2020conservative}, and BRAC~\citep{wu2019behavior}. Empirically, using \drmethodname\ in conjunction with existing offline RL methods provides about \textbf{60\%} performance improvement on the harder D4RL~\citep{fu2020d4rl} tasks, and \textbf{160\%} and \textbf{25\%} stability gains for REM and CQL, respectively, on offline RL tasks in 17 Atari 2600 games. Additionally, we observe large improvements on image-based robotic manipulation tasks~\citep{singh2020cog}.
