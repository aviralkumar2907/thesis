\vspace{-0.1cm}
\section{Introduction}
\vspace{-0.1cm}
Deep neural networks are overparameterized, with billions of parameters, which in principle should leave them vulnerable to overfitting. Despite this, supervised learning with deep networks still learn representations that generalize  well. A widely held consensus is that deep nets find simple solutions that generalize due to various \emph{implicit} regularization effects~\citep{blanc2020implicit,woodworth2020kernel,arora2018optimization,gunasekar2017implicit,wei2019regularization,li2019towards}. We may surmise that using deep neural nets in reinforcement learning~(RL) will work well for the same reason, learning effective representations that generalize due to such implicit regularization effects. But is this actually the case for value functions trained via bootstrapping? 

In this paper, we argue that, while implicit regularization leads to effective representations in supervised deep learning, it may lead to poor learned representations when training overparameterized deep network value functions. 
In order to rule out confounding effects from exploration and non-stationary data distributions, we focus on the offline RL setting -- where deep value networks must be trained from a static dataset of experience.
There is already evidence that value functions trained via bootstrapping learn poor representations: value functions trained with offline deep RL eventually degrade in performance~\citep{agarwal2019optimistic, kumar2021implicit} and this degradation is correlated with the emergence of low-rank features
in the value network~\citep{kumar2021implicit}.
Our goal is to understand the underlying cause of the emergence of poor representations during bootstrapping and develop a potential solution. Building on the theoretical framework developed by \citet{blanc2020implicit,damian2021label}, we characterize the implicit regularizer that arises when training deep value functions with TD learning. The form of this implicit regularizer implies that TD-learning would co-adapt feature representations at state-action tuples that appear on either side of a Bellman backup.

We show that this theoretically predicted aliasing phenomenon manifests in practice as feature \textbf{co-adaptation}, where the features of consecutive state-action tuples learned by the Q-value network become very similar in terms of their dot product~(\Secref{sec:problem}). This co-adaptation co-occurs with oscillatory learning dynamics, and training runs that exhibit feature co-adaptation typically converge to poorly performing solutions. Even when Q-values are not overestimated, prolonged training in offline RL can result in performance degradation as feature co-adaptation increases. 
To mitigate this co-adaptation issue, which arises as a result of implicit regularization, we propose an \emph{explicit regularizer} that we call \methodname~(\Secref{sec:method}).
%%SL.9.29: This is a relatively minor thing, but when you introduce the name DR3, can you actually say what it stands for?
%%AK: I was trying to cook up a full form, but it seems like the reason why we put it this way was "DR3: Deep Reinforcement Learning Requires Explicit Regularization", but maybe this is not the best thing to call a method?
While exactly estimating and cancelling the effects of the theoretically derived implicit regularizer is computationally difficult, \methodname\ provides a simple and tractable theoretically-inspired approximation that mitigates the issues discussed above. In practice, \methodname\ amounts to regularizing the features at consecutive state-action pairs to be dissimilar in terms of their dot-product similarity. Empirically, we find that \methodname\ prevents previously noted pathologies such as feature rank collapse~\citep{kumar2021implicit},  gives methods that train for longer and improves performance relative to the base offline RL method employed in practice.
% Empirically, we find that \methodname\ allows neural network Q-functions to use their full representational capacity, as measured by the rank of the learned features, and \textcolor{red}{enables the use of larger, more expressive neural networks}, 
% %%SL.9.29: How do we test it allows them to use their full capacity? Perhaps we should remove this claim, since it doesn't seem like we have any evidence for it.
% %%AK: rank of learned features is higher? 
% %%SL.10.27: I'm still concerned that this statement may not be backed up by evidence. Do we actually show that this *allows* using larger networks (i.e., larger net + DR3 = good, but larger net - DR3 = bad?) I think we would need something like that to back up this statement
% giving rise to methods that can train for longer without degradation and thus reach a better solution~(\Secref{sec:experiments}).

Our first contribution is the derivation of the implicit regularizer that arises when training deep net value functions via TD learning, and an empirical demonstration that it manifests as \emph{feature co-adaptation} in the offline deep RL setting.
%, which results in highly similar feature representations for state-action tuples at consecutive time steps. 
Feature co-adaptation accounts at least in part for some of the challenges of offline deep RL, including degradation of performance with prolonged training. Second, we propose a simple and effective \emph{explicit} regularizer for offline value-based RL, \methodname, which minimizes the feature similarity between state-action pairs appearing in a bootstrapping update. \methodname\ is inspired by the theoretical derivation of the implicit regularizer, it alleviates co-adaptation and can be easily combined with modern offline RL methods, such as REM~\citep{agarwal2019optimistic}, CQL~\citep{kumar2020conservative}, and BRAC~\citep{wu2019behavior}. Empirically, using \methodname\ in conjunction with existing offline RL methods provides about \textbf{60\%} performance improvement on the harder D4RL~\citep{fu2020d4rl} tasks, and \textbf{160\%} and \textbf{25\%} stability gains for REM and CQL, respectively, on offline RL tasks in 17 Atari 2600 games. Additionally, we observe large improvements on image-based robotic manipulation tasks~\citep{singh2020cog}.
