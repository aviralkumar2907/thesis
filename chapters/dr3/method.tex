\vspace{-5pt}
\subsection{\drmethodname: Explicit Regularization for Deep TD-Learning}
\label{sec:method}
\vspace{-5pt}
Since the implicit regularization effects in TD-learning can lead to feature co-adaptation, which in turn is correlated with poor performance, can we instead derive an \emph{explicit} regularizer to alleviate this issue? Inspired by the analysis in the previous section, we will propose an \emph{explicit} regularizer that attempts to counteract the second term in Equation~\ref{eqn:regularizer}, which would otherwise lead to co-adaptation and poor representations.
The \emph{explicit} regularizer that offsets the difference between the two implicit regularizers is given by: $\Delta(\theta) = \sum_i \mathrm{trace}\left[\Sigma^{* \top}_M \nabla_\theta Q_\theta(\bs_i, \mathbf{a}_i) \nabla_\theta Q_\theta(\bs'_i, \mathbf{a}'_i)^\top \right]$, which represents the second term of $R_\mathrm{TD}(\theta)$. Note that we drop the stop gradient on $Q_\theta(\bs'_i, \mathbf{a}'_i)$ in $\Delta(\theta)$, as it performs slightly better in practice~(Table~\ref{tab:rem_phi_res}), although as shown in that Table, the version with the stop gradient also significantly improves over the base method.
The first term of $R_\mathrm{TD}(\theta)$ corresponds to the regularizer from supervised learning.
%%AK: rewrite R_TD(\theta) in a way that indeed the second term represents the difference
Our proposed method, DR3, simply combines approximations to $\Delta(\theta)$ with various offline RL algorithms. For any offline RL algorithm, \textsc{Alg}, with objective $\mathcal{L}_{\textsc{Alg}}(\theta)$, the training objective with \drmethodname\ is given by: $\mathcal{L}(\theta) := \mathcal{L}_{\textsc{Alg}}(\theta) + c_0 \Delta(\theta)$, where $c_0$ is the DR3 coefficient. See Appendix~\ref{app:tuning_dr3} for details on how we tune $c_0$ in this paper.

\paragraph{Practical version of DR3.} In order to practically instantiate DR3, we need to choose a particular noise model $M$. In general, it is not possible to know beforehand the ``correct'' choice of $M$ (Equation~\ref{eq:td_update}), even in supervised learning, as this is a complicated function of the data distribution, neural network architecture and initialization. Therefore, we instantiate DR3 with two heuristic choices of $M$: \textbf{(i)} $M$ induced by label noise studied in prior work for supervised learning and for which we need to run another, separate fixed-point computation for $M$,
%%SL.12.5: Could it make sense to attach a name to this variant, so that when we talk about it later in experiments we can clearly reference it?
and \textbf{(ii)} a simpler alternative that sets $\Sigma^*_M = I$. We find that both of these variants generally perform well empirically (Figure~\ref{fig:other_penalty_main}), and improve over the base offline RL method, and so we utilize \textbf{(ii)} in practice due to low computational costs. Additionally, because computing and backpropagating through per-example gradient dot products is slow, we instead approximate $\Delta(\theta)$ with the contribution only from the last layer parameters (\ie $\sum_i \nabla_\bw Q_\theta(\bs_i, \mathbf{a}_i)^\top \nabla_\bw Q_\theta(\bs'_i, \mathbf{a}'_i)$), similarly to tractable Bayesian neural nets.
As shown in Appendix~\ref{app:theory_practice_gap}, the practical version of DR3 performs similarly to the label-noise version.
% \vspace{-0.1in}
\begin{align}
\label{eqn:explicit_regularizer}
    \text{\textbf{Explicit DR3 regularizer}}:~~~~~~~~\overline{\mathcal{R}}_\mathrm{exp}(\theta) = \sum_{i \in \mathcal{D}} \phi(\bs_i, \mathbf{a}_i)^\top \phi(\bs'_i, \mathbf{a}'_i).
\end{align} 

\paragraph{Feature normalization instead of regularization.} In many problems, it may be more convenient to completely avoid the hyperparameter that weights the explicit DR3 regularizer from Equation~\ref{eqn:explicit_regularizer} and yet attain a similar boost in performance. To this end, we propose to regularize the magnitude of the learned features of the state-action pair (or state) by introducing a ``normalization'' layer in the Q-network. This layer forces the learned features to have an $\ell_2$ norm of 1 by construction. As we will discuss in the second part of this chapter, we found that this layer speeds up learning, resulting in better performance, without needing to tune the hyperparameter associated with the regularization variant of DR3.    
% This regularizer can be easily computed, by recording the last-but-one layer features of the Q-function for $(\bs_i, \mathbf{a}_i)$ and $(\bs'_i, \mathbf{a}'_i)$ via a forward pass, then computing their dot-product and adding it to the TD error objective.
%%AK: one big thing missing right now is the discussion of the stop gradient. Should we put it here, or leave it to the appendix? I am ambivalent from a technical perspective -- while on the one hand it does affect CQL results (CQL + DR3 (no stop grad) >> CQL + DR3 (stop grad) > CQL), the improvement over REM of both methods is similar (no stop grad is slightly higher still) and large. CQL is technically not covered by the analysis, as our analysis only covers TD, and so it may be fine to not discuss this detail regarding CQL here.  But then some reviewer might say we are hiding details/limitations/etc. What do you think?


% In discrete action environments, where Q-learning algorithms utilize multi-head networks to parameterize Q-functions (one head per action), we apply \drmethodname\ on the state features $\phi(\bs)$. On continuous action domains, we apply \drmethodname\ to state-action features. 
% The weighting factor, $\alpha$, is a constant across different tasks and depends on the base offline RL method. 

% It is not possible to know beforehand the noise model $M$ (Equation~\ref{eqn:covariance_noise}) for SGD on a given TD learning problem as this is is a complicated function of the data distribution, neural network architecture and initialization. Therefore, for devising a practical explicit regularizer using the principle discussed above, we need to pick a given choice of $M$ beforehand. In our experiments, we studied two choices of $M$ including the computationally heavy alternative of $M$ induced via label-noise studied in prior work~\citep{blanc2020implicit}, in which case we run explicit fixed-point iteration to determine $\Sigma^*_M$, and a computationally simpler alternative that sets $\Sigma_M^* = I$. We find that explicit DR3 regularizers obtained from either of these choices of $M$ prevent co-adaptation and lead to performance on both diagnostic and high-dimensional domains.


% \textbf{Experimental results.} We empirically validate the efficacy of our explicit regularizer, \drmethodname\ on offline RL problems on a number of benchmark domains including 17 Atari games~\citep{agarwal2019optimistic}, D4RL~\citep{fu2020d4rl} domains and robotic manipulation from images~\citep{singh2020cog}. All experimental results are presented in Appendix~\ref{sec:experiments}. Here, we briefly summarize the results: \drmethodname\ applied on top of CQL and REM improves \emph{both} stability and final performance by 25\% and 150\% respectively, and also leads to faster and more effective learning from raw visual-observations in robotic manipulation tasks. The average learning trajectory of \drmethodname\ + REM/CQL is shown in Figure~\ref{fig:atari_rem_main} below as a function of gradient steps on the x-axis (this is offline RL, so no data is additionally collected). Note the increased stability as well as final performance of our method compared to the degradation in performance of the baseline with more training, indicating the empirical efficacy and robustness of our approach.    

%%AK: thinking of removing the stuff below -- the consequences of why the penalty works is pretty clear given the section 3 analysis and while I can show other properties like adaptive discounts, noise compounding prevention, etc, I dont think this is worth the space.
\iffalse
\textbf{Theoretical analysis of \drmethodname.} Now we shall theoretically analyze \drmethodname\ with the goal of answering the following questions: \textbf{(1)} How does \drmethodname\ regularize the learning dynamics of deep Q-learning thereby preventing aliasing?, \textbf{(2)} Does \drmethodname\ prevent the Q-function from diverging away from a good solution with more training, thereby inducing stability? and \textbf{((3)} Does \drmethodname\ robustify the learning dynamics against noise arising from the limited size of data in sampled settings, thereby leading to better solutions? 
%%AK: the notion of stability is a bit overloaded, so we need to make sure when we are talking about which stability
We provide theoretical results answering these questions next.

To answer \textbf{(1)}, we utilize the abstract model from Section~\ref{sec:theory_evidence} and show in Theorem~\ref{thm:penalty_removes_aliasing} that utilizing \drmethodname\ with the ``optimal'' weight $\alpha_{\text{opt}}$ learns features $\Phi$ such that the optimal features are close to a projection of the optimal Q-function $\bQ^*$ on the function class $\Phi$.

\textcolor{red}{Add theorem here: maybe closeness to $Q^*$ is not the best approach, but more about something else?}

In our next result, we answer \textbf{(2)} by showing that, whenever Bellman backups are performed with features $\Phi$ for which the value of the \drmethodname\ regularizer,
%%SL.5.13: really complex clause, reorder this sentence to split it into shorter simpler clauses
$\mathcal{R}(\phi) = \E_{\bs, \mathbf{a}, \bs' \sim \data, \mathbf{a}' \sim \pi}[\phi(\bs, \mathbf{a})^\top \phi(\bs', \mathbf{a}')]$, is bounded above by a certain constant $C^*$, then  Bellman backups originating in the vicinity of a good Q-function that does not alias states will converge to this fixed point. Note the direct contrast against Theorem ?? from Section~\ref{sec:bootstrapping_evidence},
%%AK: this theorem shows that Q-learning is unstable near optima because it can minimize the implicit regularizer more than the TD error.
which shows that naive TD learning will still pursue aliased solutions in the abstract model and will not converge to a good solution even starting close to it.

\textcolor{red}{Add theorem showing stable convergence to a good fixed point close to it. The previous theorem shows that regularization finds the right fixed point, and the second one shows that it converges to it.}

While the analysis so far has primarily focused on the benefits of \drmethodname\ in curbing the excessive implicit regularization effects that arise when optimizing deep networks with Bellman backups, our next result shows that in addition, \drmethodname\ reduces the degree to which error due to sampling and distributional shift compound over time as more backups are performed. By making distributional assumptions on the sampling error, we show that in certain cases, utilizing \drmethodname\ is absolutely essential to learn a meaningful policy that is better than random behavior.

\textcolor{red}{This theorem is similar to the stuff below, we will just assume that $\hat{P}^\pi- P^\pi$ is distributed according to a (truncated) normal distribution and show that the \drmethodname\ regularizer directly controls the amount of noise in the Q-function.}

\subsection{Effect of regularizer}
With the regularizer, our TD loss is
\[ \frac{1}{2}\left(\phi_\theta(s, a)w - r(s, a) - \gamma\phi_{\mathbf{a}r{\theta}}(s', a')\right)^2 + \alpha \phi_\theta(s, a)^\top \phi_{\mathbf{a}r{\theta}}(s', a'),\]
so the gradient wrt to $\theta$ is
\begin{align*}
  &\left(\phi_\theta(s, a)^\top w - r(s, a) - \gamma\phi_{\mathbf{a}r{\theta}}(s', a')^\top \mathbf{a}r{w} \right)w^\top\frac{\partial \phi_\theta(s, a)}{\partial \theta} + \alpha \phi_{\mathbf{a}r{\theta}}(s', a')^\top\frac{\partial \phi_\theta(s, a)}{\partial \theta}\\
  &=\left(\phi_\theta(s, a)^\top ww^\top - r(s, a)w^\top - \gamma\phi_{\mathbf{a}r{\theta}}(s', a')^\top \mathbf{a}r{w}w^\top + \alpha\phi_{\mathbf{a}r{\theta}}(s', a')^\top \right)\frac{\partial \phi_\theta(s, a)}{\partial \theta} \\ 
  &=\left(\phi_\theta(s, a)^\top ww^\top - r(s, a)w^\top - \phi_{\mathbf{a}r{\theta}}(s', a')^\top \left(\gamma\mathbf{a}r{w}w^\top - \alpha I \right)\right)\frac{\partial \phi_\theta(s, a)}{\partial \theta} \\ 
\end{align*}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%             OLD STUFF
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse
\section{\drmethodname: Mitigating \AliasingProblemName\ via Orthogonality Regularization}
\label{sec:method}

%%AK.1.31: I don't really like the title of this section, and no we are also committed to an acronym that "orthogonality regularization" in it because of the abstract...

If bootstrapping induces features that alias state-action tuples drawn from the offline dataset and the learned policy, which can have pathological consequences (Section~\ref{sec:consequences_of_feature_sim}), can we develop a better offline RL method by explicitly reducing this aliasing? While encouraging aliasing on two vectors is relatively straightforward, here we are interested in the inverse problem of reducing aliasing, which can be tackled in potentially many ways, not all of which are ideal for offline Q-learning. This makes it less straightforward to address this problem.
%%AK.1.31: just stressing why removing aliasing is hard, while introducing aliasing is easy... else it might appear as if our fix is straightforward -- we took a diagnostic metric and penalized that, which will seem like a hack.

%%AK.1.31: I am not a fan of the word "fortunately" here, but I couldn't find a better word to kickstart this para..
Fortunately, as we will also theoretically show in Section~\ref{sec:analysis},
%%SL.2.1: Maybe we're getting ahead of ourselves -- how about we state the point here first, some intuition, and only then say: In Section ??, we will make this intuition more precise with a theoretical proof that [something].
it turns out that simply adding a regularizer that encourages the Q-function features to have a small unnormalized similarity
%%SL.2.1: the presence of s' here is I think a little confusing...
is theoretically sufficient to address this issue. This means that features on actions drawn from the learned policy, $\mathbf{a}'$, are forced to be dissimilar to $\mathbf{a}$, on directional similarity, provided the norm of the feature vectors is large enough.
%%SL.2.1: not sure if "on a combination of direction and norm" entirely makes sense -- at first I thought you meant "on both the direction and norm," but that's actually not right...
%%AK.2.3: removed that phrase and expanded it. is it better now?
Based on this observation, our method, \drmethodname\,
penalizes the expected value of $\simunnorm(\bs, \mathbf{a}, \bs') = |\phi(\bs, \mathbf{a})^T \E_{\pi}[\phi(\bs', \cdot)]|$ in a training batch, in addition to TD-error. 
\drmethodname\ can be easily combined with existing offline RL methods, such as CQL~\citep{kumar2020conservative}, and REM~\citep{agarwal2019optimistic}, and offline policy evaluation algorithms such as FQE~\citep{le2019batch}.
%%AK.1.31: I have cited BRAC at a bunch of places, perhaps we can do some experiments with it, or we can just remove this...

While \drmethodname\ does not by itself curb overestimation in Q-values due to distributional shift, it instead aims to prevent feature aliasing with an additional loss that penalizes the absolute value of dot-products of features $\phi(\bs, \mathbf{a})$ and $\phi(\bs', \mathbf{a}')$. Thus, for an effective offline method, we combine \drmethodname\ with existing methods for mitigating distributional shift. %We will discuss how this can be theoretically derived in Section~\ref{sec:analysis}.
When combined with an offline RL algorithm, \textsc{Alg}, with objective $\mathcal{L}_{\textsc{Alg}}(\theta)$, the training objective for the Q-function, with the \drmethodname\ penalty marked in red, is: 
\begin{align}
    \min_\theta~ \mathcal{L}(\theta) := &\  \mathcal{L}_{\textsc{Alg}}(\theta) + \textcolor{red}{\alpha \E_{\bs, \mathbf{a}, \bs' \sim \data}[\simunnorm(\bs, \mathbf{a}, \bs')]}.
    %\big|\phi_\theta(\bs, \mathbf{a})^T \E_{\pi}[\phi_\theta(\bs', \cdot)]\big|.}
\label{eqn:our_method}
\end{align}
We next discuss some practical implementation details. After this, we will theoretically show in Section~\ref{sec:analysis} that optimizing the policy $\pi$ against the Q-function obtained via Equation~\ref{eqn:our_method} amounts to maximizing a lower-bound on the policy return, and the lower-bound is tighter if the value of the \drmethodname\ regularizer is small.

%%SL.2.3: I do think that it would help to include the full equation for the CQL version of FOQL in the implementation paragraph, just to make it really clear how it works (if we have space of course)
\textbf{Practical Implementation of \drmethodname.} We discuss two variants of our method: a Q-learning variant that only parameterizes the Q-function for discrete-action tasks, and an actor-critic variant that trains a separately parameterized policy for continuous action tasks. The actor-critic variant of \drmethodname\ trains the Q-function using Equation~\ref{eqn:our_method} and optimizes the policy against it.
The Q-learning variant of \drmethodname, which we use in discrete action spaces, parameterizes a multi-head Q-network~(one head per action) that learns features $\phi(\bs)$ dependent only on the state $\bs$ and not the action $\mathbf{a}$. In this case, \drmethodname\ is applied on the features $\phi(\bs)$ and $\phi(\bs')$ directly.
%%AK.1.26: do we need to have an algorithm box?
%%SL.1.26: Yes, I think an algorithm box would help. I think there was also a little bit of slaight of hand -- we were talking about how our method can be combined with any RL method, but now it looks like it's specific to CQL?
%%AK.1.31: TODO

%Implementation details
For our experiments, we build on top of prior offline RL methods: CQL~\citep{kumar2020conservative} and REM~\citep{agarwal2019optimistic}. On all the tasks, we use a default set of hyperparameters for the base method, and additionally set $\alpha_{\mathrm{\drmethodname}}$ to a constant that is kept identical across different task groups. The values of $\alpha_{\mathrm{\drmethodname}}$ can be found in Appendix ??.
%%AK.1.31: Clarify the bit about hparams since they are not all the same for the base method

%%SL.2.1: Generally, I think this section reads well. The second paragraph in the section (Fortunately...) is a little bit rough. Content-wise it's fine, but maybe go over it a couple more times to smooth out the sentence structure. The main clarity thing that I recommend addressing in this section is to clarify the point about s/a/s'/a' -- right now, the regularizer is written as a function of s/a/s', but you motivated this loss by talking about similarity between policy and data actions -- maybe it would be good to explain this leap. For example, you could define similarity in terms of pi(a|s) vs (s,a)\in D, but then explain how in practice, we already sample from pi from the target value, so it makes sense to contrast those (or explain some other reason why this is a good idea). Otherwise it comes across as a little arbitrary that the similarity of actions from data and pi is a function of s/a/s'
%%AK.2.3: this is TODO

\fi