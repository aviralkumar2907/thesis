\vspace{-0.25cm}
\section{Proof of Theorem~\ref{thm:implicit_noise_reg}}
\vspace{-0.25cm}
\label{app:proofs}

In this section, we will derive our implicit regularizer $R_\mathrm{TD}(\theta)$ that emerges when performing TD updates with a stochastic noise model with covariance matrix $M$. We first introduce our notation that we will use throughout the proof, then present our assumptions and finally derive the regularizer. Our proof utilizes the analysis techniques from \citet{blanc2020implicit} and \citet{damian2021label}, which analyze label-noise SGD for supervised learning, however key modifications need to be made to their arguments to account for non-symmetric matrices that emerge in TD learning. As a result, the form of the resulting regularizer is very different. To keep the proof concise, we will appeal to lemmas from these prior works which will allow us to bound certain concentration terms. 

\subsection{Notation}
The noisy TD-learning update for training the Q-function is given by:
\begin{align}
    \!\!\!\theta_{k+1} = \theta_k - \eta \underbrace{\left( \sum_i \nabla_\theta Q(\bs_i, \ba_i) \left(Q_\theta(\bs_i, \ba_i)\!- \!(r_i\!+\!\gamma {Q}_{\theta}(\bs'_i, \ba'_i))\right) \right)}_{:= g(\theta)}\!+\!\eta \varepsilon_k,  ~~~~ \varepsilon_k \sim \mathcal{N}(0, M)
% \label{eq:td_update_appendix}
\end{align}
% \vspace{-0.5cm}
where $g(\theta)$ denotes the parameter update. Note that $g(\theta)$ is not a full gradient of a scalar objective, but it is a form of a ``pseudo''-gradient or ``semi''-gradient. Let $\varepsilon_k$ denote an  i.i.d.random noise that is added to each update. This noise is sampled from a zero-mean Gaussian random variable with covariance matrix $M$, i.e., $\mathcal{N}(0, M)$. 

Let $\theta^*$ denote a point in the parameter space such that in the vicinity of $\theta^*$, $g(\theta) \leq \mathscr{C}$, for a small enough $\mathscr{C}$. Let $G(\theta)$ denote the derivative of $g(\theta)$ w.r.t. $\theta$: $G(\theta) = \nabla_\theta g(\theta)$ and let $\nabla G(\theta)$ denote the third-order tensor $\nabla^2_\theta g(\theta)$. For notation clarity, let $G = G(\theta^*), \nabla G = \nabla G (\theta^*)$. Let $e_i$ denote the signed TD error for a given transition $(\bs_i, \ba_i, \bs'_i) \in \mathcal{D}$ at $\theta^*$: 
\begin{align}
e_i = Q_{\theta^*}(\bs_i, \ba_i) - (r_i + \gamma Q_{\theta^*}(\bs'_i, \ba'_i)).
\end{align}
Since $\theta^*$ is a fixed point of the training TD error, $e_i = 0$. Following \citet{blanc2020implicit}, we will assume that the learning rate in gradient descent, $\eta$, is small and we will ignore terms that scale as $\mathcal{O}(\eta^{1 + \delta})$, for $\delta > 0$. Our proof will rely on using a reference Ornstein-Uhlenbeck (OU) process which the TD parameter iterates will be compared to. Let $\zeta_k$ denote the $k$-th iterate of an OU process, which is defined as:
\vspace{-0.1in}
\begin{equation}
    \label{eqn:ou_process}
    \zeta_{k+1} = (I - \eta G) \zeta_k + \eta \varepsilon_k, ~~~ \varepsilon_k \sim \mathcal{N}(0, M)
\end{equation}
We will drop $\theta$ from $\nabla_\theta$ to indicate that the gradient is being computed at $\theta^*$, and drop $(\bs_i, \ba_i)$ from $Q(\bs_i, \ba_i)$ and instead represent it as $Q_i$ for brevity; we will represent $Q(\bs'_i, \ba'_i)$ as $Q'_i$. We assume that $\nabla^2 Q_i$ is $\mathscr{L}_2$-Lipschitz and $\nabla^3 Q_i$ is $\mathscr{L}_3$-Lipschitz throughout the parameter space $\Theta$.

\subsection{Proof Strategy} 
For a given point $\theta^*$ to be an attractive fixed point of TD-learning, our proof strategy would be to derive the condition under which it mimics a given OU noise process, which as we will show stays close to the parameter $\theta^*$. This condition would then be interpreted as the gradient of a ``induced'' implicit regularizer. If the point $\theta^*$ is not a stationary point of this regularizer, we will show that the movement $\theta$ is large when running the noisy TD updates, indicating that the regularizer, at least in part guides the dynamics of TD-learning. To show this, we would write out the gradient update, isolate some terms that will give rise to the implicit regularizer, and bound the remaining terms using contraction and concentration arguments. The contraction arguments largely follow prior work (though with key exceptions in handling contraction with asymmetric and complex eigenvalue matrices), while the form of the implicit regularizer is different. Finally, we will interpret the resulting update over large timescales to show that learning is indeed guided by the implicit regularizer.   

\subsection{Assumptions and Conditions}
Next, we present some key assumptions we will need for the proof. Our first assumption is that the matrix $G \in \mathbb{R}^{d \times d}$ is of maximal rank possible, which is equal to the number of datapoints $n$ and $n \ll d$, the dimensionality of the parameter space. Crucially, this assumption do not imply that $G$ is of full rank -- it cannot be, because we are in the overparameterized regime. 
\begin{assumption}[$G$ spans an $n$-dimensional basis.]
\label{assumption:psd}
Assume that the matrix $G$ spans $n$-possible directions in the parameter space and hence, attains the maximal possible rank it can.
\end{assumption}

The second condition we require is that the matrices $\sum_i \nabla Q_i \nabla Q_i^\top$ and $M$ share the same $n$-dimensional basis as matrix $G$:
\begin{assumption}
\label{assumption:shared_basis}
$\sum_i \nabla Q_i \nabla Q_i^\top$, $M$, and $G$ span identical $n$-dimensional subspaces.
\end{assumption}
This is a technical condition that is required. If this condition is not met, as we will show the learning dynamics of noisy TD will not be a contraction in certain direction in the parameter space and TD-learning will not stabilize at such a solution $\theta^*$. We will utilize a stronger version of this statement for TD-learning to converge, and we will discuss this shortly.

\subsection{Lemmas Used In The Proof}
Next, we present some lemmas that would be useful for proving the theoretical result.  

\begin{lemma}[Expressions for the first and and second-order derivatives of $g(\theta)$.]
\label{lemma:useful}
The following definitions and expansions apply to our proof:
\begin{align*}
    G(\theta^*) &= \sum_{i} \nabla^2 Q_i e_i + \sum_i \nabla Q_i (\nabla Q_i - \gamma \nabla Q'_i)^\top\\
    \nabla G (\theta^*) [\bv, \bv] &= 2 \sum_i \nabla^2 Q_i \bv \bv^\top (\nabla Q_i - \gamma \nabla Q'_i) + \sum_i \mathrm{tr}\left((\nabla^2 Q_i - \gamma \nabla^2 Q'_i) \bv \bv^\top\right) \nabla Q_i + \nabla^3 Q_i e_i  
\end{align*}
\end{lemma}
Lemma~\ref{lemma:useful} presents a decomposition of the matrix $G$ and the directional derivative of the third order tensor $\nabla G [\bv, \bv]$ in directions $\bv$ and $\bv$, which will appear in the Taylor expansion layer. Note that at $\theta^*$ since $e_i = 0$, the first term in $G(\theta^*)$ and the third term in $\nabla G(\theta^*)[\bv, \bv]$ vanish.
Lemma~\ref{lemma:covariance_noise} derives a fixed-point recursion for the covariance matrix of the total noise accumulated in the OU-process with covariance matrix $M$ and this will appear in our proof.  

\begin{lemma}[Covariance of the random noise process $\zeta_k$]
\label{lemma:covariance_noise}
Let $\zeta_{k}$ denote the OU process satisfying: $\zeta_{k+1} = (I - \eta G) \zeta_k + \eta \varepsilon_k$, where $\varepsilon_k \sim \mathcal{N}(0, M)$, where $M \succcurlyeq 0$. Then, $\zeta_{k+1} \sim \mathcal{N}(0, \Sigma)$, where $\Sigma$ satisfies the discrete Lyapunov equation: 
\begin{equation*}
    \Sigma^*_M = (I - \eta G) \Sigma^*_M (I - \eta G)^\top + \eta^2 M.
\end{equation*}
\end{lemma}
\begin{proof}
For the OU process, $\zeta_{k+1} = (I - \eta G) \zeta_k + {\eta} \varepsilon_k$, since $\varepsilon_k$ is a Gaussian random variable, by induction so is $\zeta_{k+1}$, and therefore the covariance matrix of $\zeta_{k+1}$ is given by:
% \vspace{-0.1in}
\begin{align}
    \Sigma_{k+1} := (I - \eta G) \Sigma_k (I - \eta G^\top) + \eta^2 M.
\end{align}
% \vspace{-0.1in}
Solving for the fixed point for $\Sigma_k$ gives the desired expression.
\end{proof}

In our proofs, we will require the following contraction lemmas to tightly bound the magnitude of some zero-mean terms that will appear in the noisy TD update under certain scenarios. Unlike the analysis in \citet{damian2021label} and \citet{blanc2020implicit} for supervised learning with label noise, where the contraction terms like $(I - \eta G)^k G$ are bounded by $\approx \frac{1}{k \eta}$ intuitively because $I - \eta G$ is a contraction in the subspace spanned by matrix $G$. However, this is not true for TD-learning directly since terms like $(I - \eta G)^k S$ appear for a different matrix $S$. Therefore, TD-learning will diverge from $\theta^*$ unless matrices $G$ and $M$ have their corresponding eigenvectors assigned to the top eigenvalues be approximately ``aligned''. We formalize this definition next, and then provide a proof of the concentration guarantee. 

\begin{definition}[$(\omega, C_0)$-alignment]
\label{def:alignment}
Given a positive semidefinite matrix $A$, let $A = U_A \Lambda_A U_A^\top$ denote its eigendecomposition. Without loss of generality assume that the eiegenvalues are arranged in decreasing order, i.e., $\forall i > j, \Lambda_A(i) \leq \Lambda_A(j)$. Given another matrix $B$, let $B = U_B \Lambda_B U_B^H$ denote its complex eigendecomposition, where eigenvalues in $\Lambda_B$ are arranged in decreasing order of their complex magnitudes, i.e., $\forall i > j, |\Lambda_B(i)| \leq |\Lambda_B(j)|$. Then the matrix pair $(A, B)$ is said to be $(\omega, C_0)$-aligned if $|U_B^H(i) U_A(i)| \leq \omega$ and if $\forall~ i, \Lambda_A(i) \leq C_0 |\Lambda_B(i)|$ for a constant $C_0$. 
\end{definition}
If two matrices are $(\omega, C_0)$-aligned, this means that the corresponding eigenvectors when arranged in decreasing order of eigenvalue magnitude roughly align with each other, per the definition of alignment above. This condition would be crucial while deriving the implicit regularizer as it will quantify the rate of contraction of certain terms that define the neighborhood that the iterates of noisy TD-learning will lie in with high probability. We will operate in the setting when the matrix $G$ and $\sum_i \nabla Q_i \nabla Q_i^\top$ are $(\omega, C_0)$-aligned with each other, and matrix $M$ and $G$ are also $(\omega, C_0)$-aligned (note that we can consider $\omega', C'_0)$, which will not change our bounds and therefore we go for less notational clutter). Next we utilize this notion of alignment to show a particular contraction bound that extends the weak contraction bound in \citet{damian2021label}.  

\begin{lemma}
\label{lemma:contraction}
Assume we are given a matrix $G$ such that $|\lambda_i(I - \eta G)| \leq \rho_0 < 1$ for all $\lambda_i$ such that $\lambda_i \neq 0$. Let $G = U \Lambda U^H$ be the complex eigenvalue decomposition of $G$ (since almost every matrix is complex-diagonalizable). For a positive semi-definite matrix $S$ that is $(\omega, C_0)$-aligned with $G$, if $S = U_S \Lambda_S U_S^\top$ is its eigenvalue decomposition, the following contraction bound holds: 
\begin{align*}
    ||(I - \eta G)^k S||  = \mathcal{O}\left(\frac{\omega C_0}{\eta k}\right)
\end{align*}
\end{lemma}
\begin{proof}
To prove this statement, we can expand $(I - \eta G)$ using its eigenvalue decomposition only in the subspace that is jointly shared by $G$ and $M$, and then utilize the definition of $\omega$-alignment to bound the terms.
\begin{align}
    ||(I - \eta G)^k S|| &= ||(I - \eta U \Lambda U^H)^k U_S \Lambda_S U_S^\top ||\\
    &= \left\vert \left\vert (U U^H - \eta U \Lambda_U U^H)^k U_S \Lambda_S U_S^\top \right\vert \right\vert \\
    &= \leftnorm U \left(I - \eta \Lambda \right)^k U^H U_S \Lambda_S U_S^\top \rightnorm\\
    &\leq \omega \cdot ||\left(I - \eta \Lambda\right)^k|| \cdot \Lambda_S \\
    & \leq \omega \cdot C_0 \cdot \left( \max_i~~ |1 - \eta \Lambda (i)|^k |\Lambda(i)|\right)
\end{align}
Now we need to solve for the inner maximization term. When $\Lambda(i)$ is not complex for any $i$, the term above is $\lesssim 1/\eta k$ by applying the result from \citet{damian2021label}, but when $\Lambda(i)$ is complex, this bound can only hold under certain conditions. To note when this quantity is bounded, we expand $|1 - \eta x|^k$ for some complex number $x = r (\cos \theta + \iota \sin \theta) $:
\begin{align}
|1 - \eta x|^k &= \left\vert\left(1 - \eta r \cos \theta \right) + \iota \eta r \sin \theta \right\vert \\
&= \left[\sqrt{\left(1 - \eta r \cos \theta\right)^2 + \eta^2 r^2 \sin^2 \theta}\right]^k = \left(1 +\eta^2 r^2 - 2 \eta r \cos \theta\right)^{k/2}\\
\implies |1 - \eta x|^k |x| &= \left(1 +\eta^2 r^2 - 2 \eta r \cos \theta\right)^{k/2} r\\
& \lesssim \frac{1}{\eta k}~~~~\text{if}~ \eta \leq \min_{i} \frac{\mathrm{Re}(\Lambda(i))}{|\Lambda(i)|} ~~~~~\text{and}~~~~ \infty~~\text{otherwise}.
\end{align}
Plugging back the above expression in the bound above completes the proof.
\end{proof}

The proof of Lemma~\ref{lemma:contraction} indicates that unless the learning rate $\eta$ and the matrix $G$ are such that the $|\lambda_i(I - \eta G)| \leq \rho < 1$ in directions spanned by matrix $S$, such an expression may not converge. This is expected since the matrix $I - \eta G$  will not contract in directions of non-zero eigenvalues if the real part $r \cos \theta$ is negative or zero. Additionally, we note that under Definition~\ref{def:alignment}, we can extend several weak-contraction bounds from \citet{damian2021label} (Lemmas 9-14 in \citet{damian2021label}) to our setting. 

Next, Lemma~\ref{lemma:bounded_noise} shows that the OU noise iterates are bounded with high probability when Definition~\ref{def:alignment} holds:
\begin{lemma}[$\zeta_k$ is bounded with high probability]
\label{lemma:bounded_noise}
With probability atleast $1 - \delta$ and under Definition~\ref{def:alignment}, $||\zeta_k|| \leq n \omega \sqrt{\eta C_0} \log \frac{1}{\delta} = \mathcal{O}(\sqrt{\eta})$. 
\end{lemma}
\begin{proof}
To prove this lemma, we first bound the trace of the covariance matrix $\Sigma_{k+1}$ and then apply high probability bounds on the Martingale norm concentration. The trace of the covariance matrix $\Sigma_{k+1}$ can be bounded as follows (all the equations below are restricted to the dimensions of non-zero eigenvalues of $G$):
\begin{align}
    \mathrm{tr}\left[\Sigma_{k+1} \right] &= \sum_{j \leq k} \mathrm{tr}\left[(I - \eta G)^j M (I - \eta G^\top)^j \right]\\
    &= \sum_{j \leq k} \mathrm{tr}\left[ (U U^H - \eta U \Lambda U^H)^j M (U U^H - \eta U \Lambda U^H)^j \right]\\
    &= \sum_{j \leq k} \mathrm{tr}\left[ U (I - \eta \Lambda)^j U^H U_M \Lambda_M U_M^\top U (I - \eta \Lambda)^j U^H \right]\\
    &= \sum_{j \leq k} n \omega^2 C_0 \mathrm{tr}\left[ |I - \eta \Lambda|^j \cdot |\Lambda| \cdot |I - \eta \Lambda|^j \right]\\
    &\leq n \omega^2 C_0 \sum_{j \leq k} n \cdot \max_\lambda (|1 - \eta \lambda|^{2j} \cdot  |\lambda|) \leq {\eta n^2 C_0 \omega^2}
\end{align}
Now, we can apply Corollary 1 from \citet{damian2021label} to obtain a bound on $||\zeta_k||$ as with high probability, atleast $1 - \delta$, $||\zeta_k|| \leq \sqrt{2 \mathrm{tr}(\Sigma) \log \frac{1}{\delta}} = n \omega \sqrt{\eta C_0} \log \frac{1}{\delta}$.
\end{proof}

\subsection{Main Proof of Theorem~\ref{thm:implicit_noise_reg}}
In this section, we present the main proof of Theorem~\ref{thm:implicit_noise_reg}. The proof involves two components: \textbf{(1)} the part where we derive the regularizer, and \textbf{(2)} bounding additional terms via concentration inequalities. Part \textbf{(1)} is specific to TD-learning, while a lot of the machinery for part \textbf{(2)} is directly taken from prior work~\citep{damian2021label} and \citet{blanc2020implicit}. We focus on part \textbf{(1)} here.

Our strategy is to analyze the learning dynamics of noisy TD updates that originate at $\theta^*$. In a small neighborhood around $\theta^*$, we can expand the noisy TD update (Equation~\ref{eq:td_update}) using Taylor's expansion around $\theta^*$ which gives:
\begin{align}
    \label{eqn:nu_k_app}
    &\theta_{k+1} = \theta_k - \eta g(\theta_k)+ \eta \varepsilon_k, ~~ \varepsilon_k \sim \mathcal{N}(0, M)\\
    \implies &\theta_{k+1} = \theta_k - \eta \left( g + G (\theta_k - \theta^*) - \frac{\eta}{2} G [\theta_k - \theta^*, \theta_k - \theta^*] \right) + \eta \varepsilon_k + \mathcal{O}(\eta ||\theta_k - \theta^*||^3).
\end{align}
Denoting $\nu_k := \theta_k - \theta^*$, using the fact that $||g(\theta^*)|| \leq \mathscr{C}$, we find that $\nu_k$ can be written as:
\begin{align}
\label{eqn:nu_k_appe}
    \nu_{k+1} &= (I - \eta G) \nu_k + \varepsilon_k + \frac{\eta}{2} G [\nu_k, \nu_k] + \mathcal{O}(\eta ||\nu_k||^3 + \eta\mathscr{C})
\end{align}
Since the OU process $\zeta_k$ stays in the vicinity of the point $\theta^*$, and follows a similar recursion to the one above, our goal would be to design a regularizer so that Equation~\ref{eqn:nu_k_appe} closely follows the OU process. Thus, we would want to bound the difference between the variable $\nu_k$ and the variable $\zeta_k$, denoted as $r_k$ to be within a small neighborhood:
\begin{align*}
r_{k+1} = \nu_{k+1} - \zeta_{k+1} = (I - \eta G) \underbracket{(\nu_k - \zeta_k)}_{r_k} + \frac{1}{2} G [\nu_k, \nu_k] + \mathcal{O}(\eta ||\nu_k||^3 + \eta \mathscr{C}). 
\end{align*}
We can write down an expression for $r_k$ summing over all the terms:
\begin{equation}
\label{eqn:r_k}
    r_{k+1} = - \underbracket{\frac{\eta}{2} \sum_{j \leq k} (I - \eta G)^{k - j} \nabla G [\nu_k, \nu_k]}_{\text{term (a)}} + \underbracket{\sum_{j \leq k} (I - \eta G)^j \left[\mathcal{O}(\eta ||\nu_k||^3 + \eta \mathscr{C}) \right]}_{\text{term (b)}}.
\end{equation}
Term (a) in the above equation is the one that can induce a displacement in $r_k$ as $k$ increases and would be used to derive the regularizer, whereas term (b) primarily consists of terms that concentrate to $0$. We first analyze term (a) and then we will analyze the concentration terms later. 

To analyze term (a), note that the term $\nabla G [\nu_k, \nu_k]$, by Lemma~\ref{lemma:useful}, only depends on $\nu_k$ via the covariance matrix $\nu_k \nu_k^\top$. So we will partition this term into two terms: \textbf{(i)} a term that utilizes the asymptotic covariance matrix of the OU process and \textbf{(ii)} errors due to a finite $k$ and stochasticity that will concentrate.
\begin{align}
    2 \times \text{(a)}~ &= \eta  \sum_{j \leq k} (I - \eta G)^{k - j} \nabla G [\nu_k, \nu_k]\\
    \label{eqn:random1}
    &= \sum_{j \leq k} (I - \eta G)^{k - j} \nabla G [\zeta^*, \zeta^*] + \sum_{j \leq k} (I - \eta G)^{k - j} \nabla G ([\nu_k, \nu_k] - [\zeta^*, \zeta^*]),
\end{align}
The first term is a ``bias'' term and doesn't concentrate to $0$, and will give rise to the regularizer. We can break this term using Lemma~\ref{lemma:useful} as:
\begin{align}
\label{eqn:b_9}
   \!\!\!\!\!\!\!\!\!\!\!\!\nabla G [\zeta^*, \zeta^*] =& 2 \sum_i \nabla^2 Q_i \Sigma^*_M (\nabla Q_i - \gamma \nabla Q'_i) + \sum_i \mathrm{tr}\left[(\nabla^2 Q_i - \gamma \nabla^2 Q'_i) \Sigma^*_M \right] \nabla Q_i 
\end{align}
The regularizer $R_\mathrm{TD}(\theta)$ is the function such that:
\begin{align}
    \label{eqn:derive_regularizer}
    \nabla_\theta R_\mathrm{TD}(\theta) & = \sum_i \nabla^2 Q_i \Sigma^*_M (\nabla Q_i - \gamma \nabla Q'_i)\\
    \label{eqn:regularizer_fn_app}
    \implies R_\mathrm{TD}(\theta) &= \sum_i \nabla Q_i \Sigma^*_M \nabla Q_i^\top - \gamma \sum_i \mathrm{trace}\left(\Sigma^*_M \nabla Q_i [[\nabla Q'_i]]^\top\right),
\end{align}
where $[[\cdot]]$ denotes the stop gradient operator. If the point $\theta^*$ is a stationary point of the regularizer $R_\mathrm{TD}(\theta)$, then Equations~\ref{eqn:derive_regularizer} and \ref{eqn:regularizer_fn_app} imply that the first term of Equation~\ref{eqn:b_9} must be 0. Therefore in this case to show that $\theta^*$ is attractive, we need to show that the other terms in Equations~\ref{eqn:b_9}, \ref{eqn:random1} and term (b) in Equation~\ref{eqn:r_k} concentrate around $0$ and are bounded in magnitude. The remaining part of the proof shown in Appendix~\ref{app:concentrating} provides these details, but we first summarize the main takeaways in the proof to conclude the argument.

\subsection{Summary of the Argument}
We will show how to concentrate terms in Equation~\ref{eqn:derive_regularizer} besides the regularizer largely following the techniques from prior work, but we first summarize the entire proof. The overall update to the vector $r_k$ which measures the displacement between the parameter vector $\theta_k - \theta^*$ and the OU-process $\zeta_k$ can be written as follows, and it is governed by the derivative of the implicit regularizer (modulo error terms):
\begin{equation}
\label{Eqn:rk_final}
    r_{k+1} = - \frac{\eta}{2} \sum_{j \leq k} (I - \eta G)^{k - j} \nabla_\theta R_\mathrm{TD}(\theta^*) + \mathcal{O}\left(\sqrt{\eta t} \cdot \mathrm{poly}(\mathscr{C}, \mathscr{L}_2, \mathscr{L}_3, \omega, C_0)\right).   
\end{equation}
An important detail to note here is that since the regularizer consists of $\Sigma^*_M$ and the size of $\Sigma^*_M$ (i.e, its eigenvalues), as shown in Lemma~\ref{lemma:bounded_noise} depends on one factor of $\eta$. So, effectively the first term in Equation~\ref{Eqn:rk_final} does depend on two factors of $\eta$. Using Equation~\ref{Eqn:rk_final}, we can write the deviation between $\theta^*$ and $\theta_k$ as:
\begin{align}
\nu_{k+1} &= \zeta_{k+1} - \frac{\eta}{2} \sum_{j \leq k} (I - \eta G)^{k - j} \nabla_\theta R_\mathrm{TD}(\theta^*) + \mathcal{O}\left(\sqrt{\eta t} \cdot \mathrm{poly}(\mathscr{C}, \mathscr{L}_2, \mathscr{L}_3, \omega, C_0)\right).
\end{align}
The OU process $\zeta_k$ converges to $\theta^*$ in the subspace spanned by $G$, since the condition $\rho(I - \eta G) < 1$ is active in this subspace (if the condition that $\rho(I - \eta G) < 1$ in the subspace spanned by $G$ is not true, then as \citet{ghosh2020representations} show, TD can diverge). Now, given $G$ satisfies this spectral radius condition, $\zeta_k$ would converge to $\theta^*$ within a timescale of $\mathcal{O}\left(\frac{1}{\eta} \right)$ within this subspace, which as \citet{blanc2020implicit} put it is the strength of the ``mean-reversion'' term. On the remaining directions (note that $d \gg n$), the dynamics is guided by the regularizer, although with a smaller weight of $\eta^2$. 

% Thus if the point $\theta^*$ is not the stationary point of the regularizer $R_\mathrm{TD}(\theta)$, then we will see that the vector $r_k$ will increase linearly in $k$ as $k$ increases. Note the other conditions which are still required: \textbf{(1)} we require the spectral radius $\rho(I - \eta G) \leq 1$, with $\rho(I - \eta G) \leq \rho_0 = 1$ in directions with non-zero eigenvalues of $G$, and \textbf{(2)} $(\omega, C_0)$-contraction to ensure that various terms besides the regularizer (which is like a ``bias'' term) contract at a $\sqrt{1/\eta}$ rate. If condition \textbf{(1)} is not true, then TD learning will diverge, whereas if condition \textbf{(2)} is not true, then the learning dynamics would be dominated by \todo{fill}.     

\subsection{Additional Proof Details: Concentrating Other Terms}
\label{app:concentrating}
We first concentrate the terms in Equation~\ref{eqn:b_9}. The cumulative effect of the second term in Equation~\ref{eqn:b_9} is given by:
\begin{align}
    &\eta \sum_{j \leq k} (I - \eta G)^{j-k} \nabla Q_i \mathrm{tr}\left[ (\nabla^2 Q_i - \gamma \nabla^2 Q'_i) \Sigma^*_M \right]\\ 
    &\leq \eta \sum_{j \leq k} (I - \eta G)^{j-k} \nabla Q_i \cdot \mathcal{O}\left( \mathscr{L}_2 (1+ \gamma) \sigma \right) \leq \mathcal{O}\left( \eta \sqrt{\frac{k}{\eta}} \omega_0 C_0  \mathscr{L}_2 (1 + \gamma) \sigma \right), 
\end{align}
which follows from the fact that $\nabla^2 Q_i$ is $\mathscr{L}_2$-Lipschitz, and using Lemma~\ref{lemma:contraction} for contracting the remaining terms.

Next, we turn to concentrating the second term in Equation~\ref{eqn:random1}. This term corresponds to the contribution of difference between the empirical covariance matrix  $\nu_k \nu_k^\top$ and the asymptotic covariance matrix $\zeta^* \zeta^{* \top}$. We expand this term below using the form of $G$ from Lemma~\ref{lemma:useful}, and bound it one by one.
\begin{align}
    &\sum_{j \leq k} (I - \eta G)^{k - j} \nabla G ([\nu_k, \nu_k] - [\zeta^*, \zeta^*])\\
    & = \sum_{j \leq k} \sum_i (I - \eta G)^{k-j} \nabla^2 Q_i \left(\nu_k \nu_k - \zeta^* \zeta^{* \top}\right) (\nabla Q_i - \gamma \nabla Q'_i) + \mathcal{O}\left(\sqrt{\eta k} \omega_0 C_0 \mathscr{L}_2 (1 + \gamma) \sigma \right)
    \label{eqn:remaining}
\end{align}
Now, we note that the term $\Delta_{k+1} := \nu_{k+1} \nu_{k+1}^\top - \zeta^* \zeta^{* \top}$ can itself be written as a recursion:
\begin{align}
    \Delta_{k+1} &= (I - \eta G) (\Delta_k) (I - \eta G)^\top + \underbracket{(I - \eta G) \zeta_k \varepsilon^\top  + \varepsilon \zeta_k^\top (I - \eta G)^\top}_{A_k} + \underbracket{\varepsilon \varepsilon^\top - \eta M}_{B_k}     
\end{align}
Expanding the term $\Delta_{k+1}$ in terms of a summation over $k$, and plugging it into the expression from Equation~\ref{eqn:remaining} we get
\begin{align}
\label{eqn:remaining2}
    \sum_{i} \sum_{j \leq k} (I - \eta G)^{k-j} & \nabla^2 Q_i (I - \eta G)^j \Delta_0 (I - \eta G^\top)^j \\ 
    + \sum_i &~ \sum_{j \leq k} \sum_{p \leq j} (I - \eta G)^{k-j} \nabla^2 Q_i (I - \eta G)^{j-p-1} (A_p + B_p) (I - \eta G^\top)^{j-p-1} \nonumber
\end{align}
Now by noting that if $G$ and $\nabla Q_i$ are $(\omega, C_0)$-aligned, then so are $G^\top$ and $\nabla Q_i$, we can finish the proof by repeating the calculations used by \citet{damian2021label} (Appendix B, Equations 67-73) to bound the terms in Equation~\ref{eqn:remaining2} by $\mathcal{O}(\sqrt{\eta k})$, but with an additional factor of $\omega^2 C_0^2$.  

\textbf{Term (b) in Equation~\ref{eqn:r_k}.} When $\mathscr{C}$ is small enough, we can bound the term (b) using $\mathcal{O}(\sqrt{\eta k})$, similar to \citet{damian2021label}.  

