\vspace{-0.2cm}
\section{Related Works}
\vspace{-0.2cm}
\label{sec:extended_related}
In this section, we briefly review some related works, and in particular, try to connect feature co-adaptation and implicit regularization to various interesting results pertaining to RL lower-bounds with function approximation and self-supervised learning.

\vspace{-0.2cm}
\subsection{Brief Summary of Related Work}
\label{sec:extra_related}
\vspace{-0.2cm}
Prior analyses of the learning dynamics in RL has focused primarily on analyzing error propagation in tabular or linear settings~\citep[\eg][]{chen2019information,duan2020minimax,xie2020q, wang2021what,wang2021instabilities,farahmand2010error,de2002alp}, understanding instabilities in deep RL~\citep{achiam2019towards,bengio2020interference,kumar2020discor,van2018deep} and deriving weighted TD updates that enjoy convergence guarantees~\citep{maei09nonlineargtd,mahmood2015emphatic,sutton16emphatic}, but these methods do not reason about implicit regularization or any form of representation learning. \citet{ghosh2020representations} focuses on understanding the stability of TD-learning in underparameterized linear settings, whereas our focus is on the overparameterized setting, when optimizing TD error and learning representations via SGD.  \citet{kumar2021implicit} studies the learning dynamics of Q-learning and observes that the rank of the feature matrix, $\Phi$, drops during training. While this observation is related, our analysis characterizes the implicit preference of learning towards feature co-adaptation (Theorem~\ref{thm:implicit_noise_reg}) on out-of-sample actions as the primary culprit for aliasing. Additionally, while the goal of our work is not to increase $\srank(\Phi)$, utilizing \methodname\ not only outperforms the $\srank(\Phi)$ penalty in \citet{kumar2021implicit} by more than \textbf{100\%}, but it also alleviates rank collapse, with no apparent term that explicitly enforces high rank values. Somewhat related to DR3, \citet{durugkar2018td,pohlen2018observe} heuristically constrain gradients of TD to prevent changes in target Q-values to prevent divergence. Contrary to such heuristic approaches,  DR3 is inspired from a theoretical model of implicit regularization, and does not prevent changes in target values, but rather reduces feature dot products.

\vspace{-0.2cm}
\subsection{Extended Related Work}
\vspace{-0.2cm}

\textbf{Lower-bounds for offline RL.} \citet{zanette2020exponential} identifies hard instances for offline TD learning of linear value functions when the provided features are ``aliased''. Note that this work does not consider feature learning or implicit regularization, but their  hardness result relies heavily on the fact the given linear features are aliased in a special sense. Aliased features utilized in the hard instance inhibit learning along certain dimensions of the feature space with TD-style updates, necessitating an exponential sample size for near-accurate value estimation, even under strong coverage assumptions. A combination of \citet{zanette2020exponential}'s argument, which provides a hard instance given aliased features, and our analysis, which studies the emergence of co-adapted/similar features in the offline deep RL setting, could imply that the co-adaptation can lead to failure modes from the hard instance, even on standard offline RL problems, when provided with limited data.

\textbf{Connections to self-supervised learning (SSL).}  Several modern self-supervised learning methods~\citep{grill2020bootstrap,chen2020exploring} can be viewwed as utilizing some form of bootstrapping where different augmentations of the same input ($\bx + \text{Aug}_1, \bx + \text{Aug}_2$) serve as consecutive state-action tuples that appear on two sides of the backup. If we may extrapolate our reasoning of feature co-adaptation to this setting, it would suggest that performing noisy updates on a self-supervised bootstrapping loss will give us feature representations that are highly similar for consecutive state-action tuples, i.e., the representations for $\phi(\bx + \text{Aug}_1)^\top \phi(\bx + \text{Aug}_2)$ will be high. Intuitively, an easy way for obtaining high feature dot products is for $\phi(\cdot)$ to capture only that information in $\cdot$, which is agnostic to data augmentation, thus giving rise to features that are invariant to transformations. This aligns with what has been shown in self-supervised learning~\citep{tian2020understanding,tian2021understanding}. Another interesting point to note is that while such an explanation would indicate that highly co-adapted features are beneficial in SSL, such features can be adverse in value-based RL as discussed in Section~\ref{sec:dr3_section}. 

\textbf{Preventing divergence in deep TD-learning.} Finally, we discuss \citet{achiam2019towards} which proposes to pre-condition the TD-update using the inverse the neural tangent kernel~\citep{ntk} matrix so that the TD-update is always a contraction, for every $\theta_k$ found during TD-learning. Intuitively, this can be overly restrictive in several cases: we do not need to ensure that TD always contracts, but that is eventually stabilizes at good solution over long periods of running noisy TD updates, Our implicit regularizer (Equation`\ref{eqn:regularizer}) derives this condition, and our theoretically-inspired \methodname\ regularizer shows that empirically, it suffices to penalize the dot product similarity in practice.   

% A number of prior works have empirically studied several issues in deep RL: interference~\citep{achiam2019towards,bengio2020interference}, impact of data distributions~\citep{fu2019diagnosing,kumar2020discor,du2019distributioncheck}, inability to use big networks~\citep{bjorck2021towards,sinha2020d2rl,ota2021training} and non-stationarity~\citep{igl2020impact,fedus2020catastrophic}. Convergence of deep RL has also been shown under idealistic assumptions~\citep{yang2020theoretical,cai2019neural,zhang2020can,xu2019finite}. However, these analyses generally focus on orthogonal factors and do not analyze any form of implicit regularization effects in TD learning. While not a direct focus of our work, several methods have been proposed to handle distributional shift in offline RL (see \citep{levine2020offline} for a review) and our method, \methodname\ can be applied on many of these methods as shown in our experiments in Section~\ref{sec:experiments}.

%%SL.9.29: can probably cut out most of this section, really only the discussion of kumar2021implicit is important
% \citet{kumar2021implicit} studies the learning dynamics of Q-learning and observes that the rank of the feature matrix, $\Phi$, drops during training. While this observation is related, our analysis characterizes the implicit preference of learning towards feature co-adaptation (Theorem~\ref{thm:implicit_noise_reg}) on out-of-sample actions as the primary culprit for aliasing and rank collapse. Additionally, utilizing \methodname\ not only outperforms the $\srank(\Phi)$ penalty in \citet{kumar2021implicit} by more than \textbf{100\%}, but it also alleviates rank collapse, with no apparent term that explicitly increases rank. 
% %%AK: not sure if we want to discuss lyle2021?
% % Prior work \citep{lyle2021effect} has also studied the impact of auxilliary tasks on feature learning, which is orthogonal to the problem we study. 
% \citet{zanette2020exponential} identifies hard instances for offline TD learning of linear value functions when the provided features are ``aliased''. Aliased features may inhibit learning along certain dimensions of the feature space necessitating an exponential sample size even under strong coverage assumptions. A combination of \citet{zanette2020exponential}'s argument and our analysis would imply that the co-adaptation caused by implicit regularization can lead to failure modes from the hard instance. Additionally, \citet{durugkar2018td,pohlen2018observe} attempt to constrain gradients of TD heuristically to encourage more stationary updates by preventing the target values from changing. Finally, a similar co-adaptation also arises due to implicit regularization in self-supervised learning (SSL) methods based on bootstrapping~\citep{grill2020bootstrap,chen2020exploring} allowing the network to learn representations invariant to spurious correlations~\citep{tian2021understanding,tian2020understanding}. While this is beneficial in SSL, it can be adverse in value-based RL as discussed in Section~\ref{sec:problem}. 
