\vspace{-4pt}
\section{Preliminaries}
\vspace{-5pt}
\label{prelim}
The goal in RL is to maximize the long-term discounted reward in a Markov decision process (MDP), defined as $(\states, \actions, R, P, \gamma)$ \citep{puterman1994markov}, with state space $\states$, action space $\actions$, a reward function $R(\bs, \ba)$, dynamics $P(\bs' | \bs, \ba)$ and a discount factor $\gamma \in [0, 1)$. The Q-function $Q^\pi(\bs, \ba)$ for a policy $\pi(\ba|\bs)$ is the expected long-term discounted reward obtained by executing action $\ba$ at state $\bs$ and following $\pi(\ba|\bs)$ thereafter, 
$Q^\pi(\bs, \ba) := \E\left[ \sum\nolimits_{t=0}^\infty \gamma^t R(\bs_t, \ba_t) \right]$.
$Q^\pi(\bs, \ba)$ is the fixed point of the Bellman operator $\gT^{\pi}$, \mbox{$\gT^{\pi} Q(\bs, \ba) := 
R(\bs, \ba) + \gamma \E_{\bs' \sim P(\cdot|\bs, \ba), \ba' \sim \pi(\cdot|\bs')} \left[ Q(\bs', \ba')\right]$}. %, which can be written in vector form as: $\bQ^\pi = \bR + \gamma P^\pi \bQ^\pi$.

To disentangle the confounding effects of exploration on representations learned in RL methods, we primarily study the offline RL setting, where we are provided with a dataset $\mathcal{D}$ of transitions, $\mathcal{D} = \{(\bs_i, \ba_i, \bs'_i, r_i)\}$, generated from some behavior policy, $\behavior(\ba|\bs)$, and we are required to learn without any additional data collection. We approximate the Q-function with a neural network parameterized by $\theta$ and denote the output of the penultimate layer of the deep network (the learned \emph{features}) $\phi_\theta(\bs, \ba)$,
such that $Q_\theta(\bs, \ba) = \bw^T \phi(\bs, \ba)$, 
where $\bw \in \mathbb{R}^{d}$. Let $\Phi \in \mathbb{R}^{|\states| |\actions| \times d}$ be the matrix of such feature vectors. %Both $\bw$ and $\Phi$ depend on parameters that are learned.  
Standard deep Q-learning methods~\citep{Mnih2015,Haarnoja18} convert the Bellman equation into a temporal difference~(TD) error objective for $Q_\theta$: $\mathcal{L}_{TD}(\theta) = \sum_{\bs, \ba, \bs' \sim \mathcal{D}} \left(R(\bs, \ba) + \gamma \bar{Q}_\theta(\bs', \argmax_{\ba'} Q_\theta(\bs', \ba')) - Q_\theta(\bs, \ba) \right)^2$, where $\bar{Q}_\theta$ is a delayed copy of same Q-network, typically referred to as the \emph{target network}. 
% To measure the capacity of features in our analysis, we utilize effective rank~\citep{yang2019harnessing,kumar2021implicit}. Effective rank of a matrix $\bM \in  \mathbb{R}^n \times d, n > d$, for a given threshold $\delta$ is given by: $\mathrm{srank}_\delta(\bM) = \min \{k: \frac{\sum_{i=1}^k \sigma_i(\bM)}{\sum_{i=1}^d \sigma_i(\bM)} \geq 1 - \delta \}$, where $\{\sigma_i(\bM)\}$ denotes the singular values of $\bM$ arranged in decreasing order.  

Since our goal in this work is to study representations learned via TD-learning and not the erroneous overestimation due to distributional shift, we build on top of already existing offline RL methods. A detailed overview of the offline RL algorithms that we use -- CQL~\citep{kumar2020conservative} that penalizes erroneous Q-values during training, REM~\citep{agarwal2019optimistic} that utilizes an ensemble of Q-functions for robust learning, and BRAC~\citep{wu2019behavior} that applies a policy constraint, is provided in Appendix~\ref{app:additional_background}. 

% \textcolor{red}{Should we have a section on overparameterized networks here: that goes into min-norm solutions, relu features, or should we keep this part simple, and discuss that directly at the place we need it?}
% \rishabh{I think near the place we need it would be better.}

% While offine RL typically suffers from the issue of erroneous overestimation of Q-values~\citep{fujimoto2018off,kumar2019stabilizing,levine2020offline} caused as a result of distributional shift, we investigate methods that already tackle this out-of-distribution actions issue~\citep{levine2020offline} in this paper to prevent confounding from distribution shift.
% We consider methods that robustly estimate Q-values~(\eg REM~\citep{agarwal2019optimistic}, that utilizes a random linear combination of an ensemble of Q-function estimators) 
% or alter the Q-function training using a regularizer that induces conservatism~(\eg~BRAC~\citep{brac}, or CQL~\citep{kumar2020conservative})). Specifically, CQL optimizes $\mathcal{L}_{CQL}(\theta) = \mathcal{L}_{\text{TD}}(\theta) + \alpha_{\mathcal{R}} \mathcal{R}(\theta)$ where the regularizer
% $\mathcal{R}(\theta) := \ \E_{\bs, \ba \sim \mu}[Q_\theta(\bs, \ba)] - \E_{\bs, \ba \sim \mathcal{D}}[Q_\theta(\bs, \ba)]$, and BRAC incorporates a reward penalty equal to the KL divergence between the learned policy and the behavior policy, $\hat{r}(\bs, \ba) = r(\bs, \ba) - \alpha D_{\mathrm{KL}}(\pi(\cdot|\bs'), \behavior(\cdot|\bs'))$ into the Q-function training.
%%SL.5.17: Maybe add something like this: Our goal in this work is \emph{not} to propose a better approach to handling distributional shift in offline RL -- we build on prior methods that already account for this distributional shift. Instead, we are concerned with the properties of the representations learned by deep value functions trained via approximate dynamic programming (e.g., Q-learning or actor-critic) methods. We focus on the offline RL setting because it allows us to isolate the effects of representation learning from other confounding factors, such as exploration and active data collection. We discuss the implications of these factors briefly in Section~\ref{sec:conclusion}.

% %%AK: not sure if we want to include stuff about training the policy in more detail
% We will show in Section~\ref{sec:problem} that the features $\Phi(\bs, \ba)$ induced by training $\theta$ using $\mathcal{L}_{\text{TD}}(\theta)$ can alias together actions $a \sim \mathcal{D}$ and $a \sim \pi(\ba|\bs)$, and this can lead to poor performance. The regularizer
% %%SL.2.1: Any regularizer? Or the CQL regularizer in particular?
% %%RA.2.2: I think we only show it for CQL regularizer, so edited it accordingly.
% $\mathcal{R}(\theta)$ can prevent the issue to some extent, but choosing the optimal $\alpha_{\mathcal{R}}$ is intractable: a large $\alpha_{\mathcal{R}}$ can prevent the onset of this aliasing, but causes the performance to similar to behavior cloning, whereas a small $\alpha_{\mathcal{R}}$ may not be enough to prevent this aliasing from happening. %%AK.1.24: If we don't actually do the last thing, remove it altogether...
%%SL.2.1: I feel like there is a somewhat forced attempt in the preceding paragraph to write it in a way that is general wrt different offline RL methods, but actually it's written in a way that is specific to CQL. This comes across as a bit awkward. If you're having trouble making it general, I think it's fine to make it CQL-specific, and then just add a remark saying something like: In Section ??, we will also show how \methodname\ can be combined with other offline RL methods besides CQL.
