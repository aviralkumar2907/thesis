% \counterwithin{figure}{section}
% \counterwithin{table}{section}
% \counterwithin{equation}{section}

\input{chapters/dr3/visualizations}

%we aim to answer these questions by showing that implicit regularization induced during TD-learning is the primary cause of this co-adaptation phenomenon, and it can lead to several undesirable consequences. 
%%AK: I still kept SARSA for now, but we can remove it if needed. I removed statements saying SARSA fixes all issues, and I called it a proxy method to indicate that it is not good

\input{chapters/dr3/extended_related_work}

\input{chapters/dr3/cleaned_up_proof}


\section{Proof of Proposition~\ref{thm:co_adapted_features_are_bad}}
\label{app:new_thm}
In this section, we will prove Proposition~\ref{thm:co_adapted_features_are_bad}. First, we refer to Proposition 3.1 in \citet{ghosh2020representations}, which shows that TD-learning is stable and converges if and only if the matrix $M_\phi = \Phi^\top (\Phi - \gamma \Phi')$ has eigenvalues with all positive real entries. Now note that if, 
\begin{align}
    \sum_{\bs, \mathbf{a}} \phi(\bs, \mathbf{a})^\top \phi(\bs, \mathbf{a}) &\leq \gamma \sum_{\bs, \mathbf{a}, \bs'} \phi(\bs', \mathbf{a}')^\top \phi(\bs, \mathbf{a})\\
    \implies \mathrm{trace} \left(\Phi^\top \Phi\right) &\leq \gamma \mathrm{trace}\left(\Phi^\top \Phi'\right)\\
    \implies \mathrm{trace}\left[\Phi^\top \left(\Phi - \gamma \Phi'\right) \right] \leq 0. 
\end{align}
Since the trace of a real matrix is the sum of real components of eigenvalues, if for a given matrix $M$, $\mathrm{trace}(M) \leq 0$, then there exists atleast one eigenvalue $\lambda_i$ such that $\mathrm{Re}(\lambda_i) \leq 0$. If $\lambda_i < 0$, then the learning dynamics of TD would diverge, while if $\lambda_i = 0$ for all $i$, then learning will not contract towards the TD fixed point. This concludes the proof of this result.


\section{Experimental Details of Applying \drmethodname}
\label{app:additional_background}

In this section, we discuss the practical experimental details and hyperparameters in applying our method, \drmethodname\ to various offline RL methods. We first discuss an overview of the offline RL methods we considered in this paper, and then provide a discussion of hyperparameters for \drmethodname.

\subsection{Background on Various Offline RL Algorithms}
\label{app:details_algo}

In this paper, we consider four base offline RL algorithms that we apply DR3 on. These methods are detailed below: 

\textbf{REM}. Random ensemble mixture~\citep{agarwal2019optimistic} is an uncertainty-based offline RL algorithm uses multiple parameterized Q-functions to estimate the Q-values. During the Bellman backup, REM computes a random convex combination of the target Q-values and then trains the Q-function to match this randomized target estimate. The randomized target value estimate provides a robust estimate of target values, and delays unlearning and performance degradation that we typically see with standard DQN-style algorithms in the offline setting. {For instantiating REM}, we follow the instantiation provided by the authors and instantiate a multi-headed Q-function with 200 heads, each of which serves as an estimate of the target value. These multiple heads branch off the last-but-one layer features of the base Q-network.
The objective for REM is given by:
\begin{equation}
\!\!\!\!\!\!\!\!\!\min_{\theta} \expected_{\bs, \mathbf{a}, r, \bs' \sim \mathcal{D}} \left[ \expected_{\alpha_{1}, \dots, \alpha_{K} \sim {\Delta}} \left[ \ld \left(
\sum_{k} \alpha_{k} \Qt^{k}(\bs, \mathbf{a}) - r - \gamma\max_{\mathbf{a}'} \sum_{k} \alpha_{k}\Qtp^{k}(\bs', \mathbf{a}') \right) \right]\right] \label{eq:sqn}
\end{equation} where $l_\lambda$ denotes the Huber loss while $P_\Delta$ denotes the probability distribution over the standard (K âˆ’ 1)-simplex.

\textbf{CQL}. Conservative Q-learning~\citep{kumar2020conservative} is an offline RL algorithm that learns a conservative value function such that the estimated performance of the policy under this learned value function lower-bounds its true value. CQL modifies the Q-function training to incorporate a term that minimizes the overestimated Q-values in expectation, while maximizing the Q-values observed in the dataset, in addition to standard TD error. This CQL regularizer is typically multiplied by a coefficient $\alpha$, and we pick $\alpha=0.1$ for all our Atari experiments following \citet{kumar2021implicit} and $\alpha=5.0$ for all our kitchen and antmaze D4RL experiments. Using $\overline{y}_k(\bs, \mathbf{a})$ to denote the target values computed via the Bellman backup (we use actor-critic backup for D4RL experiments and the $\max_{\mathbf{a}'}$ backup for standard Q-learning in our Atari experiments following \citet{kumar2020conservative}), the objective for training CQL is given by: 
\begin{equation*}
    \!\!\small{\min_{Q} \alpha \left(\E_{\bs \sim \mathcal{D}}\left[\log \sum_{\mathbf{a}} \exp(Q(\bs, \mathbf{a}))\right] - \E_{\bs, \mathbf{a} \sim \mathcal{D}}\left[Q(\bs, \mathbf{a})\right] \right)\! +\! \frac{1}{2}\! \E_{\bs, \mathbf{a}, \bs' \sim \mathcal{D}}\left[\left(Q(\bs, \mathbf{a}) - \overline{y}_k(\bs, \mathbf{a}) \right)^2 \right]}.
\end{equation*}
For Atari, we utilize the standard convolutional neural network from \citet{agarwal2019optimistic,kumar2021implicit} with 3 convolutional layers borrowed from the nature DQN network and then a hidden feedforward layer of size $512$.


\textbf{COG}. COG~\citep{singh2020cog} is an algorithmic framework for utilizing large, unlabeled datasets of diverse behavior to learn generalizable policies via offline RL. Similar to real-world scenarios where large unlabeled datasets are available alongside limited task-specific data, the agent is provided with two types of datasets. The task-specific dataset consists of behavior relevant for the task, but the prior dataset can consist of a number of random or scripted behaviors being executed in the same environment/setting. The goal in this task is to actually stitch together relevant and overlapping parts of different trajectories to obtain a good policy that can work from a new initial condition that was not seen in a trajectory that actually achieved the reward. COG utilizes CQL as the base offline RL algorithm, and following \citet{singh2020cog}, we fix the hyperparameter $\alpha=1.0$ in the CQL part for both base COG and COG + DR3. All other hyperparameters including network sizes, etc are kept fixed as the prior work~\citet{singh2020cog} as well.    


\vspace{-0.2cm}
\subsection{Tasks and Environments Used}
\label{app:tasks}
\vspace{-0.2cm}

{\bf Atari 2600 games used}. For all our experiments, we used the same set of 17 games utilized by \citet{kumar2021implicit} to test rank collapse. In the case of Atari, we used the 5 standard games (\textsc{Asterix}, \textsc{Qbert}, \textsc{Pong}, \textsc{Seaquest}, \textsc{Breakout}) for tuning the hyperparameters, a strategy followed by several prior works~\citep{gulcehre2020rl,agarwal2019optimistic,kumar2021implicit}. The 17 games we test on are: \ \textsc{Asterix}, \textsc{Qbert}, \textsc{Pong}, \textsc{Seaquest}, \textsc{Breakout}, \textsc{Double Dunk}, \textsc{James Bond}, \textsc{Ms. Pacman}, \textsc{Space Invaders}, \textsc{Zaxxon}, \textsc{Wizard of Wor}, \textsc{Yars' Revenge}, \textsc{Enduro}, \textsc{Road Runner}, \textsc{BeamRider}, \textsc{Demon Attack}, \textsc{Ice Hockey}.

Following \citet{agarwal2021precipice}, we report interquartile mean~(IQM) normalized scores across all runs as mean scores can be dominated by performance on a few outlier tasks while median is independent of performance on all except 1 task -- zero score on half of the tasks would not affect the median. IQM which corresponds to 25\% trimmed mean and considers the performance on middle 50\% of the runs. IQM interpolates between mean and median, which correspond to 0\% and almost 50\% trimmed means across runs.

% {\bf D4RL tasks used.} For our experiments on D4RL, we utilize the Gym-MuJoCo-v0 environments for evaluating BRAC, since BRAC performed somewhat reasonably on these domains~\citep{fu2020d4rl}, whereas we use the harder AntMaze and Franka Kitchen domains for evaluating CQL, since these domains are challenging for CQL~\citep{kumar2020conservative}.

{\bf Robotic manipulation tasks from COG~\citep{singh2020cog}.} These tasks consist of a 6-DoF WidowX robot, placed in front of two drawers and a larger variety of objects. The robot can open or close a drawer, grasp objects from inside the drawer or on the table, and place them anywhere in the scene. The task here consists of taking an object out of a drawer. A reward of +1 is obtained when the object has been taken out, and zero otherwise. There are two variants of this domain: \textbf{(1)} in the first variant, the drawer starts out closed, the top drawer starts out open (which blocks the handle for the lower drawer), and an object starts out in front of the closed drawer,
which must be moved out of the way before opening, and \textbf{(2)} in the second variant, the drawer is blocked by an object, and this object must be removed before the drawer can be opened and the target object can be grasped from the drawer. The prior data for this environment is collected from a collection of scripted randomized policies. These policies are capable of opening and closing both drawers with 40-50\% success rates, can grasp objects in the scene with about a 70\% success rate, and place those objects at random places in the scene (with a slight bias for putting them in the tray). 


\begin{table*}[t]
\small
\caption{\textbf{Hyperparameters used by the offline RL Atari agents in our experiments.} Following \citet{agarwal2019optimistic}, the Atari environments used by us are stochastic due to sticky actions, \ie\ there is a 25\% chance at every time step that the environment will execute the agents previous action again, instead of the new action commanded. We report offline training results with same hyperparameters over 5 random seeds of the offline dataset, game simulator and network initialization.} 
\centering
\begin{tabular}{lrr}
\toprule
Hyperparameter & \multicolumn{2}{r}{Setting (for both variations)} \\
\midrule
Sticky actions && Yes        \\
Sticky action probability && 0.25\\
Grey-scaling && True \\
Observation down-sampling && (84, 84) \\
Frames stacked && 4 \\
Frame skip~(Action repetitions) && 4 \\
Reward clipping && [-1, 1] \\
Terminal condition && Game Over \\
Max frames per episode && 108K \\
Discount factor && 0.99 \\
Mini-batch size && 32 \\
Target network update period & \multicolumn{2}{r}{every 2000 updates} \\
Training environment steps per iteration && 250K \\
Update period every && 4 environment steps \\
Evaluation $\epsilon$ && 0.001 \\
Evaluation steps per iteration && 125K \\
$Q$-network: channels && 32, 64, 64 \\
$Q$-network: filter size && $8\times8$, $4\times4$, $3\times3$\\
$Q$-network: stride && 4, 2, 1\\
$Q$-network: hidden units && 512 \\
\bottomrule
\end{tabular}
\label{table:hyperparams_atari}
\end{table*}

\vspace{-0.2cm}
\subsection{The DR3 Regularizer Coefficient}
\label{app:tuning_dr3}
\vspace{-0.2cm}
We utilize identical hyperparameters of the base offline RL algorithms when DR3 is used, where the base hyper-parameters correspond to the ones provided in the corresponding publications. DR3 requires us to tune the additional coefficient $c_0$, that weights the DR3 explicit regularizer term. In order to find this value on our domains, we followed the tuning strategy typically followed on Atari, where we evaluated four different values of $c_0 \in \{0.001, 0.01, 0.03, 0.3\}$ on 5 games (\textsc{Asterix}, \textsc{Seaquest}, \textsc{Breakout}, \textsc{Pong} and \textsc{SpaceInvaders}) on the 5\% replay dataset settings, picked $c_0$ that wprked best on just these domains, and used it to report performance on all 17 games, across all dataset settings (1\% replay and 10\% initial replay) in Section~\ref{sec:experiments}. This protocol is standard in Atari and has been used previously in \citet{agarwal2019optimistic,gulcehre2020rl,kumar2021implicit} in the context of offline RL. The value of the coefficient found using this strategy was $c_0 = 0.001$ for REM and $c_0 = 0.03$ for CQL.




% For CQL on D4RL, we ran DR3 with multiple values of $c_0 \in \{0.0001, 0.001, 0.01, 0.5, 1.0, 10.0\}$, and picked the smallest value of $c_0$ which did not lead to eventually divergent (either negatively diverging or positively diverging) Q-values, in average. For the antmaze domains, this corresponded to $c_0=0.001$ and for the FrankaKitchen domains, this corresponded to $c_0=1.0$. 

% \section{\drmethodname\ Address Rank Collapse~\citep{kumar2021implicit} With Bootstrapping}
% \label{app:extra_results}
% In this section, we provide empirical evidence to show that utilizing \drmethodname\ alleviates the rank collapse issue pointed out by \citet{kumar2021implicit}, without explicitly aiming to address it. As shown in Figure~\ref{fig:iup_is_fixed}, we plot the effective rank $\mathrm{srank}(\Phi)$~\citep{kumar2021implicit} of the last-but-one layer representations of the Q-function. The effective rank of a matrix $\bM \in  \mathbb{R}^n \times d, n > d$, for a given threshold $\delta$ is given by: $\mathrm{srank}_\delta(\bM) = \min \{k: \frac{\sum_{i=1}^k \sigma_i(\bM)}{\sum_{i=1}^d \sigma_i(\bM)} \geq 1 - \delta \}$, where $\{\sigma_i(\bM)\}$ denotes the singular values of $\bM$ arranged in decreasing order. While it has been noted that bootstrapping can lead to rank collapse of these feature representations in the sense that the effective rank of features decreases, we find that DR3 addresses this issue allowing the Q-function to use its complete representational capacity as shown below.  

% \begin{figure}[ht]
% \small \begin{center}
% % \vspace{-5pt}
% \includegraphics[width=0.7\linewidth]{figures/rank_trends_dr3_dqn.pdf}
% % \includegraphics[width=\linewidth]{figures/pickplace_open_grasp.png}
% % \vspace{-20pt}
% \caption{Trend of effective rank, $\mathrm{srank}(\Phi)$ of features $\Phi$ learned by the Q-function when trained with TD error (red, ``Without DR3'') and with TD error + \drmethodname\ (blue, ``With DR3'') on three Atari games using the 5\% replay dataset~\citep{agarwal2019optimistic} in the offline RL regime. Note that the usage of \drmethodname\ clearly alleviates rank collapse, without actually explicitly correcting for the rank.}
% \label{fig:iup_is_fixed}
% \end{center}
% \end{figure}
% \section{Extra Results}
% \label{app;extra_results}

% \subsection{Atari Results}
% \label{app:atari_results}
% ABCD 



\section{Full Results for DR3}
\label{app:full_results}

% In this section, we present the results obtained by running DR3 on the Atari and D4RL domains which were not discussed in the main paper due to lack of space. We first understand the effect of applying DR3 on BRAC~\citep{wu2019behavior}, which was missing from the main paper, and then present the per-game Atari results. 

\begin{table}[H]
\small
\centering
 \caption{\small{Normalized interquartile mean~ (IQM) final performance (last iteration return) of CQL, CQL + \drmethodname, REM and REM + \drmethodname\ after 6.5M gradient steps for the 1\% setting and 12.5M gradient steps for the 5\%, 10\% settings. Intervals in brackets show 95\% CIs computed using stratified percentile bootstrap~\citep{agarwal2021precipice}}}.
 \label{tab:cql_res_median}
    \vspace{0.1cm}
\begin{tabular}{ccccc}
\toprule
% \multirow{2}{*}{\textbf{Data}} &  \multicolumn{4}{c|}{\textbf{Last iteration performance}} & \multicolumn{4}{c}{\textbf{Stability performance}} \\
Data & CQL &  CQL + \drmethodname & REM & REM + \drmethodname \\
\midrule
1\% & 44.4~\ss{(31.0, 54.3)} & \textbf{61.6}~\ss{(39.1, 71.5)} & 0.0~\ss{(-0.7, 0.1)} & \textbf{13.1}~\ss{(9.9, 18.3)} \\
\midrule
5\%  & 89.6~\ss{(67.9, 98.1)} & \textbf{100.2}~\ss{(90.6, 102.7)} & 3.9~\ss{(3.1, 7.6)} & \textbf{74.8}~\ss{(59.6, 84.4)} \\
\midrule
10\%  & 57.4~\ss{(53.2, 62.4)} &  \textbf{67.0}~\ss{(62.8, 73.0)} & 24.9~\ss{(15.0, 29.1)} &  \textbf{72.4}~\ss{(65.7, 81.7)} \\
\bottomrule
% \vspace{-10pt}
\end{tabular}
\end{table}


% \begin{table}[H]
% \fontsize{9}{9}
% \centering
% \caption{\textbf{Performance of \drmethodname\ when applied in conjunction with BRAC~\citep{wu2019behavior}.} Note that DR3 attains a larger final performance (at the end of 2M steps of training) as well as a higher average performance (i.e. stability score) across all iterations of training. %\todo{add bold}
% }
% \label{tab:brac}
% \vspace{0.2cm}
% \begin{tabular}{ccccc}
% \toprule
% \multirow{2}{*}{Task} & \multicolumn{2}{c}{Average Performance across Iterations}   & \multicolumn{2}{c}{Final Performance} \\
% & BRAC & BRAC + \drmethodname & BRAC & BRAC + \drmethodname \\
% \midrule
% %('True', '0.1', '2')
% halfcheetah-expert-v0 & 1.7 $\pm$ 1.9 & 49.9 $\pm$ 16.7  & 2.1 $\pm$ 3.3 & 71.5 $\pm$ 24.9 \\
% halfcheetah-medium-v0 & 43.5 $\pm$ 0.2 & 43.2 $\pm$ 0.2  & 45.1 $\pm$ 0.8 & 44.9 $\pm$ 0.6 \\
% halfcheetah-medium-expert-v0 & 17.0 $\pm$ 5.4 & 6.0 $\pm$ 5.5  & 24.8 $\pm$ 9.3 & 6.7 $\pm$ 7.3 \\
% halfcheetah-random-v0 & 24.4 $\pm$ 0.4 & 18.4 $\pm$ 0.3  & 24.9 $\pm$ 0.8 & 18.2 $\pm$ 1.0 \\
% halfcheetah-medium-replay-v0 & 44.9 $\pm$ 0.3 & 44.1 $\pm$ 0.4  & 45.0 $\pm$ 1.4 & 44.9 $\pm$ 0.5 \\
% hopper-expert-v0 & 15.7 $\pm$ 1.5 & 21.8 $\pm$ 3.2  & 16.6 $\pm$ 6.0 & 20.8 $\pm$ 5.3 \\
% hopper-medium-v0 & 32.8 $\pm$ 1.4 & 46.3 $\pm$ 7.1  & 36.2 $\pm$ 1.7 & 58.3 $\pm$ 13.7 \\
% hopper-medium-expert-v0 & 40.2 $\pm$ 5.7 & 37.0 $\pm$ 2.9  & 31.7 $\pm$ 11.8 & 21.8 $\pm$ 4.9 \\
% hopper-random-v0 & 11.7 $\pm$ 0.0 & 11.2 $\pm$ 0.0  & 12.2 $\pm$ 0.0 & 11.1 $\pm$ 0.0 \\
% hopper-medium-replay-v0 & 31.6 $\pm$ 0.3 & 30.3 $\pm$ 0.8  & 31.3 $\pm$ 1.2 & 36.1 $\pm$ 5.7 \\
% walker2d-expert-v0 & 25.5 $\pm$ 14.4 & 33.6 $\pm$ 11.8  & 54.0 $\pm$ 31.0 & 60.6 $\pm$ 20.2 \\
% walker2d-medium-v0 & 81.3 $\pm$ 0.3 & 80.8 $\pm$ 0.2  & 83.8 $\pm$ 0.2 & 83.4 $\pm$ 0.3 \\
% walker2d-medium-expert-v0 & 5.8 $\pm$ 5.2 & 6.4 $\pm$ 3.4  & 22.4 $\pm$ 22.0 & 39.5 $\pm$ 23.3 \\
% walker2d-random-v0 & 1.4 $\pm$ 0.8 & 1.7 $\pm$ 0.9  & 0.0 $\pm$ 0.1 & 2.9 $\pm$ 2.1 \\
% walker2d-medium-replay-v0 & 26.1 $\pm$ 6.4 & 47.4 $\pm$ 4.1  & 11.7 $\pm$ 7.0 & 38.7 $\pm$ 9.6 \\
% % \midrule
% % Median Normalized Performance & 25.5 & \textbf{33.6} & 24.9 & \textbf{38.7} \\
% % Mean Normalized Performance & 26.9 & \textbf{31.9} & 29.5 & \textbf{37.3} \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table*}[H]
\small
 \caption{\small{Normalized median final performance (last iteration return) and mediann average performance (our metric for stability) of CQL, CQL + \drmethodname, REM and REM + \drmethodname\ after 6.5M gradient steps for the 1\% setting and 12.5M gradient steps for the 5\%, 10\% settings. Intervals in brackets show 95\% CIs computed using stratified percentile bootstrap~\citep{agarwal2021precipice}}}.
 \label{tab:cql_res_median}
    \vspace{0.1cm}
\begin{tabular}{c|cccc|cccc}
\toprule
\multirow{2}{*}{\textbf{Data}} &  \multicolumn{4}{c|}{\textbf{Last iteration performance}} & \multicolumn{4}{c}{\textbf{Stability performance}} \\
& CQL & CQL+\drmethodname & REM & REM+\drmethodname & CQL & CQL+\drmethodname & REM & REM+\drmethodname \\
\midrule
1\% & 44.4~\ss{(31.0, 54.3)} & 61.6~\ss{(39.1, 71.5)} & 0.0~\ss{(-0.7, 0.1)} & \textbf{13.1}~\ss{(9.9, 18.3)} & 43.6~\ss{(36.4, 52.7)} & 56.3~\ss{(46.9, 70.3)} & 4.1~\ss{(2.9, 4.9)} & \textbf{18.1}~\ss{(11.3, 22.5)}\\
\midrule
5\%  & 89.6~\ss{(67.9, 98.1)} & 100.2~\ss{(90.6, 102.7)} & 3.9~\ss{(3.1, 7.6)} & \textbf{74.8}~\ss{(59.6, 84.4)} & 85.8~\ss{(77.3, 95.8)} & \textbf{107.6}~\ss{(105.4, 109.5)} & 28.7~\ss{(20.4, 30.0)} & \textbf{60.5}~\ss{(55.1, 65.5)} \\
\midrule
10\%  & 57.4~\ss{(53.2, 62.4)} &  \textbf{67.0}~\ss{(62.8, 73.0)} & 24.9~\ss{(15.0, 29.1)} &  \textbf{72.4}~\ss{(65.7, 81.7)} & 53.6~\ss{(51.9, 56.5)} & \textbf{71.5}~\ss{(66.5, 73.9)} & 49.4~\ss{(47.7, 54.1)} & \textbf{63.9}~\ss{(67.1, 73.9)} \\
\bottomrule
\vspace{-10pt}
\end{tabular}
\end{table*}

\iffalse

\begin{table}[H]
\centering
    \caption{\textbf{Mean evaluation returns per Atari game across 5 runs with standard deviations for 1\% dataset}. The coefficient for \drmethodname\ is 0.03 with a CQL coefficient of 1.0.
    The average performance is computed over 20 checkpoints spaced uniformly over training for 100 iterations where 1 iteration corresponds to 62,500 gradient updates.}
    \label{tab:cql_dqn_1}
    \vspace{0.2cm}
\resizebox{0.99\textwidth}{!}{\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Game} & \multicolumn{2}{c}{Final Performance}   & \multicolumn{2}{c}{Average Performance across Iterations} \\
& CQL & CQL + \drmethodname & CQL & CQL + \drmethodname \\
\midrule
Asterix       &      656.9 $\pm$ 91.0 &      821.4 $\pm$ 75.1 &      650.2 $\pm$ 65.3 &      814.1 $\pm$ 25.1 \\
Breakout      &        23.9 $\pm$ 3.8 &        32.0 $\pm$ 3.2 &        23.8 $\pm$ 0.5 &        32.8 $\pm$ 3.1 \\
Pong          &        16.7 $\pm$ 1.7 &        14.2 $\pm$ 3.3 &        15.7 $\pm$ 2.0 &        15.1 $\pm$ 2.3 \\
Seaquest      &      449.0 $\pm$ 11.0 &      446.6 $\pm$ 26.9 &      474.5 $\pm$ 30.3 &      456.1 $\pm$ 17.0 \\
Qbert         &   8033.8 $\pm$ 1513.2 &    9162.7 $\pm$ 993.6 &    7980.0 $\pm$ 379.9 &    9000.7 $\pm$ 225.2 \\
SpaceInvaders &     386.0 $\pm$ 123.2 &      351.9 $\pm$ 77.1 &      371.7 $\pm$ 47.5 &      440.6 $\pm$ 29.6 \\
Zaxxon        &     829.4 $\pm$ 813.3 &    1757.4 $\pm$ 879.4 &     834.6 $\pm$ 504.0 &    1634.0 $\pm$ 673.9 \\
YarsRevenge   &  11848.2 $\pm$ 2977.7 &  16011.3 $\pm$ 1409.0 &  15077.9 $\pm$ 1301.9 &   17741.6 $\pm$ 613.6 \\
RoadRunner    &  37000.7 $\pm$ 1148.5 &  24928.7 $\pm$ 7484.5 &   35899.9 $\pm$ 653.1 &  32063.3 $\pm$ 1011.4 \\
MsPacman      &    1869.8 $\pm$ 167.2 &    2245.7 $\pm$ 193.8 &     1991.9 $\pm$ 55.1 &     2224.1 $\pm$ 80.8 \\
BeamRider     &      780.3 $\pm$ 64.5 &      617.9 $\pm$ 25.1 &      782.0 $\pm$ 36.1 &      619.9 $\pm$ 20.9 \\
Jamesbond     &     558.5 $\pm$ 124.8 &     460.5 $\pm$ 102.0 &     524.6 $\pm$ 118.5 &      484.2 $\pm$ 89.4 \\
Enduro        &      198.4 $\pm$ 34.2 &      253.5 $\pm$ 14.2 &      259.8 $\pm$ 16.4 &      276.1 $\pm$ 16.9 \\
WizardOfWor   &     771.1 $\pm$ 358.2 &     904.6 $\pm$ 343.7 &     833.7 $\pm$ 168.4 &     935.2 $\pm$ 174.4 \\
IceHockey     &        -8.7 $\pm$ 1.3 &        -7.8 $\pm$ 0.9 &        -8.8 $\pm$ 0.9 &        -7.9 $\pm$ 0.7 \\
DoubleDunk    &       -15.1 $\pm$ 1.9 &       -14.0 $\pm$ 2.8 &       -15.3 $\pm$ 0.9 &       -14.5 $\pm$ 1.0 \\
DemonAttack   &    1970.2 $\pm$ 161.3 &      386.2 $\pm$ 75.3 &    1338.8 $\pm$ 298.4 &      414.0 $\pm$ 46.0 \\
\bottomrule
\end{tabular}}
\end{table}



\begin{table}[H]
\centering
    \caption{\textbf{Mean evaluation returns per Atari game across 5 runs with standard deviations for 5\% dataset.} The coefficient for \drmethodname\ is 0.03 with a CQL coefficient of 0.1.
    The average performance is computed over 20 checkpoints spaced uniformly over training for 200 iterations where 1 iteration corresponds to 62,500 gradient updates.}
    \label{tab:cql_dqn_5}
    \vspace{0.2cm}
    \resizebox{0.99\textwidth}{!}{\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Game} & \multicolumn{2}{c}{Final Performance}   & \multicolumn{2}{c}{Average Performance across Iterations} \\
& CQL & CQL + \drmethodname & CQL & CQL + \drmethodname \\
\midrule
Asterix       &    1798.2 $\pm$ 168.6 &    3318.5 $\pm$ 301.7 &     1812.7 $\pm$ 64.0 &    3790.5 $\pm$ 218.0 \\
Breakout      &       94.1 $\pm$ 44.4 &      166.0 $\pm$ 23.1 &      105.1 $\pm$ 10.4 &       196.5 $\pm$ 4.4 \\
Pong          &        13.1 $\pm$ 4.2 &        17.9 $\pm$ 1.1 &        15.2 $\pm$ 1.3 &        17.4 $\pm$ 1.2 \\
Seaquest      &    1815.9 $\pm$ 722.8 &    2030.7 $\pm$ 822.8 &    1382.3 $\pm$ 258.1 &    3722.3 $\pm$ 969.5 \\
Qbert         &  10595.7 $\pm$ 1648.5 &   9605.6 $\pm$ 1593.5 &    9552.0 $\pm$ 925.6 &   10830.7 $\pm$ 783.1 \\
SpaceInvaders &      758.9 $\pm$ 56.9 &    1214.6 $\pm$ 281.8 &      662.0 $\pm$ 58.1 &     1323.7 $\pm$ 94.4 \\
Zaxxon        &   1501.0 $\pm$ 1165.7 &    4250.1 $\pm$ 626.2 &    1508.8 $\pm$ 437.5 &    3556.5 $\pm$ 531.3 \\
YarsRevenge   &  24036.7 $\pm$ 3370.6 &  17124.7 $\pm$ 2125.6 &  22733.1 $\pm$ 1175.3 &  18339.8 $\pm$ 1299.7 \\
RoadRunner    &  40728.4 $\pm$ 3318.9 &  38432.6 $\pm$ 1539.7 &   42338.4 $\pm$ 471.4 &  41260.2 $\pm$ 1008.6 \\
MsPacman      &    2975.9 $\pm$ 522.1 &    2790.6 $\pm$ 353.1 &    2923.6 $\pm$ 251.3 &    3101.2 $\pm$ 381.6 \\
BeamRider     &    1897.6 $\pm$ 473.7 &      785.8 $\pm$ 43.5 &    2218.5 $\pm$ 242.4 &      775.9 $\pm$ 12.5 \\
Jamesbond     &      108.8 $\pm$ 49.1 &       96.8 $\pm$ 43.2 &        76.5 $\pm$ 4.6 &      106.1 $\pm$ 34.8 \\
Enduro        &     764.3 $\pm$ 168.7 &      938.5 $\pm$ 63.9 &      797.7 $\pm$ 47.8 &      923.2 $\pm$ 40.3 \\
WizardOfWor   &     943.2 $\pm$ 380.3 &     612.0 $\pm$ 343.3 &    1004.3 $\pm$ 314.7 &    1007.4 $\pm$ 313.2 \\
IceHockey     &       -17.3 $\pm$ 0.6 &       -15.0 $\pm$ 0.7 &       -16.6 $\pm$ 0.5 &       -12.0 $\pm$ 0.3 \\
DoubleDunk    &       -18.1 $\pm$ 1.5 &       -16.2 $\pm$ 1.7 &       -17.3 $\pm$ 1.0 &       -16.0 $\pm$ 1.6 \\
DemonAttack   &    4055.8 $\pm$ 499.7 &   8517.4 $\pm$ 1065.9 &    4062.4 $\pm$ 465.8 &    8396.7 $\pm$ 689.4 \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[H]
\centering
    \caption{\textbf{Mean returns per Atari game across 5 runs with standard deviations for initial 10\% dataset.} The coefficient for \drmethodname\ is 0.03 with a CQL coefficient of 0.1. The average performance is computed over 20 checkpoints spaced uniformly over training for 200 iterations.}
    \label{tab:cql_dqn_10}
    \vspace{0.2cm}
    \resizebox{0.99\textwidth}{!}{\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Game} & \multicolumn{2}{c}{Final Performance}   & \multicolumn{2}{c}{Average Performance across Iterations} \\
& CQL & CQL + \drmethodname & CQL & CQL + \drmethodname \\
\midrule
Asterix       &    2803.9 $\pm$ 294.6 &    3906.2 $\pm$ 521.3 &    2903.2 $\pm$ 217.7 &    4692.2 $\pm$ 377.0 \\
Breakout      &        64.7 $\pm$ 7.3 &        70.8 $\pm$ 5.5 &        65.6 $\pm$ 5.7 &        75.4 $\pm$ 6.0 \\
Pong          &         5.3 $\pm$ 6.8 &         5.5 $\pm$ 6.2 &         7.3 $\pm$ 5.0 &         8.1 $\pm$ 5.2 \\
Seaquest      &     222.3 $\pm$ 219.5 &    1313.0 $\pm$ 220.0 &     704.9 $\pm$ 254.5 &    1327.9 $\pm$ 250.0 \\
Qbert         &    4803.2 $\pm$ 489.5 &   5395.3 $\pm$ 1003.6 &    4492.5 $\pm$ 240.8 &    4708.5 $\pm$ 463.0 \\
SpaceInvaders &     704.9 $\pm$ 121.5 &      938.1 $\pm$ 80.3 &      737.8 $\pm$ 23.8 &      902.1 $\pm$ 60.0 \\
Zaxxon        &     231.6 $\pm$ 450.9 &     836.8 $\pm$ 434.7 &     394.4 $\pm$ 385.1 &     725.7 $\pm$ 370.3 \\
YarsRevenge   &  13076.2 $\pm$ 2427.0 &  12413.9 $\pm$ 2869.7 &   12493.2 $\pm$ 543.6 &  12395.6 $\pm$ 1044.2 \\
RoadRunner    &  45063.5 $\pm$ 1749.7 &  45336.9 $\pm$ 1366.7 &  45522.7 $\pm$ 1068.1 &   44808.0 $\pm$ 911.7 \\
MsPacman      &    2459.5 $\pm$ 381.3 &    2427.5 $\pm$ 191.3 &    2528.1 $\pm$ 149.2 &    2488.3 $\pm$ 109.8 \\
BeamRider     &    4200.7 $\pm$ 470.2 &    3468.0 $\pm$ 238.0 &     4729.5 $\pm$ 94.8 &    3344.3 $\pm$ 289.0 \\
Jamesbond     &       84.6 $\pm$ 25.4 &       89.7 $\pm$ 15.6 &      108.7 $\pm$ 34.1 &      111.7 $\pm$ 10.9 \\
Enduro        &     946.7 $\pm$ 289.7 &     1160.2 $\pm$ 81.5 &     1013.9 $\pm$ 29.7 &     1136.2 $\pm$ 32.5 \\
WizardOfWor   &     520.4 $\pm$ 451.2 &     764.7 $\pm$ 250.0 &     499.8 $\pm$ 238.5 &     792.2 $\pm$ 101.3 \\
IceHockey     &       -18.1 $\pm$ 0.7 &       -16.0 $\pm$ 1.3 &       -17.6 $\pm$ 0.5 &       -15.2 $\pm$ 1.0 \\
DoubleDunk    &       -21.2 $\pm$ 1.1 &       -20.6 $\pm$ 1.0 &       -20.6 $\pm$ 0.3 &       -19.7 $\pm$ 0.5 \\
DemonAttack   &    4145.2 $\pm$ 400.6 &    7152.9 $\pm$ 723.2 &    4839.4 $\pm$ 586.7 &    7278.5 $\pm$ 701.3 \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[H]
\centering
    \caption{\textbf{Mean returns per Atari game across 5 runs with standard deviations for 1\% dataset.} The coefficient for \drmethodname\ is 0.001 while we use a multi-headed REM with 200 Q-heads~\citep{agarwal2019optimistic}. The average performance is computed over 20 checkpoints spaced uniformly over training for 100 iterations.}
    \label{tab:rem_dqn_1}
    \vspace{0.2cm}
    \resizebox{0.99\textwidth}{!}{\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Game} & \multicolumn{2}{c}{Final Performance}   & \multicolumn{2}{c}{Average Performance across Iterations} \\
& REM & REM + \drmethodname & REM & REM + \drmethodname \\
\midrule
Asterix       &     240.4 $\pm$ 29.1 &     405.7 $\pm$ 46.5 &     304.4 $\pm$ 9.3 &      413.7 $\pm$ 39.6 \\
Breakout      &        0.7 $\pm$ 0.7 &       14.3 $\pm$ 2.8 &       6.3 $\pm$ 1.0 &        10.3 $\pm$ 1.1 \\
Pong          &      -14.2 $\pm$ 1.7 &       -7.7 $\pm$ 6.3 &     -14.1 $\pm$ 2.2 &       -15.3 $\pm$ 3.0 \\
Seaquest      &      81.0 $\pm$ 78.5 &    293.3 $\pm$ 191.5 &    246.6 $\pm$ 49.5 &     489.9 $\pm$ 128.6 \\
Qbert         &    239.6 $\pm$ 133.2 &    436.3 $\pm$ 111.5 &    255.5 $\pm$ 76.0 &     471.0 $\pm$ 116.5 \\
SpaceInvaders &     152.8 $\pm$ 27.5 &     206.6 $\pm$ 77.6 &     188.6 $\pm$ 5.8 &      262.7 $\pm$ 22.4 \\
Zaxxon        &    534.9 $\pm$ 731.3 &  2596.4 $\pm$ 1726.4 &  1807.9 $\pm$ 478.2 &     707.7 $\pm$ 577.4 \\
YarsRevenge   &  1452.6 $\pm$ 1631.0 &   5480.2 $\pm$ 962.3 &  4018.8 $\pm$ 987.8 &    7352.0 $\pm$ 574.7 \\
RoadRunner    &        0.0 $\pm$ 0.0 &  3872.9 $\pm$ 1616.4 &  1601.2 $\pm$ 637.9 &  14231.9 $\pm$ 2406.0 \\
MsPacman      &    698.8 $\pm$ 129.5 &   1275.1 $\pm$ 345.6 &    690.4 $\pm$ 69.7 &      860.4 $\pm$ 57.1 \\
BeamRider     &     703.0 $\pm$ 97.4 &     522.9 $\pm$ 42.2 &    745.5 $\pm$ 30.7 &      592.2 $\pm$ 27.7 \\
Jamesbond     &      41.0 $\pm$ 27.0 &     157.6 $\pm$ 65.0 &     53.3 $\pm$ 12.1 &       88.8 $\pm$ 27.2 \\
Enduro        &        0.5 $\pm$ 0.4 &     132.4 $\pm$ 16.1 &      21.7 $\pm$ 4.0 &      197.5 $\pm$ 19.1 \\
WizardOfWor   &    362.5 $\pm$ 321.8 &   1663.7 $\pm$ 417.8 &   552.1 $\pm$ 253.1 &    1460.8 $\pm$ 194.8 \\
IceHockey     &      -16.7 $\pm$ 0.9 &       -9.1 $\pm$ 5.1 &     -12.1 $\pm$ 0.8 &        -4.8 $\pm$ 1.8 \\
DoubleDunk    &      -21.8 $\pm$ 1.0 &      -17.6 $\pm$ 1.5 &     -20.4 $\pm$ 0.6 &       -17.1 $\pm$ 1.6 \\
DemonAttack   &     102.0 $\pm$ 17.3 &     162.0 $\pm$ 34.7 &    124.0 $\pm$ 10.7 &      145.6 $\pm$ 27.2 \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[H]
\centering
    \caption{\textbf{Mean returns per Atari game across 5 runs with standard deviations for the 5\% dataset.} The coefficient for \drmethodname\ is 0.001 while we use a multi-headed REM with 200 Q-heads~\citep{agarwal2019optimistic}. The average performance is computed over 20 checkpoints spaced uniformly over training for 200 iterations.}
    \label{tab:rem_dqn_5}
    \vspace{0.2cm}
    \resizebox{0.99\textwidth}{!}{\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Game} & \multicolumn{2}{c}{Final Performance}   & \multicolumn{2}{c}{Average Performance across Iterations} \\
& REM & REM + \drmethodname & REM & REM + \drmethodname \\
\midrule
Asterix       &      876.8 $\pm$ 201.1 &    2317.0 $\pm$ 838.1 &      958.9 $\pm$ 50.9 &    1252.6 $\pm$ 395.1 \\
Breakout      &         15.2 $\pm$ 4.9 &        33.4 $\pm$ 4.0 &        16.3 $\pm$ 3.4 &        17.7 $\pm$ 2.4 \\
Pong          &          7.5 $\pm$ 5.2 &        -0.7 $\pm$ 9.9 &        -4.7 $\pm$ 3.0 &       -12.0 $\pm$ 3.2 \\
Seaquest      &     1276.0 $\pm$ 417.3 &   2753.6 $\pm$ 1119.7 &    1484.3 $\pm$ 367.7 &    1602.0 $\pm$ 603.7 \\
Qbert         &    2421.4 $\pm$ 1841.8 &   7417.0 $\pm$ 2106.7 &    1330.7 $\pm$ 431.0 &    4045.8 $\pm$ 898.9 \\
SpaceInvaders &       431.5 $\pm$ 23.3 &      443.5 $\pm$ 67.4 &      349.5 $\pm$ 22.6 &      362.1 $\pm$ 33.6 \\
Zaxxon        &     6738.2 $\pm$ 966.6 &   1609.7 $\pm$ 1814.1 &    3630.7 $\pm$ 751.4 &     346.1 $\pm$ 512.1 \\
YarsRevenge   &   14454.2 $\pm$ 1644.4 &  16930.4 $\pm$ 2625.8 &  14628.3 $\pm$ 1945.1 &  12936.5 $\pm$ 1286.0 \\
RoadRunner    &  15570.9 $\pm$ 12795.6 &  46601.6 $\pm$ 2617.2 &  22740.3 $\pm$ 1977.2 &  33554.1 $\pm$ 1880.4 \\
MsPacman      &     1272.2 $\pm$ 215.3 &    2303.1 $\pm$ 202.7 &    1147.7 $\pm$ 126.1 &    1438.7 $\pm$ 140.4 \\
BeamRider     &     1922.5 $\pm$ 589.1 &      674.8 $\pm$ 21.4 &      886.9 $\pm$ 82.1 &      698.3 $\pm$ 21.5 \\
Jamesbond     &       189.6 $\pm$ 77.0 &      130.5 $\pm$ 45.7 &       120.2 $\pm$ 9.3 &       88.6 $\pm$ 41.5 \\
Enduro        &       172.7 $\pm$ 55.9 &     583.9 $\pm$ 108.7 &      236.8 $\pm$ 11.3 &      457.7 $\pm$ 39.3 \\
WizardOfWor   &      838.4 $\pm$ 670.0 &    2661.6 $\pm$ 371.4 &     1281.3 $\pm$ 66.7 &    1863.7 $\pm$ 261.2 \\
IceHockey     &         -9.7 $\pm$ 4.2 &        -6.5 $\pm$ 3.1 &        -8.1 $\pm$ 0.7 &        -4.1 $\pm$ 1.5 \\
DoubleDunk    &        -18.4 $\pm$ 0.9 &       -17.6 $\pm$ 2.6 &       -19.6 $\pm$ 1.0 &       -17.8 $\pm$ 1.9 \\
DemonAttack   &      507.7 $\pm$ 120.1 &   5602.3 $\pm$ 1855.5 &     581.6 $\pm$ 207.0 &    1452.3 $\pm$ 765.0 \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[H]
\centering
    \caption{\textbf{Mean returns per Atari game across 5 runs with standard deviations for initial 10\% dataset.} The coefficient for \drmethodname\ is 0.001 while we use a multi-headed REM with 200 Q-heads~\citep{agarwal2019optimistic}. The average performance is computed over 20 checkpoints spaced uniformly over training for 200 iterations.}
    \label{tab:rem_dqn_10}
    \vspace{0.2cm}
    \resizebox{0.99\textwidth}{!}{\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Game} & \multicolumn{2}{c}{Final Performance}   & \multicolumn{2}{c}{Average Performance across Iterations} \\
& REM & REM + \drmethodname & REM & REM + \drmethodname \\
\midrule
Asterix       &    2254.7 $\pm$ 403.6 &    5122.9 $\pm$ 328.9 &   2684.6 $\pm$ 184.4 &    3432.1 $\pm$ 257.5 \\
Breakout      &       81.2 $\pm$ 13.9 &       96.8 $\pm$ 21.2 &       63.5 $\pm$ 4.6 &        62.4 $\pm$ 6.1 \\
Pong          &         8.8 $\pm$ 3.1 &        7.6 $\pm$ 11.1 &        2.6 $\pm$ 2.1 &        -2.5 $\pm$ 5.6 \\
Seaquest      &    1540.2 $\pm$ 354.6 &     981.3 $\pm$ 605.9 &   1029.5 $\pm$ 260.6 &     836.2 $\pm$ 234.3 \\
Qbert         &    4330.7 $\pm$ 250.2 &    4126.2 $\pm$ 495.7 &   3478.0 $\pm$ 248.0 &    3494.7 $\pm$ 380.3 \\
SpaceInvaders &      895.2 $\pm$ 68.3 &      799.0 $\pm$ 28.3 &     699.7 $\pm$ 31.4 &      653.1 $\pm$ 21.5 \\
Zaxxon        &     950.7 $\pm$ 897.4 &         0.0 $\pm$ 0.0 &    490.2 $\pm$ 306.6 &         0.0 $\pm$ 0.0 \\
YarsRevenge   &  10913.1 $\pm$ 1519.1 &  11924.8 $\pm$ 2413.8 &  11508.5 $\pm$ 290.0 &  10977.7 $\pm$ 1026.9 \\
RoadRunner    &  45521.7 $\pm$ 2502.1 &  49129.4 $\pm$ 1887.9 &  37997.4 $\pm$ 638.6 &  41995.2 $\pm$ 1482.1 \\
MsPacman      &    2177.4 $\pm$ 393.0 &    2268.8 $\pm$ 455.0 &   1930.5 $\pm$ 141.7 &    2126.6 $\pm$ 147.6 \\
BeamRider     &    2921.7 $\pm$ 308.7 &    4154.9 $\pm$ 357.2 &   3727.5 $\pm$ 304.3 &     2871.0 $\pm$ 44.3 \\
Jamesbond     &      197.8 $\pm$ 73.8 &     149.3 $\pm$ 304.5 &    149.0 $\pm$ 120.5 &      83.3 $\pm$ 162.4 \\
Enduro        &     529.5 $\pm$ 200.7 &      832.5 $\pm$ 65.5 &     584.6 $\pm$ 85.3 &      801.8 $\pm$ 39.3 \\
WizardOfWor   &     606.5 $\pm$ 823.2 &     920.0 $\pm$ 497.0 &    838.3 $\pm$ 343.7 &     926.3 $\pm$ 318.5 \\
IceHockey     &        -4.3 $\pm$ 0.6 &        -5.9 $\pm$ 5.1 &       -7.0 $\pm$ 1.1 &        -5.4 $\pm$ 3.7 \\
DoubleDunk    &       -17.7 $\pm$ 3.9 &       -19.5 $\pm$ 2.5 &      -16.9 $\pm$ 0.5 &       -16.7 $\pm$ 1.0 \\
DemonAttack   &   6097.9 $\pm$ 1251.3 &   9674.7 $\pm$ 1600.6 &   4649.1 $\pm$ 514.6 &    5141.9 $\pm$ 361.4 \\
\bottomrule
\end{tabular}}
\end{table}




\begin{table}[H]
\centering
    \caption{Average returns across 5 runs for the random agent and the average performance of the trajectories in the DQN~(Nature) dataset. For Atari normalized scores reported in the paper, the random agent is assigned a score of 0 while the average DQN replay is assigned a score of 100. Note that the random agent scores are also evaluated on Atari 2600 games with sticky actions.}
    \label{tab:random_dqn_scores}
    \vspace{0.2cm}
    \resizebox{0.6\textwidth}{!}{\begin{tabular}{ccc}
\toprule
Game &  Random &  Average DQN-Replay \\
\midrule
Asterix       &   279.1 &              3185.2 \\
Breakout      &     1.3 &               104.9 \\
Pong          &   -20.3 &                14.5 \\
Seaquest      &    81.8 &              1597.4 \\
Qbert         &   155.0 &              8249.7 \\
SpaceInvaders &   149.5 &              1529.8 \\
Zaxxon        &    10.6 &              1854.1 \\
YarsRevenge   &  3147.7 &             21015.0 \\
RoadRunner    &    15.5 &             38352.3 \\
MsPacman      &   248.0 &              3108.8 \\
BeamRider     &   362.0 &              4576.4 \\
Jamesbond     &    27.6 &               560.3 \\
Enduro        &     0.0 &               671.9 \\
WizardOfWor   &   686.6 &              1128.5 \\
IceHockey     &    -9.8 &                -8.5 \\
DoubleDunk    &   -18.4 &               -11.3 \\
DemonAttack   &   166.0 &              4407.5 \\
\bottomrule
\end{tabular}}
\end{table}



\fi

% \begin{table*}[t]
% \fontsize{9}{9}\selectfont
%     \centering
%     \vspace{-0.1cm}
%     \caption{\small{Normalized final performance (last iteration return) and average performance (our metric for stability) of CQL, CQL + \drmethodname, REM and REM + \drmethodname\ after 6.5M gradient steps for the 1\% setting and 12.5M gradient steps for the 5\%, 10\% settings. Individual performance for all 17 games are provided in the Tables~\ref{tab:cql_dqn_1}-\ref{tab:rem_dqn_10}. \drmethodname\ improves the performance of both CQL and REM.} As recommended by \citet{agarwal2021precipice}, we report interquartile mean~(IQM) performance that trims 50\% of outlier runs and computes the mean of the remaining runs.}
%     \label{tab:cql_res}
%     \vspace{0.1cm}
% \begin{tabular}{c|cccc|cccc}
% \toprule
% \multirow{2}{*}{\textbf{Data}} &  \multicolumn{4}{c|}{\textbf{Last iteration performance}} & \multicolumn{4}{c}{\textbf{Stability performance}} \\
% & CQL & CQL+\drmethodname & REM & REM+\drmethodname & CQL & CQL+\drmethodname & REM & REM+\drmethodname \\
% \midrule
% 1\%  & 44.0~\ss{(38.1, 49.8)} & \textbf{50.9}~\ss{(45.8, 56.3)} & -0.1~\ss{(-0.7, 0.6)} & \textbf{13.4}~\ss{(11.1, 16.4)} & 43.7~\ss{(39.6, 48.6)} & \textbf{56.9}~\ss{(52.5, 61.2)} & 4.0~\ss{(3.3, 4.8)} & \textbf{16.5}~\ss{(14.5, 18.6)}  \\
% \midrule
% 5\%  & 74.1~\ss{(67.3, 81.3)} & \textbf{93.6}~\ss{(88.1, 99.2)} &  6.4~\ss{(5, 7.9)} & \textbf{67.5}~\ss{(62.5, 73.3)} &  78.1~\ss{(74.5, 82.4)} & \textbf{105.7}~\ss{(101.9, 110.9)} & 25.9~\ss{(23.4, 28.8)} & \textbf{60.2}~\ss({55.8, 65.1}) \\
% \midrule
% 10\% & 54.6~\ss{(49.9, 59.7)}  & \textbf{67.3}~\ss{(63.6, 72.0)} & 23.3~\ss{(19.7, 26.9)} & \textbf{77.1}~\ss{(71.8, 84.2)} & 59.3~\ss{(56.4, 61.9)} & \textbf{65.8}~\ss{(63.3, 68.3)} & 53.3~\ss{(51.4, 55.3)} & \textbf{73.8}~\ss{(69.3, 78)} \\
% \bottomrule
% \vspace{-10pt}
% \end{tabular}
% \end{table*}

% \subsection{\rebuttal{Per-Game Learning Curves for Atari Games}}
% \label{per_game_figures}


% \begin{figure}[H]
%     \centering
%     \vspace{-0.3cm}
%     \includegraphics[width=0.85\textwidth]{chapters/dr3/rebuttal/cql_dr3_5m.pdf}
%     \vspace{-0.2cm}
%     \caption{\footnotesize{\label{fig:cql_5_percent_all_games} \rebuttal{\textbf{Per-game learning curves of CQL and CQL + DR3 on the 5\% uniform replay dataset, for which the normalized average learning curve is shown in Figure~\ref{fig:atari_all_combined}.} Note that CQL + DR3 attains a higher performance than CQL for a majority of games, and rises up to a higher peak. }}}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \vspace{-5pt}
%     \includegraphics[width=0.85\textwidth]{chapters/dr3/rebuttal/rem_vs_rem_dr3_5p.pdf}
%     \vspace{-5pt}
%     \caption{\footnotesize{\label{fig:rem_5_percent_all_games} \rebuttal{\textbf{Per-game learning curves of REM and REM + DR3 on the 5\% uniform replay dataset, for which the normalized average learning curve is shown in Figure~\ref{fig:atari_all_combined}.} Note that REM + DR3 attains a higher performance than REM for a majority of games. }}}
%     \vspace{-0.5cm}
% \end{figure}


% \subsection{\rebuttal{Dot Product Similarities For CQL+DR3 and REM+DR3 on 17 Games}}
% \begin{figure}[H]
%     \centering
%     \vspace{-0.3cm}
%     \includegraphics[width=0.85\textwidth]{chapters/dr3/rebuttal/cql_vs_dr3_dot_products.pdf}
%     \vspace{-0.2cm}
%     \caption{\footnotesize{\label{fig:cql_5_percent_all_games_dot_products} \rebuttal{\textbf{Per-game feature dot products (\underline{in log scale}) of CQL and CQL + DR3 on the 5\% uniform replay dataset}. Note that CQL + DR3 attains a smaller value of the feature dot product.}}}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \vspace{-5pt}
%     \includegraphics[width=0.85\textwidth]{chapters/dr3/rebuttal/rem_vs_dr3_dot_products.pdf}
%     \vspace{-5pt}
%     \caption{\footnotesize{\label{fig:rem_5_percent_all_games_dot_products} \rebuttal{\textbf{Per-game feature dot products (\underline{in log scale}) of REM and REM + DR3 on the 5\% uniform replay dataset} Note that REM + DR3 attains a higher performance than REM for a majority of games. Note that the dot products for REM+DR3 stabilize are small, and decreases for a majority of the training steps for a number of games, or stabilize at a small value. }}}
%     \vspace{-0.5cm}
% \end{figure}



