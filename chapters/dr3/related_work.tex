\vspace{-5pt}
\subsection{Related Work}
\label{sec:related}
\vspace{-5pt}
Prior analyses of the learning dynamics in RL has focused primarily on analyzing error propagation in tabular or linear settings~\citep[\eg][]{chen2019information,duan2020minimax,xie2020q, wang2021what,wang2021instabilities,farahmand2010error,de2002alp}, understanding instabilities in deep RL~\citep{achiam2019towards,bengio2020interference,kumar2020discor,van2018deep} and deriving weighted TD updates that enjoy convergence guarantees~\citep{maei09nonlineargtd,mahmood2015emphatic,sutton16emphatic}, but these methods do not reason about implicit regularization or any form of representation learning. \citet{ghosh2020representations} focuses on understanding the stability of TD-learning in underparameterized linear settings, whereas our focus is on the overparameterized setting, when optimizing TD error and learning representations via SGD.  \citet{kumar2021implicit} studies the learning dynamics of Q-learning and observes that the rank of the feature matrix, $\Phi$, drops during training. While this observation is related, our analysis characterizes the implicit preference of learning towards feature co-adaptation (Theorem~\ref{thm:implicit_noise_reg}) on out-of-sample actions as the primary culprit for aliasing. Additionally, while the goal of our work is not to increase $\srank(\Phi)$, utilizing \methodname\ not only outperforms the $\srank(\Phi)$ penalty in \citet{kumar2021implicit} by more than \textbf{100\%}, but it also alleviates rank collapse, with no apparent term that explicitly enforces high rank values. Somewhat related to DR3, \citet{durugkar2018td,pohlen2018observe} heuristically constrain gradients of TD to prevent changes in target Q-values to prevent divergence. Contrary to such heuristic approaches,  DR3 is inspired from a theoretical model of implicit regularization, and does not prevent changes in target values, but rather reduces feature dot products.
% A number of prior works have empirically studied several issues in deep RL: interference~\citep{achiam2019towards,bengio2020interference}, impact of data distributions~\citep{fu2019diagnosing,kumar2020discor,du2019distributioncheck}, inability to use big networks~\citep{bjorck2021towards,sinha2020d2rl,ota2021training} and non-stationarity~\citep{igl2020impact,fedus2020catastrophic}. Convergence of deep RL has also been shown under idealistic assumptions~\citep{yang2020theoretical,cai2019neural,zhang2020can,xu2019finite}. However, these analyses generally focus on orthogonal factors and do not analyze any form of implicit regularization effects in TD learning. While not a direct focus of our work, several methods have been proposed to handle distributional shift in offline RL (see \citep{levine2020offline} for a review) and our method, \methodname\ can be applied on many of these methods as shown in our experiments in Section~\ref{sec:experiments}.

%%SL.9.29: can probably cut out most of this section, really only the discussion of kumar2021implicit is important
% \citet{kumar2021implicit} studies the learning dynamics of Q-learning and observes that the rank of the feature matrix, $\Phi$, drops during training. While this observation is related, our analysis characterizes the implicit preference of learning towards feature co-adaptation (Theorem~\ref{thm:implicit_noise_reg}) on out-of-sample actions as the primary culprit for aliasing and rank collapse. Additionally, utilizing \methodname\ not only outperforms the $\srank(\Phi)$ penalty in \citet{kumar2021implicit} by more than \textbf{100\%}, but it also alleviates rank collapse, with no apparent term that explicitly increases rank. 
% %%AK: not sure if we want to discuss lyle2021?
% % Prior work \citep{lyle2021effect} has also studied the impact of auxilliary tasks on feature learning, which is orthogonal to the problem we study. 
% \citet{zanette2020exponential} identifies hard instances for offline TD learning of linear value functions when the provided features are ``aliased''. Aliased features may inhibit learning along certain dimensions of the feature space necessitating an exponential sample size even under strong coverage assumptions. A combination of \citet{zanette2020exponential}'s argument and our analysis would imply that the co-adaptation caused by implicit regularization can lead to failure modes from the hard instance. Additionally, \citet{durugkar2018td,pohlen2018observe} attempt to constrain gradients of TD heuristically to encourage more stationary updates by preventing the target values from changing. Finally, a similar co-adaptation also arises due to implicit regularization in self-supervised learning (SSL) methods based on bootstrapping~\citep{grill2020bootstrap,chen2020exploring} allowing the network to learn representations invariant to spurious correlations~\citep{tian2021understanding,tian2020understanding}. While this is beneficial in SSL, it can be adverse in value-based RL as discussed in Section~\ref{sec:problem}. 
