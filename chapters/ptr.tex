\documentclass[../thesis.tex]{subfiles}

\usepackage{wrapfig}
\usepackage{cite}
% \usepackage{natbib}
% \usepackage[round]{natbib}

\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usetikzlibrary{positioning}
\usepackage{mathtools}

\usepackage{multicol}

\usepackage{xspace}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,bookmarks=true]{hyperref}

\usepackage{wrapfig}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage[skins,theorems]{tcolorbox}

\newcommand{\rebuttal}[1]{\textcolor{red}{#1}}


% \makeatletter
% \let\@oldmaketitle\@maketitle%
% \renewcommand{\@maketitle}{\@oldmaketitle%
%     \centering
%   \includegraphics[width=0.9\linewidth]{system_overview.pdf}
%   \vspace{-0.2cm}
%   \captionof{figure}{ \label{fig:system_overview} \footnotesize \textbf{Overview of \methodname:} We first perform general offline pre-training on diverse multi-task robot data and subsequently fine-tune on one or several target tasks while mixing batches between the prior data and the target dataset using a batch mixing ratio of $\tau$. Additionally, a separate online fine-tuning phase can be done, where offline pre-training is done on a static dataset and an online replay buffer is collected using rollouts in the environment. The offline and online buffers are mixed per batch with a ratio of $\beta$.}
%   \vspace{-0.35cm}
%  }
% \makeatother

\begin{document}

% \blfootnote{This chapter is based on \cite{kumar2020conservative}, published at NeurIPS 2020, which is joint work with Aurick Zhou, George Tucker, and Sergey Levine.}

\vspace{-0.4cm}
\begin{AIbox}{\large{\textbf{Abstract}}}
\vspace{4mm}
In this chapter, we present an application of offline RL that leverages diverse robotic datasets to achieve effective generalization in robotic learning. Specifically, we aim to answer the question: How can we use existing diverse offline datasets along with small amounts of task-specific data to solve new tasks? We demonstrate that employing the end-to-end offline RL techniques discussed in earlier sections of this dissertation can be an effective approach to address this, without the need for any self-supervised representation learning or vision-based pre-training. Our offline RL approach, ``pre-training for robots'' (PTR), aims to efficiently learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations. PTR directly utilizes conservative Q-learning (CQL) from Chapter~\ref{chapter:cql}, but extends it with several crucial design decisions that enable PTR to outperform various prior methods. To the best of our knowledge, PTR represents the first RL method that successfully learns new tasks in a new domain on a real WidowX robot, with limited data, effectively leveraging an existing dataset of diverse multi-task robot data collected from various toy kitchens. Furthermore, we demonstrate that PTR enables effective autonomous online improvement in just a handful of trials.
\vspace{2mm}
\end{AIbox}

% Progress in deep learning highlights the tremendous potential of utilizing  An accompanying overview video can be found in the supplementary material and at this anonymous URL: \url{https://sites.google.com/view/ptr-rss}


\input{chapters/ptr/introduction}

%===============================================================================

	
%===============================================================================

\input{chapters/ptr/prelims}

\input{chapters/ptr/method}

\input{chapters/ptr/experiments}

\input{chapters/ptr/related}

\vspace{-0.2cm}
\section{Discussion and Limitations}
\label{sec:conclusion}
\vspace{-0.2cm}
We presented a system that uses diverse prior data for general-purpose offline RL pre-training, followed by fine-tuning to downstream tasks. The prior data, sourced from a publicly available dataset, consists of over a hundred tasks across ten scenes and our policies can be fine-tuned with as few as 10 demonstrations. We show that this approach outperforms prior pre-training and fine-tuning methods based on imitation learning. One of the most exciting directions for future work is to further scale up this pre-training to provide a single policy initialization, that can be utilized as a starting point, similar to GPT3~\citep{brown2020language}. 
An exciting future direction is to scale PTR up to more complex settings, including to novel robots and incorporate training from diverse sources of robot video data. {Since joint training with offline RL was worse than pre-training and then fine-tuning with PTR, another exciting direction for future work is to understand the pros and cons of joint training and fine-tuning in the context of robot learning.}

\end{document}
