\documentclass[../thesis.tex]{subfiles}

\usepackage{wrapfig}
\usepackage{cite}
% \usepackage{natbib}
% \usepackage[round]{natbib}

\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usetikzlibrary{positioning}
\usepackage{mathtools}

\usepackage{multicol}

\usepackage{xspace}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,bookmarks=true]{hyperref}

\usepackage{wrapfig}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage[skins,theorems]{tcolorbox}

\newcommand{\rebuttal}[1]{\textcolor{red}{#1}}


% \makeatletter
% \let\@oldmaketitle\@maketitle%
% \renewcommand{\@maketitle}{\@oldmaketitle%
%     \centering
%   \includegraphics[width=0.9\linewidth]{system_overview.pdf}
%   \vspace{-0.2cm}
%   \captionof{figure}{ \label{fig:system_overview} \footnotesize \textbf{Overview of \methodname:} We first perform general offline pre-training on diverse multi-task robot data and subsequently fine-tune on one or several target tasks while mixing batches between the prior data and the target dataset using a batch mixing ratio of $\tau$. Additionally, a separate online fine-tuning phase can be done, where offline pre-training is done on a static dataset and an online replay buffer is collected using rollouts in the environment. The offline and online buffers are mixed per batch with a ratio of $\beta$.}
%   \vspace{-0.35cm}
%  }
% \makeatother

\begin{document}

% \blfootnote{This chapter is based on \cite{kumar2020conservative}, published at NeurIPS 2020, which is joint work with Aurick Zhou, George Tucker, and Sergey Levine.}

\vspace{-0.4cm}
\begin{AIbox}{\large{\textbf{Abstract}}}
\vspace{4mm}
In this chapter, we will study the efficacy of offline RL in leveraging broad robotic datasets for attaining effective generalization in robotic learning.  Concretely, we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that utilizing the end-to-end offline RL techniques discussed in this dissertation can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. To our knowledge, PTR is the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. We also demonstrate that PTR can enable effective autonomous online fine-tuning and improvement in a handful of trials.
\vspace{2mm}
\end{AIbox}

% Progress in deep learning highlights the tremendous potential of utilizing  An accompanying overview video can be found in the supplementary material and at this anonymous URL: \url{https://sites.google.com/view/ptr-rss}


\input{chapters/ptr/introduction}

%===============================================================================

	
%===============================================================================

% \input{chapters/ptr/prelims}

\input{chapters/ptr/method}

\input{chapters/ptr/experiments}

\input{chapters/ptr/related}


\section{Discussion and Limitations}
\label{sec:conclusion}
We presented a system that uses diverse prior data for general-purpose offline RL pre-training, followed by fine-tuning to downstream tasks. The prior data, sourced from a publicly available dataset, consists of over a hundred tasks across ten scenes and our policies can be fine-tuned with as few as 10 demonstrations. We show that this approach outperforms prior pre-training and fine-tuning methods based on imitation learning. One of the most exciting directions for future work is to further scale up this pre-training to provide a single policy initialization, that can be utilized as a starting point, similar to GPT3~\citep{brown2020language}. 
An exciting future direction is to scale PTR up to more complex settings, including to novel robots. {Since joint training with offline RL was worse than pre-training and then fine-tuning with PTR, another exciting direction for future work is to understand the pros and cons of joint training and fine-tuning in the context of robot learning.}

\end{document}
