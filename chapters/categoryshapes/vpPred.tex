\section{Learning to Predict Viewpoint}
\todo{REMOVE. Smooth out}
\seclabel{viewpoints}

In our proposed framework, viewpoint prediction is an important component towards reconstructing the objects present in an image. We are interested in a system that is accurate across all instances of a category as well as robust to localization errors in object detection. We present a CNN based system for the viewpoint prediction task and demonstrate  that it leads to significant improvements over previous approaches.


\paragraph{Related Work.}
Recently, CNNs have been shown to outperform Deformable Part Model (DPM) \cite{felzens_latent_pami10} based methods for recognition tasks \cite{rcnn,Krizhevsky}. Whereas DPMs explicitly model part appearances and their deformations, the CNN architecture allows such relations to be captured implicitly using a hierarchical convolutional structure.  Girshick \etal \cite{DPMsCNNs} argued that DPMs could also be thought as a specific instantiation of CNNs and therefore training an end-to-end CNN for the corresponding task should outperform a method which instead explicitly models part appearances and relations.

This result is particularly applicable to viewpoint estimation where the prominent approaches, from the initial instance based methods \cite{huttenlocher1990recognizing} to current state-of-the-art  \cite{pascal3d,pepik12dpm} explicitly model local appearances and aggregate evidence to infer viewpoint. Pepik \etal \cite{pepik12dpm} extend DPMs to 3D to model part appearances and rely on these to infer pose and Xiang \etal \cite{pascal3d} introduce a separate DPM component corresponding to each viewpoint. Ghodrati \etal \cite{ghodrati14viewpoint} differ from the explicit part-based methodology, using a fixed global descriptor to estimate viewpoint. We build on both these approaches by using a method which, while using a global descriptor, can implicitly capture part appearances.


\paragraph{Formulation.}
We formulate the global pose estimation for rigid categories as predicting the viewpoint wrt to a canonical pose. This is equivalent to determining the three euler angles corresponding to azimuth ($\phi$), elevation($\varphi$) and cyclo-rotation($\psi$). We frame the task of predicting the euler angles as a classification problem where the classes $\{1,\ldots N_{\theta}\}$ correspond to $N_{\theta}$ disjoint angular bins. We note that the euler angles, and therefore every viewpoint, can be equivalently described by a rotation matrix. We will use the notion of viewpoints, euler angles and rotation matrices interchangeably.


\paragraph{Learning.}
Viewpoint is manifested in a 2D image by the spatial relationships among the different features of the object. CNN based methods which can implicitly capture and hierarchically build on such relations are therefore suitable candidates for viewpoint prediction. Let $N_c$ be the number of object classes, $N_a$ be number of angles to be predicted per instance. The number of output units per class is $N_{a} \times N_{\theta}$ resulting in a total of $N_{c} \times N_{a} \times N_{\theta}$ outputs. We adopt an approach similar to Girshick \etal \cite{rcnn} and finetune a CNN model whose weights are initialized from a model pretrained on the Imagenet \cite{imagenet_cvpr09} classification task. We experimented with the architectures from Krizhevsky \etal \cite{Krizhevsky} (denoted as TNet) and Simonyan \etal \cite{Simonyan14c} (denoted as ONet). The architecture of our network is the same as the corresponding pre-trained network with an additional fully-connected layer having $N_{c} \times N_{a} \times N_{\theta}$ output units.

Instead of training a separate CNN for each class, we implement a loss layer that selectively considers the $N_{a} \times N_{\theta}$ outputs corresponding the class of the training instance and computes  a logistic loss for each of the angle predictions. This allows us to train a CNN which can jointly predict viewpoint for all classes, thus enabling learning a shared feature representation across all categories. We use the Caffe framework \cite{jia2014caffe} to train and extract features from the CNN described above. We also use data-augmentation by jittering ground-truth bounding boxes and generating additional training examples by using boxes that overlap with the annotated bounding box with IoU $>$ 0.7.
