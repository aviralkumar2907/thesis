\section{Why are Actor Constraints Insufficient with Function Approximation?}
%%SL.4.25: I think starting with this as the first technical section doesn't set quite the right tone. It positions the work entirely as a "response" to a couple of recent papers. Positioning our paper in relation to prior work is important, but it's not the most important. It's more critical to make the work stand on its own. What I would recommend organizationally for this paper is to first motivate it from first principles, and then later have a section that describes how it relates to prior work. One way to facilitate this kind of organization is to put the related work section at the end. You could e.g. have a Related Work section after the technical derivations and before experiments, and potentially have some of this discussion in there. But having a large section dedicated entirely to why prior work is not good enough is not really a good idea I think. I think a half-page (max) section on relationship to prior work would be better, with more of the details off-loaded to an appendix.

Prior work has used actor constraints or penalties to prevent out-of-distribution action queries in the Q-function backup in the absence of function approximation. However, in this section, we demonstrate that actor constraints may be insufficient to provide effective learning progress in the presence of function approximation.
%%SL.4.25: Do we really need to argue that all actor constraints are insufficient? I feel like it's not strategically optimal to make that argument unless we are 110% convinced of this. It may well be that next year we'll figure out a better way to do actor constraints, and we'll feel a bit silly for trying to publish a paper with a section like this. Maybe we should scope the prior work discussion more narrowly in regard to the particular methods that have been proposed in prior work?
Our reasoning behind this statement is based on two reasons.
%%SL.4.25: Somewhat repetitive phrasing (our reasoning is based on two reasons)
First, typical actor constraints or penalties aim at penalizing the learned actor, $\pi_\phi$ against
%%SL.4.25: somewhat awkward phrasing (penalizing the learned actor against...) -- maybe constraining the learned actor against?
the effective Markovian policy for the dataset
%%SL.4.25: what does the word "effective" mean, and why is the dataset policy Markovian? perhaps what you mean is that it makes the assumption that the dataset is produced by a Markovian policy, but this way of writing it makes it hard to follow.
defined as, $\pi_b(a|s) = \frac{\mu(s, a)}{\mu(s)}$. $\pi_b$ can be a highly multimodal and complex distribution, and the performance of most prior methods that rely on fitting an accurate parametric model for $\pi_b$, may thus be bottlenecked by the inability to fit this behavior distribution.
%%SL.4.25: This is a reasonable argument, but it would be much cleaner I think to write it like this: (1) summarize and formally define what an actor constraint is; (2) describe (in one sentence!) the Markovian generating policy issue; (3) then briefly describe the second reason.

The second reason is more subtle. In scenarios where the dataset distribution, $\mu(s, a)$ is ``narrow'',
%%SL.4.25: comma goes inside the quotes
%%SL.4.25: need to be more precise about what narrow means
actor constraints or penalties may be insufficient to constrain the critic in the presence of function approximation, since the critic is trained by minimizing squared Bellman error on a narrow set of in-distribution actions (i.e., actions with $\mu(a|s) > 0$), and the Q-value outputs on other actions are pretty much uncontrolled.
%%SL.4.25: This comes across as pretty vague.
Thus, generalization effects of the Q-function approximator may give rise erroneously over-optimistic Q-values on out-of-distribution actions, even when the target values are constrained to utilize in-distribution actions. When these erroneously high Q-values on out-of-distribution actions are then used to update the actor, it introduces errors in the actor, even in the presence of an actor penalty, which can slow down the amount of error accumulation in the actor, but not completely prevent it in certain scenarios.
%%SL.4.25: I think we need to find a more concise, clear, and concrete way to state this. Is the key that the actor constraint is itself enforced at sampled states? If so, maybe explaining that would work better?

%%SL.4.25: I think this is too much. If the purpose is just to discuss prior work, this example is spending too much time on it, maybe put it in an appendix if you really want to keep it, or delete.
In order to build intuition for the argument presented above, we will now present a simple didactic example of a 1D ``line-world'' MDP. This MDP has continuous states in the interval $[-1, 1]$ on the 1D number-line. At any state, there are two available actions: $a_1$, which transitions the agent to the left by $0.2$ units, and $a_2$ that transits the agent to the right by $0.2$ units.
%%SL.4.25: so is it continuous or discrete? it looks like a discrete MDP masquerading as a continuous one
The features provided for training a linear Q-function approximator are given by: $\phi(s, a_1) = s, \phi(s, a_2) = s + \varepsilon$, and the Q-function is parameterized via two scalar parameters, $w, b$. By suitably choosing the behavior policy density and the initialization of the Q-functions, for any given policy penalty coefficient, $\beta$,
% TODO (aviral): define this coefficient in preliminaries, and add figure, and more calculations from notebook explaining as to why this is the case
we can set $\varepsilon$, such that a single gradient step on a uniform over actions initial policy, updates the actor towards choosing an out-of-distribution action $a$, for which $\pi_b(a|s) << 1$. 
%%SL.4.25: I don't think this last sentence quite makes sense to me... just having a lot of trouble understanding it.

The example provides some intuition for why actor penalties may not suffice when narrow distributions can induce erroneous critic generalization. In order to formally quantify the generalization effects induced in the Q-function, we assume that the Q-function is parameterized by $\theta \in \mathbb{R}^d$, and is updated via gradient descent, and 
% \begin{equation}
% \label{eqn:gradient_vanilla}
%     \theta_{k+1} = \theta_{k} - \alpha_k \cdot \expec_{s, a \sim \mu} [(Q_\theta(s, a) - \bellman^\pi Q_{\bar{\theta}}(s, a) ) \cdot \nabla_\theta Q_\theta(s, a)]
% \end{equation}
compute the change in Q-values for a state action pair $(\bar{s}, \bar{a})$ due to a change in the parameter vector $\theta$ caused by the critic update, up to first order, as
\begin{equation}
\label{eqn:first_order}
    Q_{k+1}(\bar{s}, \bar{a}) \approx Q_{k}(\bar{s}, \bar{a}) - \alpha_k \expec_{s, a \sim \mu} [(Q_k(s, a) - \bellman^\pi \bar{Q}_k(s, a) ) \cdot k_\theta((s, a), (\bar{s}, \bar{a}))]
\end{equation}
%%SL.4.25: put comma at the end of equation (in general, equations should have punctuation!)
where $k_\theta((s, a), (\bar{s}, \bar{a})) := \nabla_\theta Q_\theta(s, a)^T \nabla_\theta Q_\theta(\bar{s}, \bar{a})$ is the neural tangent kernel (NTK)~\citep{jacot18ntk}. 
When function approximation is not used, the corresponding Q-function iterate is given by:
\begin{equation}
\label{eqn:regular_update}
    Q_{k+1}(\bar{s}, \bar{a}) = Q_k(\bar{s}, \bar{a}) + \alpha_k \mu(\bar{s}, \bar{a}) (\bellman^\pi \bar{Q}_k(\bar{s}, \bar{a}) - Q_k(\bar{s}, \bar{a}))
\end{equation}
%%SL.4.25: put comma at the end of equation (in general, equations should have punctuation!)
Now, we may ask: how does an update in Equation~\ref{eqn:first_order} affect the actor, which uses these new Q-values for its update? The policy update is:
\begin{equation}
\label{eqn:policy_constraint}
    \pi_{k+1} \leftarrow \max_{\pi}~~ \expec_{s \sim \mu(s)}\left[\expec_{a \sim \pi(\cdot|s)} [Q_{k+1}(s, a)] \right] + \beta D(\pi, \mu).
\end{equation}
Denoting $Q_{k+1}^{\text{no}}$ as the updated Q-function, $Q_{k+1}$, from Equation~\ref{eqn:regular_update}, and re-writing the policy update, we obtain:
% TODO (aviral): define K_\theta (it is the kernel matrix for the NTK kernel)
\begin{equation}
    \pi_{k+1} \leftarrow \max_{\pi}~~~ \expec_{s \sim \mu(s)} \left[\expec_{a \sim \pi(\cdot|s)}[Q_{k+1}^{\text{no}}(s, a) + \alpha_k (K_\theta - I) \text{diag}(\mu) (\bellman^{\pi_k}\bar{Q}_k - Q_k)(s, a)] + \beta D(\pi, \mu) \right]
\end{equation}
Observe that the additional error in the Q-function due to function approximation can accidently lead to overestimated Q-values at out-of-distribution actions, as shown in the example discussed previously. Even when an actor penalty is used, as shown in Equation~\ref{eqn:policy_constraint}, the additional error in the Q-values can bias the policy towards actions that are sub-optimal. Note that this phenomenon is absent without function approximation, if Q-values at OOD actions are initialized to pessimistic values, since these are unaffected with the update shown in Equation~\ref{eqn:regular_update}. 

%%SL.4.25: I don't think we need this paragraph.
To summarize, the key issue here is the inability to control the function approximation error in the Q-function only via an actor constraint that can control target values, and the ability of this function approximation error to bias the policy towards OOD actions. As shown in our example, even in the presence of penalties on the actor, certain choices of features and initialization may prevent the actor from learning meaningfully, and enter into a cycle of gradual error accumulation. 
\textcolor{red}{AK: Probably add some stuff about how in an iterative sense this error accumulates. And put an example on a 1D lineworld, with linear function approximation that shows this issue. Or show a gridworld example.}

%%SL.4.25: As I wrote at the top, I think motivating our work from the standpoint that actor penalties are not good is not doing us any favors. It needlessly picks a fight, makes the work appear incremental, and doesn't lead with our strength. Let's motivate the method for what it is first, and then later talk about actor penalties in prior work.

\section{Controlling Function Approximation Error via Penalized Q-Learning}
%%SL.4.25: I think "controlling function approximation error" is too vague -- almost all deep RL improvements are in some sense controlling function approximation error. Let's also come up with a better name for what we are doing than "Penalized Q-Learning" -- somehow it sounds rather negative ;)

Now that we have seen that function approximation error in the Q-function -- which arises when backups are performed on certain choices of data distribution $\mu$ -- can cause problems with learning even in the presence of constrained bootstrapped target values, we seek a solution to this issue. A na\"ive solution would be to simply mimic tabular updates but with function approximators as previously explored in \citet{achiam2019towards}, however, this scheme is not desirable since computing gradient inner products ($K_\theta$) over all the states and empirically inverting the matrix $K_\theta$ is computationally extremely expensive. One can choose to just account for $K_\theta$ within a mini-batch, analogous to \citet{achiam2019towards}, however, this is not known to provide huge benefits and requires modifying substantial architectural details and design details for numerical stability reasons.
%%SL.4.25: I don't feel great about this way of motivating things. Perhaps this is because this section follows up on Section 3, which is all about prior work, but I feel like we should motivate our solution from first principles.

Instead, our approach, which we refer to as ALGONAME, chooses to penalize the Q-function under a suitably chosen ``negative'' distribution, and upweights Q-function values at in-distribution actions.
%%SL.4.25: This sounds very ad hoc to me.
This ensures that the Bellman error minimization procedure does not erroneously downweight the Q-values at in-distribution actions and upweight those at out-of-distribution actions, as a consequence of function approximation. More concretely, our objective for training the Q-function, as shown in Equations~\ref{eqn:generic_new_update} and \ref{eqn:generic_new_update_2} now minimizes the Bellman error, subject to explicitly up-weighting Q-values on in-distribution actions while penalizing them on actions drawn from a different distribution $\nu$, which is of our choice. We refer to this family of optimization problems as: $\mathsf{PenalizedQ(\nu)}$. 
%%SL.4.25: Let's discuss next time how we can motivate this better. Perhaps it's easier to first motivate minimizing the Q-function only (without the maximization term), then show that this is a bound, then show we get a better bound by also maximizing, and then relate this to a KL-divergence constraint?
\begin{align}
    \label{eqn:generic_new_update}
    \mathsf{PenalizedQ(\nu)}:~~ \theta_{k+1} = \arg\min_{\theta} ~~& \sum_{i=1}^N \left( Q_\theta(s_i, a_i) - \bellman^\pi \bar{Q}(s_i, a_i) \right)^2 \\ %%SL.4.25: would it make more sense to write the objective as an expectation?
    \text{s.t.}~~ & \expec_{s \sim \mu(s)}\left[ \expec_{a \sim \nu(\cdot|s)} [Q_\theta(s, a)] - \expec_{a \sim \mu(\cdot|s)}[Q_\theta(s, a)] \right] \leq \varepsilon
    \label{eqn:generic_new_update_2}
\end{align}
For different choices of the negative distribution $\nu$,
%%SL.4.25: why are we calling this a negative distribution?
this objective amounts to solving different problems: If $\mu$
%%SL.4.25: mu or nu? maybe just define a latex command so that you can easily change it later?
is chosen to be the Boltzmann distribution corresponding to the Q-function, i.e. $\nu(a|s) \propto \exp(Q_\theta(s, a))$, then, Equation~\ref{eqn:generic_new_update_2} is equivalent to penalizing the suboptimality of in-distribution actions,
%%SL.4.25: "penalizing the suboptimality" seems like somewhat awkward phrasing
$\mu(a|s)$ under the learned Q-function. If $\nu$ is chosen to be a function that is inversely proportional to the dataset action marginal, $\mu$, this amounts to penalizing the least likely actions in the dataset.
%%SL.4.25: I think it's a bit easier to understand what this is doing if you write it as a max/min, rather than the present discussion, which comes across as quite ad hoc.

More generally, we can choose $\nu$ via a separate optimization procedure that optimizes $\mu$ subject to a concave regularizer $\mathcal{R}(\nu)$, giving rise to:
\begin{equation}
    \min_\theta~~ \max_\nu \sum_{i=1}^N \left( Q_\theta(s_i, a_i) - \bellman^\pi \bar{Q}\right)^2 + \beta \cdot \expec_{s \sim \mu}\left[ \expec_{a \sim \nu} [Q_\theta(s, a)] - \expec_{a \sim \mu}[Q_\theta(s, a)] \right] + \mathcal{R}(\nu)
\end{equation}
We provide two examples that are of special interest in our setting. When $\mathcal{R}(\nu)$ is chosen to be the Shannon entropy, $\mathcal{H}(\nu)$, we obtain the following objective:
\begin{equation}
\label{eqn:logsumexp_obj}
    \ell_{\mathcal{H}}(\theta) = \sum_{i=1}^N \left( Q_\theta(s_i, a_i) - \bellman^\pi \bar{Q}(s_i, a_i) \right)^2 + \beta \cdot \expec_{s \sim \mu}\left[\log \left( \int_{a} \exp(Q_\theta) \right) - \expec_{a' \sim \mu}[Q_\theta]\right]
\end{equation}
When $\mathcal{R}(\nu) = -\kldiv(\nu || \hat{\mu})$, where $\hat{\mu}$ denotes the empirical distribution of actions at a state, i.e. $\hat{\mu}(a_0|s_0) = \sum_{s_i, a_i \in \mathcal{D}} 1\{s_i = s_0, a_i = a_0 \} / \sum_{s_i} 1\{s_i = s_0\}$, then using the result from \citet{duchi2017variance}, we obtain the following variance-regularized objective:
\begin{equation}
\label{eqn:variance_regularized}
    \ell_{\mathcal{\kldiv(\hat{\mu})}}(\theta) = \sum_{i=1}^{|\mathcal{D}|} \left( Q_\theta(s_i, a_i) - \bellman^\pi \bar{Q}(s_i, a_i) \right)^2 + \beta \cdot \expec_{s \sim \mu} \sqrt{\frac{\var_{\hat{\mu}}\left( Q_\theta(s, \cdot) \right)}{|\mathcal{D}|}}
\end{equation}
We discuss several more objectives of the family arising from different choices of $\mathcal{R}$ in Appendix ??.
%%SL.4.25: Organizationally, it's getting a little hard to see where this is going. Maybe we should present the method more gradually: first the max/min, then the regularizer, then the "positive" term?

Next, we present our argument to show that our objective $\mathsf{Penalized}(\nu)$ solves the problem of incorrect generalization
%%SL.4.25: "incorrect generalization" seems like a huge sweeping claim -- can we scope this more precisely?
in Q-functions, building on the argument from the previous section. As before, we compute an expression for Q-values after a gradient update.
% We provide an explanation for the objective $\ell_\mathcal{H}(\theta)$ shown in Equation~\ref{eqn:logsumexp_obj}, but the explanation similarly follows for $\ell_{\mathcal{\kldiv(\hat{\mu})}}(\theta)$. 
% We first compute the gradient update for the Q-function parameters.
% \begin{equation}
%     \theta_{k+1} = \theta_{k+1}^{\text{FQI}} - \alpha_k \cdot \beta \cdot \expec_{s \sim \mu}\left[ \int_{a} (\nu(a|s) - \mu(a|s)) \nabla_\theta Q_\theta(s, a) \right]
% \end{equation}
% where $\theta_{k+1}^{\text{FQI}}$ is the value of $\theta_{k+1}$ from Equation~\ref{eqn:gradient_vanilla}. Now, we can compute the next iterate of Q-values as a result of function approximation:
% \begin{equation}
% \label{eqn:new_gradient_update}
%     Q_{k+1} = Q_k + \alpha_k K_\theta \text{diag}(\mu) (\bellman^\pi \bar{Q}_k - Q_k) + \alpha_k \beta K_\theta \text{diag}(\mu(s) \times (\mu(a|s) - \nu(a|s)))\mathbf{1}
% \end{equation}
For a particular state-action pair, $(\bar{s}, \bar{a})$, we can write:
\begin{equation}
\label{eqn:new_kernelized_update}
    Q_{k+1}(\bar{s}, \bar{a}) = Q_{k+1}^{\text{FQI}}(\bar{s}, \bar{a}) + \alpha_k \cdot \beta \cdot \expec_{s \sim \mu} [(\mu(a|s) - \nu(a|s)) \cdot k_\theta((s, a), (\bar{s}, \bar{a}))]
\end{equation}
%%SL.4.25: I feel like this discussion should be somewhere else, this seems like analysis, whereas the rest of this section is explaining the algorithm...
We now provide an explanation centered around objective $\ell_\mathcal{H}(\theta)$ shown in Equation~\ref{eqn:logsumexp_obj}, but an explanation similarly follows for $\ell_{\mathcal{\kldiv(\hat{\mu})}}(\theta)$. If action $\bar{a}$ is out-of-distribution at state $\bar{s}$, an overestimation error that appears in $Q_{k+1}^{\text{FQI}}$ now gets nullified by an underestimation bias caused by $\mu(\bar{a}|\bar{s}) - \nu(\bar{a}|\bar{s})$, which will be a negative quantity if Q-value $Q_k(\bar{s}, \bar{a})$ is overestimated (since, $\nu(\bar{a}|\bar{s}) > \mu(\bar{a}|\bar{s})$). By suitably choosing weights $\beta$, we can ensure that Q-values are not high
%%SL.4.25: ``not high'' seems rather vague
for out-of-distribution actions. 

% Moreover, for an OOD state, for which $\mu(s) < 0$, 
% For any state-action tuple $(s, a)$ such that $k_\theta((s, a), (\bar{s}, \bar{a}))$ is positive and high in magnitude, we note that the change in the Q-value, $Q_{k+1}(\bar{s}, \bar{a})$ now does not just depend on the Bellman error: $\bellman^\pi \bar{Q}(s, a) - Q_\theta(s, a)$, but is offset by a difference: $\beta \cdot (\mu(a|s) - \nu(a|s))$. If the action $a$ is out-of-distribution, and the corresponding Q-value, $Q(s, a)$ is high, i.e. when $\mu(a|s) \leq \nu(a|s)$ holds, Equation~\ref{eqn:new_kernelized_update} reduces the contribution of $(s, a)$ to overestimation in $Q_{k+1}(\bar{s}, \bar{a})$ by adding a highly negative quantity $(\mu(a|s) - \nu(a|s))$ to the Taylor expansion. If action $a$ is out-of-distribution, but the corresponding Q-value is too small, i.e. $\mu(a|s) - \nu(a|s)$ is small in magnitude, then \textcolor{red}{AK: finish and say something about in distribution actions, and how those actions can be used for generalization}

%%SL.4.25: Put this in a separate section?
Finally, we present our practical actor-critic algorithm, ALGONAME, that uses Equation~\ref{eqn:logsumexp_obj} for learning effective policies for offline RL. Our algorithm is shown in Algorithm ??. It utilizes the framework of the soft-actor critic (SAC) algorithm, with the difference that the Q-function is trained via $\ell_\mathcal{H}(\theta)$ instead of simple Bellman error. The actor update is void of any special constraint, since the Q-function is trained to output smaller values for OOD actions.    

% Instead, our approach, which we refer to as ALGONAME, chooses to penalize the relative advantage of an action in the Q-function, thus ensuring that the Bellman error minimization procedure does not erroneously downweight the Q-values on in-distribution actions, due to parameter sharing and function approximation. More concretely, our Q-learning objective, now corresponds to adding an additional penalty shown in Equation~\ref{eqn:penalized}, giving rise to the objective in Equation~\ref{eqn:new_update}.
% \begin{align}
%     \label{eqn:penalized}
%     \Delta_{\theta}(s) = & \log \left( \int_{a} \exp(Q_\theta(s, a)) \right) - \expec_{a' \sim \mu(\cdot|s)}[Q_\theta(s, a')] \\
%     \min_{\theta}~~ & \expec_{s \sim \mu(s)}[\Delta_{\theta}(s)] + \sum_{i=1}^N \left( Q_\theta(s_i, a_i) - \bellman^\pi \bar{Q}(s_i, a_i) \right)^2
%     \label{eqn:new_update}
% \end{align}
% Minimizing $\Delta_\theta(s)$ ensures that the difference in the Q-values for an in-distribution action sampled from the data distribution $\mu(\cdot|s)$ 

\subsection{Lower-Bound Argument}
% TODO (aviral): break this section into theorems!
While our motivation for optimization problems of the form of $\mathsf{Penalized}(\nu)$ was primarily based on considerations arising due to undesirable effects of function approximation,
%%SL.4.25: I think this "undesirable effects of function approximation" feels like we're writing the DisCor paper again :) can we try to use more specific phrasing for these parts?
in this section, we show that in settings without function approximation, ALGONAME that trains
%%SL.4.25: "ALGONAME that trains" seems like awkward phrasing
Q-functions by optimizing $\ell_\mathcal{H}(\theta)$ and then using the corresponding Boltzmann policy for evaluation, gives rise to the strongest lower bound on the policy optimization objective.
%%SL.4.25: strongest relative to what? though perhaps that's not even so important -- just arguing it is a lower bound seems fine, then afterwards discuss which is the stronger bound

First, note that the optimal solution to minimizing $\ell_\mathcal{H}$ at iteration $k$ of learning gives rise to the following Q-function iterate $Q_{k+1}$:
\begin{equation}
    \label{eqn:Q_iterate_tabular}
    Q_{k+1} = \bellman^{\pi_k} Q_{k} - \beta \cdot \left[\frac{\nu_k(a|s) - \mu(a|s)}{n(a|s)} \right]_{|S| \times |A|}
\end{equation}
%%SL.4.25: put comma at end of equation; is the above statement obvious? is it proven somewhere?
where $n(a|s)$ denotes the number of times action $a$ is observed at state $s$ in dataset $\mathcal{D}$. Furthermore, note that $\nu_k(\cdot|s) = \pi_k(\cdot|s)$, since $\ell_{\mathcal{H}}$ is the chosen objective. Note also that the objective for training the actor via a gradient step
%%SL.4.25: the objective doesn't prescribe how to train, so the phrase "objective for training the actor via a gradient step" sounds really weird
is given by $J_k(\pi) = \expec_{s \sim \mu(s)} \left[ \expec_{a \sim \pi_k(\cdot|s)}[Q_{k+1}(s, a) \right]$. By substituting Equation~\ref{eqn:Q_iterate_tabular} into this expression, we obtain
\begin{equation}
    J_k(\pi) = \expec_{s \sim \mu(s)} \left[ \expec_{a \sim \pi_k(\cdot|s)}[\bellman^{\pi_k} Q_k(s, a) - \beta (\pi_k(a|s) - \mu(a|s)) \right]
\end{equation}
%%SL.4.25: put period at end of equation
Since the inner product of two vectors $\pi_k$, and $\mu$ cannot be more than the norm of $\pi_k$, we observe that $J_k(\pi) \leq \expec_{s \sim \mu(s)} \left[ \expec_{a \sim \pi_k(\cdot|s)}[\bellman^{\pi_k} Q_k(s, a)] \right] = \expec_{s \sim \mu(s)} \left[ \expec_{a \sim \pi_k(\cdot|s)}[Q_{k+1}(s, a)] \right]$, where the right hand side denotes the objective for policy improvement at iteration $k$ without any constraint. Note that this condition is strong, in that if $\nu_k(a|s) \neq \pi_k(a|s)$, then $J_k(\pi)$ may not be a lower bound for atleast one choice of the dataset distribution $\mu$.

Finally, we note that prior methods that penalize the target values instead incur an additional penalty due to the conservative estimates of target values used for backups. We discuss this in detail for two methods: BEAR and BRAC in Appendix ??.

%%SL.4.25: maybe add some discussion to summarize the implications of this
