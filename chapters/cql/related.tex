
\subsection{Related Work}
\label{sec:related}
% \vspace{-8pt}

We now briefly discuss prior work in offline RL and off-policy evaluation, comparing and contrasting these works with our approach. 
% More technical discussion of related work is provided in Appendix~\ref{sec:extended_related_work}.

\textbf{Off-policy evaluation (OPE).} Several different paradigms have been used to perform off-policy evaluation. Earlier works~\citep{precup2000eligibility,peshkin2002learning,precup2001off} used per-action importance sampling on Monte-Carlo returns to obtain an OPE return estimator. Recent approaches~\citep{liu2018breaking,gelada2019off,nachum2019dualdice,Zhang2020GenDICE:} use marginalized importance sampling by directly estimating the state-distribution importance ratios via some form of dynamic programming~\citep{levine2020offline} and typically exhibit less variance than per-action importance sampling at the cost of bias. Because these methods use dynamic programming, they can suffer from OOD actions
% this is already introduced in background before this section.
~\citep{levine2020offline,gelada2019off,hallak2017consistent,nachum2019dualdice}. In contrast, the regularizer in CQL explicitly addresses the impact of OOD actions due to its gap-expanding behavior, and obtains conservative value estimates.

\textbf{Offline RL.} Prior works have attempted to solve this problem by constraining the learned policy to be ``close'' to the behavior policy, for example as measured by  KL-divergence~\citep{jaques2019way,wu2019behavior,peng2019awr,siegel2020keep}, Wasserstein distance~\citep{wu2019behavior}, or MMD~\citep{kumar2019stabilizing}, and then 
only using actions sampled from this constrained policy in the Bellman backup or applying a value penalty. SPIBB~\citep{laroche2017safe,nadjahi2019safe} methods bootstrap using the behavior policy in a Q-learning algorithm for unseen actions. Most of these methods require a separately estimated model to the behavior policy, $\behavior(\mathbf{a}|\bs)$~\citep{fujimoto2018off,kumar2019stabilizing,wu2019behavior,jaques2019way,siegel2020keep,simao2019safe}, and {are thus limited by their ability to accurately estimate the unknown behavior policy~\citep{nair2020accelerating}, which might be especially complex in settings where the data is collected from multiple sources~\citep{levine2020offline}. In contrast, CQL does not require estimating the behavior policy.} Prior work has explored some forms of Q-function penalties~\citep{hester2018deep,vecerik2017leveraging}, but only in the standard online RL setting with demonstrations. \citet{luo2019learning} learn a conservatively-extrapolated value function by enforcing a linear extrapolation property over the state-space, and a learned dynamics model to obtain policies for goal-reaching tasks. \citet{kakade2002approximately} proposed the CPI algorithm, that improves a policy conservatively in online RL.

Alternate prior approaches to offline RL estimate some sort of uncertainty to determine the trustworthiness of a Q-value prediction~\citep{kumar2019stabilizing,agarwal2019optimistic,levine2020offline}, typically using uncertainty estimation techniques from exploration in online RL~\citep{osband2016deep,jaksch2010near,osband2017posterior,burda2018exploration}. These methods have not been generally performant in offline RL~\citep{fujimoto2018off,kumar2019stabilizing,levine2020offline} due to the high-fidelity requirements of uncertainty estimates in offline RL~\citep{levine2020offline}. Robust MDPs~\citep{iyengar2005robust,ghavamzadeh2016safe,tamar2014scaling,nilim2004robustness} have been a popular theoretical abstraction for offline RL, but tend to be highly conservative in policy improvement. We expect CQL to be less conservative since CQL does not underestimate Q-values for all state-action tuples. Works on high confidence policy improvement~\citep{thomas2015high} provides safety guarantees for improvement but tend to be conservative. The gap-expanding property of CQL backups, shown in Theorem~\ref{thm:gap_amplify}, is related to how gap-increasing Bellman backup operators~\citep{bellemare2016increasing,lu2018general} are more robust to estimation error in online RL. 

%%AK.08.16: added this result to indicate what's different with theoretical results.
\textbf{Theoretical results.} 
Our theoretical results (Theorems~\ref{thm:well_defined_obj}, \ref{thm:zeta_safe}) are related to prior work on safe policy improvement~\citep{laroche2017safe,ghavamzadeh2016safe}, and a direct comparison to Theorems 1 and 2 in \citet{laroche2017safe} suggests similar quadratic dependence on the horizon and an inverse square-root dependence on the counts. Our bounds improve over the $\infty$-norm bounds in \citet{ghavamzadeh2016safe}. 
% Prior analyses have also focused on error propagation in approximate dynamic programming~\citep{farahmand2010error,chen2019information,xie2020q,scherrer2014approximate}, and generally obtain bounds in terms of concentrability coefficients that capture the effect of distribution shift.
% Our theoretical results (Theorems~\ref{thm:well_defined_obj}, \ref{thm:zeta_safe}) use terminology similar to
% %%SL.8.17: Is our theory identical to Laroche? "Analogous" in this context (without any explanation) would probably be interpreted by most readers as "our theorems are copied from this prior paper"
% %%AK.8.18: Added terminology similar to
% Theorems 1 and 2 in \citet{laroche2017safe}. A direct comparison of the bounds suggests similar quadratic dependence on the horizon and an inverse square-root dependence on the counts. Our results are stronger than \citet{ghavamzadeh2016safe}, which only provides bounds that depend on $\infty-$norm. }
%%SL.8.17: This last sentence is hard to understand -- what is the first of its kind? How is it obtained? Is this explained anywhere?
%%AK.8.18: I changed it to say that a similar analysis can also be usedf for policy constraint methods
%%SL.8.18: Here is a suggested way we could rewrite this paragraph:
% Our theoretical results (Theorems~\ref{thm:well_defined_obj}, \ref{thm:zeta_safe}) are related to prior work that studies lower bounds on value functions~\citep{laroche2017safe,ghavamzadeh2016safe}, and a direct comparison to \citet{laroche2017safe} suggests similar quadratic dependence on the horizon and an inverse square-root dependence on the counts. However [something about how it's different]
%%SL.8.18: I recommend omitting the last sentence (it's not saying anything prior work), not picking on Petrik et al directly, but stating clearly and directly how our result is distinct.

%%%%%%%%%%%%%%% COMMENTING OUT THE STUFF BELOW %%%%%%%%%%%
%%SL.5.27: What do you think of just cutting this paragraph? This would give us the space that we need to get the paper to 8 pages. I still don't think this argument is very convincing, and perhaps including it in the main paper will do more harm than good. Basically, there is only circumstantial empirical evidence that this is true, and it's too easy for reviewers to disagree with this and use that as the basis to reject the paper. If I had to pick something in the current paper to cut, this would be the least bad thing to remove.
% \textbf{Consequences of the gap-expanding nature of CQL.}
%%SL.5.17: If the right term is gap-increasing, can we just use this throughout?
% gap-increasing has been used in prior work in a different sense. I think calling our thing gap-increasing is incorrect, since we do not increase the action gap = max_{a} Q(s, a) - Q(s, second best action), but rather a modified notion of it (see Thm 3.3). So, we can rename it to even something different, like "OOD action gap" or something, but I would not prefer calling it gap increasing. What do you think?


% In contrast, as we show in Appendix~\ref{app:gap_amplify}, even when policy constraint methods back up corrected targets, any Q-function estimation error can make the value of an OOD action appear erroneously high, which as we show empirically, in certain cases can lead to poor performance. 
% It is unlikely that these erroneous Q-values can then provide any meaningful update signal to the policy. 
% In Appendix~\ref{app:gap_amplify}, we illustrate how the gap-expanding property of CQL mitigates instabilities that afflict prior policy-constraint methods.
%%SL.5.17: Rephrase the last sentence to be more idiomatic, something like this: In Appendix~??, we illustrate how the gap-increasing property of CQL mitigates the instabilities that afflict prior policy constraint methods, on both a toy didactic task and higher-dimensional benchmark domains.
% done
