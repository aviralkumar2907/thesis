\vspace{-0.25cm}
\section{Introduction}
\vspace{-0.25cm}
% motivation: generic RL, and offline RL
Recent advances in reinforcement learning (RL), especially when combined with expressive deep network function approximators, have produced promising results in domains ranging from robotics~\citep{kalashnikov2018qtopt} to strategy games~\citep{alphastar} and recommendation systems~\citep{li2010contextual}. However, applying RL to real-world problems consistently poses practical challenges: in contrast to the kinds of data-driven methods that have been successful in supervised learning~\citep{resnet,bert}, RL is classically regarded as an active learning process, where each training run requires active interaction with the environment. Interaction with the real world can be costly and dangerous, and the quantities of data that can be gathered online are substantially lower than the offline datasets that are used in supervised learning~\citep{imagenet}, which only need to be collected once. Offline RL, also known as batch RL, offers an appealing alternative~\citep{ernst2005tree,fujimoto2018off,kumar2019stabilizing,agarwal2019optimistic,jaques2019way,siegel2020keep,levine2020offline}. Offline RL algorithms learn from large, previously collected datasets, without interaction. This in principle can make it possible to leverage large datasets, but in practice fully offline RL methods pose major technical difficulties, stemming from the distributional shift between the policy that collected the data and the learned policy. This has made current results fall short of the full promise of such methods.

Directly utilizing existing value-based off-policy RL algorithms in an offline setting generally results in poor performance, due to issues with bootstrapping from out-of-distribution actions~\citep{kumar2019stabilizing,fujimoto2018off} and overfitting~\citep{fu2019diagnosing,kumar2019stabilizing,agarwal2019optimistic}. This typically manifests as erroneously optimistic value function estimates.
% motivating the technical approach
If we can instead learn a \emph{conservative} estimate of the value function, which provides a lower bound on the true values, this overestimation problem could be addressed. In fact, because policy evaluation and improvement typically only use the value of the policy, we can learn a less conservative lower bound Q-function, such that only the expected value of Q-function under the policy is lower-bounded, as opposed to a point-wise lower bound.
We propose a novel method for learning such conservative Q-functions via a simple modification to standard value-based RL algorithms. The key idea behind our method is to minimize values under an appropriately chosen distribution over state-action tuples, and then further tighten this bound by also incorporating a \emph{maximization} term over the data distribution.
%%SL.5.2: See the language I used in the abstract to explain this lower bound stuff. I think we could describe the nuance (that we lower bound the expected return of a policy, vs lower bounding the whole Q-function) quite concisely here using similar language.
%% added that

% last para, contributions
Our primary contribution is an algorithmic framework, which we call conservative Q-learning (CQL), for learning conservative, lower-bound estimates of the value function, by regularizing the Q-values during training.
%%SL.5.2: I don't think it's accurate to characterize the method as "simply penalizing" -- but maybe see the language I used in the abstract for inspiration?
Our theoretical analysis of CQL shows that \textit{only} the expected value of this Q-function under the policy lower-bounds the true policy value, preventing extra under-estimation that can arise with point-wise lower-bounded Q-functions, that have typically been explored in the opposite context in exploration literature~\citep{osband2017posterior,jaksch2010near}.
%%SL.5.17: The above sentence is a bit problematic, because I think most readers won't understand its significance. Perhaps we can instead say something like: We present a simplified version of our method that can learn point-wise lower bounds, and then extend this approach to provide a tighter bound that only lower-bounds the expected value under the policy, which also provides improved empirical performance.
%%AK.5.22: I would not prefer calling that a simplified version of our method since that seems like a detail. ALso I wanted to refer more generally to exploration methods that learn point-wise lower bounds, rather than just referring to CQL without the data term. I edited it a bit, but if it does not seem fine to you I can also remove this line.
We also empirically demonstrate the robustness of our approach to Q-function estimation error.
%%SL.5.17: I think we could probably remove this entire sentence.
Our practical algorithm uses these conservative estimates for policy evaluation and offline RL. CQL can be implemented with less than \textbf{20} lines of code on top of a number of standard, online RL algorithms~\citep{haarnoja,dabney2018distributional}, simply by adding the CQL regularization terms to the Q-function update. In our experiments, we demonstrate the efficacy of CQL for offline RL, in domains with complex dataset compositions, where prior methods are typically known to perform poorly~\citep{d4rl} and domains with high-dimensional visual inputs~\citep{bellemare2013arcade,agarwal2019optimistic}. CQL outperforms prior methods by as much as \textbf{2-5x} on many benchmark tasks, and is the only method that can outperform simple behavioral cloning on a number of realistic datasets collected from human interaction.
