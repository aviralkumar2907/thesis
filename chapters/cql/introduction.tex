% \vspace{-0.25cm}
% \subsection{Introduction}
% \vspace{-0.25cm}
% motivation: generic RL, and offline RL
%% This is a good para, but we can't use right now
% Recent advances in reinforcement learning (RL), especially when combined with expressive deep network function approximators, have produced promising results in domains ranging from robotics~\citep{kalashnikov2018qtopt} to strategy games~\citep{alphastar} and recommendation systems~\citep{li2010contextual}. However, applying RL to real-world problems consistently poses practical challenges: in contrast to the kinds of data-driven methods that have been successful in supervised learning~\citep{resnet,bert}, RL is classically regarded as an active learning process, where each training run requires active interaction with the environment. Interaction with the real world can be costly and dangerous, and the quantities of data that can be gathered online are substantially lower than the offline datasets that are used in supervised learning~\citep{imagenet}, which only need to be collected once. Offline RL, also known as batch RL, offers an appealing alternative~\citep{ernst2005tree,fujimoto2018off,kumar2019stabilizing,agarwal2019optimistic,jaques2019way,siegel2020keep,levine2020offline}. Offline RL algorithms learn from large, previously collected datasets, without interaction. This in principle can make it possible to leverage large datasets, but in practice fully offline RL methods pose major technical difficulties, stemming from the distributional shift between the policy that collected the data and the learned policy. This has made current results fall short of the full promise of such methods.

As we observed so far, directly utilizing existing value-based off-policy RL algorithms in an offline setting generally results in poor performance, due to issues with bootstrapping from out-of-distribution actions~\citep{kumar2019stabilizing,fujimoto2018off} and overfitting~\citep{fu2019diagnosing,kumar2019stabilizing,agarwal2019optimistic}. This typically manifests as erroneously optimistic value function estimates. If we can instead learn a \emph{conservative} estimate of the value function, which provides a lower bound on the true values, this overestimation problem could be addressed. We propose a novel method for learning such conservative Q-functions via a simple modification to standard value-based RL algorithms. The key idea behind our method is to minimize values under an appropriately chosen distribution over state-action tuples. Our theoretical analysis of conservative Q-learning shows that the expected value of this Q-function under the policy lower-bounds the true policy value. We also empirically validate this observation and demonstrate the robustness of our approach to Q-function estimation error. Our practical algorithm uses these conservative estimates for policy evaluation and offline RL. CQL can be implemented with less than \textbf{20} lines of code on top of a number of standard, online RL algorithms~\citep{haarnoja,dabney2018distributional}, simply by adding the CQL regularization terms to the Q-function update. In our experiments, we demonstrate the efficacy of CQL for offline RL, in domains with complex dataset compositions, where prior methods are typically known to perform poorly~\citep{d4rl} and domains with high-dimensional visual inputs~\citep{bellemare2013arcade,agarwal2019optimistic}. CQL outperforms prior methods by as much as \textbf{2-5x} on many benchmark tasks, and is the only method that can outperform simple behavioral cloning on a number of realistic datasets collected from human interaction.
