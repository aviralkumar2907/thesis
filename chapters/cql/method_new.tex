\vspace{-0.05cm}
\subsection{The Conservative Q-Learning (CQL) Framework}
\vspace{-0.05cm}

In this section, we develop a model-free conservative Q-learning algorithm, such that the expected value of a policy under the learned Q-function lower-bounds its true value. Lower-bounded Q-values prevent the over-estimation that is common in offline RL settings due to OOD actions and function approximation error~\citep{levine2020offline,kumar2019stabilizing}. {We use the term CQL to refer broadly to both Q-learning and actor-critic methods.} We start with the policy evaluation step in CQL, which can be used by itself as an off-policy evaluation procedure, or integrated into a complete offline RL algorithm, as we will discuss in Section~\ref{sec:framework}.

\subsection{Conservative Off-Policy Evaluation}
\label{sec:policy_eval}

We aim to estimate the value $V^\policy(\bs)$ of a target policy $\policy$ given access to a dataset, $\mathcal{D}$, generated by following a behavior policy ${\behavior}(\mathbf{a} | \bs)$. Because we are interested in preventing overestimation of the policy value, we learn a \textit{conservative}, lower-bound Q-function by additionally minimizing Q-values alongside a standard Bellman error objective. Our choice of penalty is to minimize the expected Q-value under a particular distribution of state-action pairs, $\mu(\bs, \mathbf{a})$. Since standard Q-function training does not query the Q-function value at unobserved states, but queries the Q-function at unseen actions, we restrict $\mu$ to match the state-marginal in the dataset, such that $\mu(\bs, \mathbf{a}) = d^\behavior(\bs) \mu(\mathbf{a}|\bs)$. This gives rise to the iterative update for training the Q-function:

\begin{equation}
    \small{\hat{Q}^{k+1} \leftarrow \arg\min_{Q}~ \alpha~ \E_{\bs \sim \mathcal{D}, \mathbf{a} \sim \mu(\mathbf{a}|\bs)}\left[Q(\bs, \mathbf{a})\right] + \frac{1}{2}~ \E_{\bs, \mathbf{a} \sim \mathcal{D}}\left[\left(Q(\bs, \mathbf{a}) - \hat{\bellman}^\policy \hat{Q}^{k} (\bs, \mathbf{a}) \right)^2 \right],} 
    \label{eqn:objective_1}
\end{equation}
In Theorem~\ref{thm:min_q_underestimates}, we show that the resulting Q-function, $\hat{Q}^\policy \coloneqq \lim_{k \rightarrow \infty} \hat{Q}^k$, lower-bounds $Q^\policy$ at all $\bs \in \mathcal{D}, \mathbf{a} \in \mathcal{A}$. 
However, we can substantially tighten this bound if we are \textit{only} interested in estimating  $V^\policy(\bs)$. If we only require that the expected value of the $\hat{Q}^\pi$ under $\policy(\mathbf{a}|\bs)$ lower-bound $V^\pi$, we can improve this by introducing an additional Q-value \textit{maximization} term on the data, $\behavior(\mathbf{a}|\bs)$, resulting in (changes from Equation~\ref{eqn:objective_1} in red):
\begin{multline}
    \small{\hat{Q}^{k+1} \leftarrow \arg\min_{Q}~~ \alpha \cdot \left(\E_{\bs \sim \mathcal{D}, \mathbf{a} \sim \mu(\mathbf{a}|\bs)}\left[Q(\bs, \mathbf{a})\right] - \textcolor{red}{\E_{\bs \sim \mathcal{D}, \mathbf{a} \sim \hatbehavior(\mathbf{a}|\bs)}\left[Q(\bs, \mathbf{a})\right]} \right)} \\
    \small{+ \frac{1}{2}~ \E_{\bs, \mathbf{a}, \bs' \sim \mathcal{D}}\left[\left(Q(\bs, \mathbf{a}) - \hat{\bellman}^\policy \hat{Q}^{k} (\bs, \mathbf{a}) \right)^2 \right]}.
    \label{eqn:modified_policy_eval}
\end{multline}
% The resulting Q-iterate is given by: $\hat{Q}^{k+1}(\bs, \mathbf{a}) = \bellman^\pi \hat{Q}^k(\bs, \mathbf{a}) - \alpha \frac{\mu(\mathbf{a}|\bs) - \behavior(\mathbf{a}|\bs)}{\behavior(\bs|\bs)}$.
%%SL.5.17: Again, we don't actually update it this way -- can we save the resulting iterate equation for the theorem (i.e., move the above sentence, and start with In Theorem...)?
% done
In Theorem~\ref{thm:cql_underestimates}, we show that, while the resulting Q-value 
%%SL.5.11: Same comment, let's avoid interleaving these, and also try to avoid big inline equations.
% do you think that one equation is okay, for completeness? I moved the other analysis to later.
%%SL.5.17: I think it's still pretty confusing, makes the reader think that we actually solve for the Q-function this way, which we don't.
$\hat{Q}^{\policy}$ may not be a point-wise lower-bound, we have $\E_{\policy(\mathbf{a}|\bs)}[\hat{Q}^\pi(\bs, \mathbf{a})] \leq V^\pi(\bs)$
 when $\mu(\mathbf{a}|\bs) = \policy(\mathbf{a}|\bs)$. 
%%AK.5.30: deleted the word "always" from here, since sampling error is going to hurt.
% \ak{Similarly to Proposition~\ref{thm:min_q_underestimates}, this result can also be generalized to include sampling error by choosing $\alpha$ to compensate for any potential overestimation.}
%%SL.5.30: Same as before, I would recommend cutting this sentence here and instead putting it next to the corresponding proposition. The reason is that so far, you haven't actually said anything about sampling error, and the reader has no reason to believe this \emph{doesn't} handle sampling error (after all, your equation is written in terms of samples!)
% yeah, done. My reason for putting it here was because, I wanted to remove the claim "for any alpha > 0", but till say something that shows that we are doing offline RL.
Intuitively, since Equation~\ref{eqn:modified_policy_eval} maximizes Q-values under the behavior policy $\hatbehavior$, Q-values for actions that are likely under $\hatbehavior$ might be overestimated, and hence $\hat{Q}^\policy$ may not lower-bound $Q^\policy$ pointwise. 
% and thus the resulting value estimate is \textit{strictly} less conservative as compared to the point-wise lower bound estimate obtained via Equation~\ref{eqn:objective_1}.
While in principle the maximization term can utilize other distributions besides $\hatbehavior(\mathbf{a}|\bs)$, we prove in Appendix~\ref{app:maximizing_distributions} that the resulting value is not guaranteed to be a lower bound for other distribution besides $\hatbehavior(\mathbf{a}|\bs)$, though other distributions besides $\hatbehavior(\mathbf{a}|\bs)$ can still yield a lower bound if the Bellman error is also re-weighted to come from the distribution chosen to maximize the expected Q-value.
%%SL.5.17: I think you added this last sentence because you were concerned about justifying why the new turn should be an expectation under $\behavior$, but this is not obvious to the reader. Maybe add something like: In principle, the maximization term can utilize other distributions besides $\behavior(\mathbf{a}|bs)$. However, we will show in ....
% changed

%%SL.5.17: A major problem with the two equations above is that it is not made explicit how they can actually be estimated. Part of the confusion will stem from the fact that the Bellman error is in expectation under D, but the states and actions for the min/max terms are under d^\beta, and some readers will miss that these are the same! Maybe consider changing one or the other, or making it more explicit in the text how this is estimated?
% Adding in preliminaries is one option, and I did that

~~

\niparagraph{\large{\textbf{Theoretical Analysis}}}

% \ak{ Proposition~\ref{thm:min_q_underestimates} and Theorem~\ref{thm:cql_underestimates} consider the effect of potential over-estimation that occurs as a result of access to only limited number of samples $\mathcal{D}$.} 
% or due to stochastic optimization, but we show how to incorporate such terms into all of our analysis in Appendix~\ref{app:handling_unobserved_actions}. 

\noindent We first note that Equations~\ref{eqn:objective_1} and \ref{eqn:modified_policy_eval} use the empirical Bellman operator, $\hat{\bellman}^\policy$, instead of the actual Bellman operator, $\bellman^\policy$.  Following~\citep{osband2016deep,jaksch2010near,o2018variational}, we use concentration properties of $\hat{\bellman}^\policy$ to control this error. Formally,
for all $\bs, \mathbf{a} \in \mathcal{D}$, with  probability $\geq 1 - \delta$, $|\hat{\bellman}^\policy - \bellman^\policy|(\bs, \mathbf{a}) \leq \frac{C_{r,T, \delta}}{\sqrt{|\mathcal{D}(\bs, \mathbf{a})|}}$, where $C_{r, T, \delta}$ is a constant dependent on the concentration properties (variance) of $r(\bs, \mathbf{a})$ and $\transitions(\bs'|\bs, \mathbf{a})$, and $\delta\in (0, 1)$.
{For simplicity, we assume that $\hatbehavior(\mathbf{a}|\bs) > 0, \forall \mathbf{a} \in \mathcal{A}~, \forall \bs \in \mathcal{D}$. Let $\frac{1}{\sqrt{|\mathcal{D}|}}$ denote a vector of size $|\mathcal{S}| |\mathcal{A}|$ containing square root inverse counts for each state-action pair, except when $\mathcal{D}(\bs, \mathbf{a}) = 0$, in which case the corresponding entry is a very large but finite value $\delta \geq \frac{2 R_{\max}}{1 - \gamma}$.}
% Now, we show that the conservative Q-function learned by iterating Equation~\ref{eqn:objective_1} lower-bounds the true Q-function. Proofs can be found in Appendix~\ref{app:missing_proofs}.

\begin{tcolorbox}[colback=blue!6!white,colframe=black,boxsep=0pt,top=3pt,bottom=5pt]
\begin{theorem} %[Equation~\ref{eqn:objective_1} results in a lower bound]
\label{thm:min_q_underestimates}
For any $\mu(\mathbf{a}|\bs)$ with $\supp \mu \subset \supp \hatbehavior$, with probability $\geq 1 - \delta$, $\hat{Q}^\pi$ (the Q-function obtained by iterating Equation~\ref{eqn:objective_1}) satisifies, $\forall \bs \in \mathcal{D}, \mathbf{a}$:
% That is, $\hat{Q}^k(\bs, \mathbf{a}) \leq Q^k(\bs, \mathbf{a}), \forall \bs, \mathbf{a}$. 
% The resulting asymptotic ($k \rightarrow \infty$) estimate of the policy value satisfies: 
\begin{equation*}
\small{\hat{Q}^\policy(s, a) \leq Q^\policy(\bs, \mathbf{a}) - \alpha \left[\left(I - \gamma P^\pi \right)^{-1} \frac{\mu}{\hatbehavior} \right](\bs, \mathbf{a}) + \left[ (I - \gamma P^\pi)^{-1}\frac{C_{r, T, \delta} R_{\max}}{(1- \gamma) \sqrt{|\mathcal{D}|}} \right](\bs, \mathbf{a}).}    
\end{equation*}
%%AK.08.16: Do we need to exactly write the expression here -- I think the expresison is not so simplifiable and may only seem to confuse people -- but maybe we want to be very explicit? 
Thus, if {$\alpha$ is sufficiently large},
% $\alpha > \frac{C_{r, T} R_{\max}}{1 - \gamma} \cdot \max_{\bs, \mathbf{a} \in \mathcal{D}} \frac{\hatbehavior(\mathbf{a}|\bs)}{\mu(\mathbf{a}|\bs)}$,
then $\hat{Q}^\policy(\bs, \mathbf{a})  \leq Q^\policy(\bs, \mathbf{a}), \forall \bs \in \mathcal{D}, \mathbf{a}$. When $\hat{\bellman}^\policy = \bellman^\policy$, any $\alpha > 0$ guarantees $\hat{Q}^\policy(\bs, \mathbf{a})  \leq Q^\policy(\bs, \mathbf{a}), \forall \bs \in \mathcal{D}, \mathbf{a} \in \mathcal{A}$.
\end{theorem}
\end{tcolorbox}
%%SL.5.30: Why did you choose to make this a proposition rather than a theorem? Maybe theorem makes more sense, so that they are all cleanly numbered, otherwise having two "3.1" might be confusing to readers.
% yeah, earlier it seemed like a trivial result, you are subtracting a positive quantity, obviouslyu it will be lower, but I guess now we can have this be a theorem
Next, we show that Equation~\ref{eqn:modified_policy_eval} lower-bounds the expected value under the policy $\policy$, when $\mu = \policy$. We also show that Equation~\ref{eqn:modified_policy_eval} does not lower-bound the Q-value estimates pointwise. {For this result, we abuse notation and assume that $\frac{1}{\sqrt{|\mathcal{D}|}}$ refers to a vector of inverse square root of only state counts, with a similar correction as before used to handle the entries of this vector at states with zero counts.}

\begin{tcolorbox}[colback=blue!6!white,colframe=black,boxsep=0pt,top=3pt,bottom=5pt]
\begin{theorem}[Equation~\ref{eqn:modified_policy_eval} results in a tighter lower bound]
\label{thm:cql_underestimates}
The value of the policy under the Q-function from Equation~\ref{eqn:modified_policy_eval}, $\hat{V}^\policy(\bs) = \E_{\policy(\mathbf{a}|\bs)}[\hat{Q}^\policy(\bs, \mathbf{a})]$, lower-bounds the true value of the policy obtained via exact policy evaluation, $V^\policy(\bs) = \E_{\policy(\mathbf{a}|\bs)}[Q^\policy(\bs, \mathbf{a})]$, when $\mu = \policy$, according to:
\begin{equation*}
\small{\!\!\hat{V}^\policy(\bs) \leq V^\policy(\bs) - \alpha \left[\left(I - \gamma P^\pi \right)^{-1} \E_{\policy}\left[\frac{\policy}{\hatbehavior} - 1 \right] \right](\bs) + \left[ (I - \gamma P^\pi)^{-1} \frac{C_{r, T, \delta} R_{\max}}{(1- \gamma) \sqrt{|\mathcal{D}|}}\right](\bs).}
\end{equation*}
$\forall \bs \in \mathcal{D}$. Thus, if $\alpha > \frac{C_{r, T} R_{\max}}{1 - \gamma} \cdot \max_{\bs \in \mathcal{D}} \frac{1}{|\sqrt{|\mathcal{D}(\bs)|}} \cdot \left[\sum_{\mathbf{a}} \policy(\mathbf{a}|\bs) (\frac{\policy(\mathbf{a}|\bs)}{\hatbehavior(\mathbf{a}|\bs))} - 1)\right]^{-1}$
,~ $\forall \bs \in \mathcal{D},~\hat{V}^\policy(\bs) \leq {V}^\policy(\bs)$, with probability $\geq 1 - \delta$. When $\hat{\bellman}^\policy = {\bellman}^\policy$, then any $\alpha > 0$ guarantees $\hat{V}^\policy(\bs) \leq V^\policy(\bs), \forall \bs \in \mathcal{D}$.
%%SL.5.30: is it a typo that alpha doesn't appear here?
% I hadn't edited this theorem, only the other one, so as to give a visual comparison of with and without sampling error, but now it does appear
\end{theorem}
\end{tcolorbox}

% Now generalize to function approximation
The analysis presented above assumes that no function approximation is used in the Q-function, meaning that each iterate can be represented exactly.
%%SL.5.17: It's especially critical to explain what you mean by "tabular" above in the statement of the theorems, else readers might misunderstand and think the result is in fact very trivial (i.e., bounding one tabular thing by another)!
% done
We can further generalize the result in Theorem~\ref{thm:cql_underestimates} to the case of both linear function approximators and non-linear neural network function approximators, where the latter builds on the neural tangent kernel (NTK) framework~\citep{ntk}. We present these results in Theorem~\ref{thm:policy_eval_func_approx} and Theorem~\ref{corr:nonlinear_ntk} in Appendix~\ref{app:cql_linear_non_linear}. 
%%SL.5.30: Maybe rewrite as something like this: In Theorem~\ref{thm:policy_eval_func_approx}, we generalize this analysis to linear function approximation. This theorem can be generalized to also include sampling error, and can be extended to arbitrary neural network function approximators by building on the NTK framework~\citep{stuff}. Due to space constraints, we present the more general theorem in Appendix ??, while here we show the result for linear function approximation.
%%SL.5.30: Another possibility is to move all of function approximation analysis to an appendix entirely. I think this would be preferable, because we are short on space, and showing just the linear result without the nonlinear result might lead people to misunderstand think that our thing only works in the linear case. In this case, I would write something like this:
% We can further generalize the result in Theorem~\ref{thm:cql_underestimates} to the case of both linear function approximators and non-linear function approximators, where the latter result builds on the NTK framework~\citep{stuff}. Due to space constraints and the need for additional background, we present the more general theorem in Appendix ??.

\textbf{In summary}, we showed that the basic CQL evaluation in Equation~\ref{eqn:objective_1} learns a Q-function that lower-bounds the true Q-function $Q^\pi$, and the evaluation in Equation~\ref{eqn:modified_policy_eval} provides a \emph{tighter} lower bound on the expected Q-value of the policy $\pi$. For suitable $\alpha$, both bounds hold under sampling error and function approximation. {We also note that as more data becomes available and $|\mathcal{D}(\bs, \mathbf{a})|$ increases, the theoretical value of $\alpha$ that is needed to guarantee a lower bound decreases, which indicates that in the limit of infinite data, a lower bound can be obtained by using extremely small values of $\alpha$.} Next, we will extend on this result into a complete RL algorithm.

%%SL.5.30: If we are out of space, I think we could move this to the appendix.
% \textbf{A note on the value of $\alpha_k$.} 
% In the presence of sampling error or function approximation, standard Bellman backups may overestimate Q-values. 

% We leave analysis of the case of non-linear function approximation for future work. This analysis is complicated by the fact that even convergence of Q-learning methods under non-linear function approximation is not guaranteed, making it difficult to prove convergence to lower bounds. However, we will show empirically in Section~\ref{sec:experiments} that CQL can learn effective conservative Q-functions with multilayer neural networks.
% Our practical algorithm, presented in Section~\ref{sec:practical_alg}, utilizes deep neural networks function approximators. It is challenging to extend this analysis to the deep net setting, since even convergence guarantees for ADP under general function approximation do not hold, but we find empirically that this method performs well in that setting, and consistently learns lower bounds on true values empirically.
%%SL.5.17: Could also phrase it something like this: We leave analysis of the case of non-linear function approximation for future work. This analysis is complicated by the fact that even convergence of Q-learning methods under non-linear function approximation is not guaranteed, making it difficult to prove convergence to lower bounds. However, we will show empirically in Section ?? that CQL can learn effective conservative Q-functions with multilayer neural networks.
%%SL.5.17: Can we show some experiments demonstrating that we are in fact learning a bound? That would also settle some concerns about non-linear function approximation

