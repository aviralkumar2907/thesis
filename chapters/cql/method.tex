\vspace{-5pt}
\section{The Conservative Q-Learning (CQL) Framework}
\vspace{-5pt}
In this section, we develop a conservative Q-learning (CQL) algorithm, such that the expected value of a policy under the learned Q-function lower-bounds its true value. A lower bound on the Q-value prevents the over-estimation that typically appear in in offline RL settings due to OOD actions, access to only limited samples, uncertainty, and function approximation error~\citep{levine2020offline,kumar2019stabilizing}. 
We start by focusing on the policy evaluation step in CQL, which can be used by itself as an off-policy evaluation procedure, or integrated into a complete offline RL algorithm, as we will discuss in Section~\ref{sec:framework}.

\subsection{Conservative Off-Policy Evaluation (OPE)}
\label{sec:policy_eval}
\vspace{-5pt}
% motivate the setup, briefly describe OPE, though this can go in background section
We aim to estimate the value, $V^\policy(\bs)$ of a target policy $\policy$ given access to a dataset, $\mathcal{D}$, generated by following a behavior policy $\behavior(\ba|\bs)$.
Because we are interested in preventing overestimation of the policy value, we learn a \textit{conservative}, lower-bound Q-function by additionally minimizing Q-values alongside a standard Bellman error objective. 

% min Q only, no data subtract
A natural choice of penalty is to minimize the expected Q-value under a particular distribution, $\mu(\bs, \ba)$. Since standard Q-function training does not query the Q-function value at unobserved states, but queries the Q-function at unseen actions, 
we restrict $\mu$ to match the state-marginal in the dataset,
such that $\mu(\bs, \ba) = d^\behavior(\bs) \mu(\ba|\bs)$, and only vary the conditional action distribution\footnote{In Appendix~\ref{app:error_bound_analysis}, we discuss how the policy value can be computed without querying the Q-function at a variable state distribution, justifying the sufficiency of this particular choice of $\mu(\bs, \ba)$.}.
This gives rise to the following objective for training the Q-function, as a function of a tradeoff factor $\alpha$:
\begin{equation}
    \small{\hat{Q}^{k+1} \leftarrow \arg\min_{Q}~ \alpha~ \E_{\bs \sim d^\behavior(\bs), \ba \sim \mu(\ba|\bs)}\left[Q(\bs, \ba)\right] + \frac{1}{2}~ \E_{\bs, \ba \sim d^\behavior(\bs) \behavior(\ba|\bs)}\left[\left(Q(\bs, \ba) - \bellman^\policy \hat{Q}^{k} (\bs, \ba) \right)^2 \right],} 
    \label{eqn:objective_1}
\end{equation}
In Proposition~\ref{thm:min_q_underestimates}, we will show that the resulting Q-value iterates, $\hat{Q}^k$, lower-bound the true Q-value iterate, $Q^k$, obtained by performing each iteration of exact policy evaluation, in the absence of function approximation (i.e., in the tabular setting).
%%SL.5.17: It's not clear what you mean by "tabular policy evaluation" -- do you mean something like "the true iterations $Q^k$ that would be obtained by performing each iteration of exact policy evaluation, without sampling error.
% yes, fixed
As a consequence, the Q-values $\hat{Q}^\policy$ estimated by this procedure lower bound the actual Q-function of the policy, $Q^\policy$. 

% Now talk about why min Q and how we can improve this
While the penalty in Equation~\ref{eqn:objective_1} yields lower-bound estimates of the actual Q-function iterate, thus  
$\E_{\pi}[\hat{Q}(\bs, \ba)] \leq \E_{\pi}[Q(\bs, \ba) ] = V^\pi(\bs)$, 
this bound can be substantially tightened. Equation~\ref{eqn:objective_1} results in uniform underestimates of the value for any state.
% $\frac{\policy(\ba|\bs)}{\behavior(\ba|\bs)} \cdot K$, which is \textit{linear} in the number of iterations, $K$, and increases with every iteration which could yield highly underestimated values
Fortunately, since we are \textit{only} interested in estimating the value of the policy $\policy(\ba|\bs)$, we can obtain a much tighter conservative Q-function by instead only requiring the expected value of the Q-function under $\policy(\ba|\bs)$ to lower-bound its true expected value. We can obtain such an estimate by introducing an additional Q-value \textit{maximization} term under the data distribution, $\behavior(\ba|\bs)$, resulting in the objective (changes from Equation~\ref{eqn:objective_1} in red):
\begin{multline}
    \small{\hat{Q}^{k+1} \leftarrow \arg\min_{Q}~~ \alpha \cdot \left(\E_{\bs \sim d^\behavior(\bs), \ba \sim \mu(\ba|\bs)}\left[Q(\bs, \ba)\right] - \textcolor{red}{\E_{\bs \sim d^\behavior(\bs), \ba \sim \behavior(\ba|\bs)}\left[Q(\bs, \ba)\right]} \right)} \\
    \small{+ \frac{1}{2}~ \E_{\bs, \ba, \bs' \sim \mathcal{D}}\left[\left(Q(\bs, \ba) - \bellman^\policy \hat{Q}^{k} (\bs, \ba) \right)^2 \right]}.
    \label{eqn:modified_policy_eval}
\end{multline}
% The resulting Q-iterate is given by: $\hat{Q}^{k+1}(\bs, \ba) = \bellman^\pi \hat{Q}^k(\bs, \ba) - \alpha \frac{\mu(\ba|\bs) - \behavior(\ba|\bs)}{\behavior(\bs|\bs)}$.
%%SL.5.17: Again, we don't actually update it this way -- can we save the resulting iterate equation for the theorem (i.e., move the above sentence, and start with In Theorem...)?
% done
In Theorem~\ref{thm:cql_underestimates}, we show that the resulting Q-value, 
%%SL.5.11: Same comment, let's avoid interleaving these, and also try to avoid big inline equations.
% do you think that one equation is okay, for completeness? I moved the other analysis to later.
%%SL.5.17: I think it's still pretty confusing, makes the reader think that we actually solve for the Q-function this way, which we don't.
$\hat{Q}^{\policy}$ may not lower-bound ${Q}^\policy$ at each state-action pair, but the expected Q-value under $\policy$, $\E_{\policy(\ba|\bs)}[Q(\bs, \ba)]$,
is always lower-bounded, when $\mu(\ba|\bs) = \policy(\ba|\bs)$. Intuitively, since Equation~\ref{eqn:modified_policy_eval} maximizes Q-values under the dataset action distribution $\behavior$, Q-values for actions with higher density under $\behavior$ might overestimated, and thus the resulting value estimate is \textit{strictly} less conservative as compared to the point-wise lower bound estimate obtained via Equation~\ref{eqn:objective_1}. We also remark that, while in principle, the maximization term can utilize other distributions, besides the behavior policy $\behavior(\ba|\bs)$, however, we prove in Appendix~\ref{app:maximizing_distributions} that the resulting value estimate need not be a lower bound, if any other distribution besides $\behavior(\ba|\bs)$ is used.
%%SL.5.17: I think you added this last sentence because you were concerned about justifying why the new turn should be an expectation under $\behavior$, but this is not obvious to the reader. Maybe add something like: In principle, the maximization term can utilize other distributions besides $\behavior(\ba|bs)$. However, we will show in ....
% changed

%%SL.5.17: A major problem with the two equations above is that it is not made explicit how they can actually be estimated. Part of the confusion will stem from the fact that the Bellman error is in expectation under D, but the states and actions for the min/max terms are under d^\beta, and some readers will miss that these are the same! Maybe consider changing one or the other, or making it more explicit in the text how this is estimated?
% Adding in preliminaries is one option, and I did that

\textbf{Theoretical analysis.} We now present the lower-bound results discussed above. For clarity, our results below do not consider potential over-estimatation that occurs as a result of access to only limited number of samples $\mathcal{D}$, or due to stochastic optimization, but we show how to incorporate such terms into all of our analysis in Appendix~\ref{app:handling_unobserved_actions}. Here, we first show that the Q-function learned by optimizing Equation~\ref{eqn:objective_1} lower-bounds the true Q-function. All proofs can be found in Appendix~\ref{app:missing_proofs}.
\begin{proposition}[Equation~\ref{eqn:objective_1} results in a lower bound]
\label{thm:min_q_underestimates}
For any $k \in \mathbb{N}$ and any $\mu(\ba|\bs)$, the Q-function iterate obtained from Equation~\ref{eqn:objective_1}, $\hat{Q}^k$, lower-bounds the actual Q-function iterate, $Q^k$, obtained by running exact policy evaluation.
That is, $\hat{Q}^k(\bs, \ba) \leq Q^k(\bs, \ba), \forall \bs, \ba$. 
The resulting asymptotic ($k \rightarrow \infty$) estimate of the policy value satisfies: 
\begin{equation*}
\forall \bs, \ba, ~~ \hat{Q}^\policy(s, a) = Q^\policy(\bs, \ba) - \left(I - \gamma P^\pi \right)^{-1} \left[\frac{\mu(\ba|\bs)}{\behavior(\ba|\bs)} \right](\bs, \ba) \leq Q^\policy(\bs, \ba).    
\end{equation*}
\end{proposition}

Next, we show that Equation~\ref{eqn:modified_policy_eval} lower-bounds the expected value under the policy $\policy$, when $\mu = \policy$. We further show that Equation~\ref{eqn:modified_policy_eval} does not lower-bound the Q-value estimates pointwise.
\begin{theorem}[Equation~\ref{eqn:modified_policy_eval} results in a tighter lower bound]
\label{thm:cql_underestimates}
For any $k \in \mathbb{N}$, the value of the policy under the Q-function iterate from Equation~\ref{eqn:modified_policy_eval}, $\hat{V}^k = \E_{\policy(\ba|\bs)}[\hat{Q}^k(\bs, \ba)]$, lower-bounds the true value of the policy obtained via exact policy evaluation, $V^k = \E_{\policy(\ba|\bs)}[Q^k(\bs, \ba)]$ when $\mu = \policy$, 
% i.e. $\hat{V}^k(\bs) := \E_{\ba \sim \policy(\ba|\bs)}[\hat{Q}^k(\bs, \ba)] \leq \E_{\ba \sim \policy(\ba|\bs)}[Q^k(\bs, \ba)] := V^k(\bs)$,
and the resulting asymptotic ($k \rightarrow \infty$) estimate of the policy value satisfies: 
\begin{equation*}
\forall \bs, ~~ \hat{V}^\policy(\bs) = V^\policy(\bs) - \left(I - \gamma P^\pi \right)^{-1} \E_{\policy(\ba|\bs)}\left[\frac{\policy(\ba|\bs)}{\behavior(\ba|\bs)} - 1 \right](\bs) \leq V^\policy(\bs).    
\end{equation*}
\end{theorem}


% Now generalize to function approximation
The analysis presented above assumes that no function approximation is used in the Q-function, meaning that each iterate can be represented exactly.
%%SL.5.17: It's especially critical to explain what you mean by "tabular" above in the statement of the theorems, else readers might misunderstand and think the result is in fact very trivial (i.e., bounding one tabular thing by another)!
% done
In Theorem~\ref{thm:policy_eval_func_approx}, we generalize this analysis to linear function approximation, where the amount of underestimation is state-dependent.
\begin{theorem}
\label{thm:policy_eval_func_approx}
Assume that the Q-function is represented as a linear function of given state-action feature vectors $\Qfeat$, i.e., $Q(s, a) = w^T \Qfeat(s, a)$. Let $D = \text{diag}\left(d^\behavior(\bs) \behavior(\ba|\bs)\right)$ denote the diagonal matrix with data density, and assume that $\Qfeat^T D \Qfeat$ is invertible. Then, the expected value of the policy under Q-value from Eqn~\ref{eqn:modified_policy_eval} at iteration $k+1$, $\E_{d^\behavior(\ba)}[\hat{V}^{k+1}(\bs)] = \E_{d^\behavior(\bs), \policy(\ba|\bs)}[\hat{Q}^{k+1}(\bs, \ba)]$, lower-bounds the corresponding tabular value, $\E_{d^\behavior(\bs)}[V^{k+1}(\bs)] =\E_{d^\behavior(\bs), \policy(\ba|\bs)}[Q^{k+1}(\bs, \ba)]$, if
\begin{equation*}
\small{\alpha_k \geq \max \left(\frac{D^T\left[\Qfeat \left(\Qfeat^T D \Qfeat \right)^{-1} \Qfeat^T - I \right]\left((\bellman^\policy \hat{Q}^k)(\bs, \ba)\right)}{D^T\left[ \Qfeat \left(\Qfeat^T D \Qfeat \right)^{-1} \Qfeat^T \right] \left(D \left[\frac{\policy(\ba|\bs) - \behavior(\ba|\bs)}{\behavior(\ba|\bs)} \right] \right)},~ 0 \right).}
    %  \small{\alpha_k \geq \max \left(\frac{\left(d^\behavior(\bs) \mu(\ba|\bs)\right)^T\left[\Qfeat \left(\Qfeat^T D \Qfeat \right)^{-1} \Qfeat^T - I \right]\left((\bellman^\policy \hat{Q}^k)(\bs, \ba)\right)}{\left(d^\behavior(\bs) \mu(\ba|\bs)\right)^T\left[ \Qfeat \left(\Qfeat^T D \Qfeat \right)^{-1} \Qfeat^T \right] \left(d^\behavior(\bs) \left( \mu(\ba|\bs) - \behavior(\ba|\bs) \right) \right)},~ 0 \right).}
\end{equation*}
\end{theorem}
%intuitive justification for the theorem
The choice of $\alpha_k$ in Theorem~\ref{thm:policy_eval_func_approx} intuitively amounts to compensating for overestimation in value induced if the true value function cannot be represented in the chosen linear function class (numerator), by the potential decrease in value due to the CQL regularizer (denominator). This implies that if the actual value function can be represented in the linear function class, such that the numerator can be made $0$, then any $\alpha > 0$ is sufficient to obtain a lower bound. We prove this result in Appendix~\ref{app:missing_proofs}. 
% Also note that if $\Qfeat^T D \Qfeat$ is non-invertible -- that is, it has incomplete support -- we can choose $\alpha_k = 0$, as we discuss in Appendix~\ref{app:missing_proofs}.
%%SL.5.17: Fill in above appendix reference.
%%SL.5.17: If any alpha > 0 will work when the true Q-function can be represented, we should definitely state that also!
% done

We can generalize Theorem~\ref{thm:policy_eval_func_approx} to non-linear function approximation, such as neural networks, assuming that each iteration $k$ is performed by a single step of gradient descent on Equation~\ref{eqn:modified_policy_eval}, rather than a complete minimization of this objective. As we show in Corollary~\ref{corr:nonlinear_ntk}, by choosing $\alpha_k$ suitably, CQL learns provably lower-bounded values. We will also empirically show in Appendix~\ref{app:additional_results} that CQL can learn effective conservative Q-functions with multilayer neural networks.

% We leave analysis of the case of non-linear function approximation for future work. This analysis is complicated by the fact that even convergence of Q-learning methods under non-linear function approximation is not guaranteed, making it difficult to prove convergence to lower bounds. However, we will show empirically in Section~\ref{sec:experiments} that CQL can learn effective conservative Q-functions with multilayer neural networks.
% Our practical algorithm, presented in Section~\ref{sec:practical_alg}, utilizes deep neural networks function approximators. It is challenging to extend this analysis to the deep net setting, since even convergence guarantees for ADP under general function approximation do not hold, but we find empirically that this method performs well in that setting, and consistently learns lower bounds on true values empirically.
%%SL.5.17: Could also phrase it something like this: We leave analysis of the case of non-linear function approximation for future work. This analysis is complicated by the fact that even convergence of Q-learning methods under non-linear function approximation is not guaranteed, making it difficult to prove convergence to lower bounds. However, we will show empirically in Section ?? that CQL can learn effective conservative Q-functions with multilayer neural networks.
%%SL.5.17: Can we show some experiments demonstrating that we are in fact learning a bound? That would also settle some concerns about non-linear function approximation

\vspace{-5pt}
\subsection{Conservative Q-Learning for Offline RL}
\label{sec:framework}

% Define the optimiziation problem
We now build on the ideas in Section~\ref{sec:policy_eval}
to develop a general approach for offline policy learning, which we refer to as conservative Q-learning (CQL). 
As discussed in Section~\ref{sec:policy_eval}, we can obtain Q-value estimates such that value of a policy $\policy$
%%SL.5.17: but we don't use the "every state" version?
% it is the same version actually, but I removed the term "every state" from the above sentence
is lower-bounded by solving Equation~\ref{eqn:modified_policy_eval} with $\mu = \policy$. How should we utilize this for policy optimization? In principle, we could alternate between performing full off-policy evaluation for each policy iterate, and one step of policy improvement. However, this can be computationally expensive and may limit the algorithm performance due to Q-function overfitting~\citep{fu2019diagnosing}. Alternatively, since the policy $\policy$ is typically derived from the Q-function, either via greedy maximization, 
or via soft maximization, we could instead choose $\mu(\ba|\bs)$ to approximate the policy that would maximize the current Q-values, giving rise to an online algorithm.
%%SL.5.17: I think the above intuition is not entirely clear. Perhaps we can phrase it more along these lines: we could instead choose $\mu(\ba|\bs)$ to approximate the policy that would maximize the current Q-values, giving rise to an online algorithm.
% done

% Stating the final optimization framework
We can formally capture this intuition by defining a family of optimization problems over the variable $\mu(\ba|\bs)$.
%%SL.5.17: I think you can just delete "such that ... problem" -- of course if it's an optimization over mu then mu is the solution!
% done
This family of optimization problems, with modifications from Equation~\ref{eqn:modified_policy_eval} marked in red, is shown in Equation~\ref{eqn:cql_framework}. An instance of this family is denoted by CQL($\mathcal{R}$) and is characterized by a particular choice of concave regularizer $\mathcal{R}(\mu)$:
\begin{multline}
    \label{eqn:cql_framework}
    \small{\min_{Q} \textcolor{red}{\max_{\mu}}~~ \alpha \left(\E_{\bs \sim d^\behavior(\bs), \ba \sim \textcolor{red}{\mu(\ba|\bs)}}\left[Q(\bs, \ba)\right] - \E_{\bs \sim d^\behavior(\bs), \ba \sim \behavior(\ba|\bs)}\left[Q(\bs, \ba)\right] \right)}\\
    \small{+ \frac{1}{2}~ \E_{\bs, \ba, \bs' \sim \mathcal{D}}\left[\left(Q(\bs, \ba) - \bellman^{\policy_k} \hat{Q}^{k} (\bs, \ba) \right)^2 \right] + \textcolor{red}{\mathcal{R}(\mu)} ~~~ \left(\text{CQL}(\mathcal{R})\right).}
\end{multline}

% provide examples, to build intuition, and mention the one we use in practice
\textbf{Variants of CQL.} To demonstrate the generality of the CQL family of optimization problems, we discuss two specific instances within this family that are of special interest, and we evaluate them empirically in Section~\ref{sec:experiments}. First, if we choose $\mathcal{R}(\mu) = 0$, we obtain \mbox{$\mu(\ba|\bs) = \delta(\ba = \arg\max_{a} {Q}(\bs, \ba))$}, which is the ``greedy" distribution (analogous to a greedy policy) corresponding to $Q$. Second, if we choose $\mathcal{R}(\mu)$ to be the KL-divergence against a prior distribution, $\rho(\ba|\bs)$, i.e., $\mathcal{R}(\mu) = D_{\mathrm{KL}}(\mu, \rho)$, then we obtain $\mu(\ba|\bs) \propto \rho(\ba|\bs) \cdot \exp (Q(\bs, \ba))$ (for a derivation, see Appendix~\ref{app:cql_variants}). If $\rho = \text{Unif}(\ba)$,
then the first term in Equation~\ref{eqn:cql_framework} corresponds to a soft-maximum of the Q-values at any state $\bs$ and gives rise to the following variant of Equation~\ref{eqn:cql_framework}, referred as CQL($\mathcal{H}$):
\begin{equation}
    \small{\min_{Q}~ \alpha \E_{\bs \sim d^\behavior(\bs)}\left[\log \sum_{\ba} \exp(Q(\bs, \ba))\!-\!\E_{\ba \sim \behavior(\ba|\bs)}\left[Q(\bs, \ba)\right]\right]\!+\!\frac{1}{2}\!\E_{\bs, \ba, \bs' \sim \mathcal{D}}\left[\left(Q - \bellman^{\policy_k} \hat{Q}^{k} \right)^2 \right]\!.}
    \label{eqn:practical_objective}
\end{equation}
On the other hand, if $\rho(\ba|\bs)$ is chosen to be the behavior policy $\behavior$ or the previous policy $\policy^{k-1}$, the first term in Equation~\ref{eqn:practical_objective} is replaced by an exponential weighted average of Q-values of actions drawn from the chosen $\rho(\ba|\bs)$. Empirically, we find that this variant can be more stable on tasks with high-dimensional action spaces (e.g., Table~\ref{table:adroit_antmaze}) where it is challenging to estimate $\log \sum_{\ba} \exp$ via sampling due to high variance. In Appendix~\ref{app:cql_variants}, we discuss additional variants of CQL, which can be derived via a connection to distributionally robust optimization~\citep{namkoong2017variance}.  

We will discuss a practical instantiation of a deep RL algorithm based on CQL in Section~\ref{sec:practical_alg}. More generally, CQL can be instantiated as either a Q-learning algorithm (with $\bellman^*$ instead of $\bellman^{\policy}$ in Equations~\ref{eqn:cql_framework}, \ref{eqn:practical_objective}) or as an actor-critic algorithm. When Q-learning is used, exact maximization or an approximate maximization scheme, such as CEM~\citep{kalashnikov2018qtopt}, may be used to recover the greedy policy. In an actor-critic algorithm, a separate policy is trained to maximize the Q-value (Section~\ref{sec:background}).

\textbf{Theoretical analysis of CQL.}
Next, we will theoretically analyze CQL to show that the policy updates derived in this way are indeed ``conservative'', in the sense that each successive policy iterate is optimized against a lower bound on its value. Theorem~\ref{thm:cql_underestimation} shows that any instance of the CQL family learns Q-value estimates that lower-bound the actual Q-function under the action-distribution defined by the policy, $\policy^{k}$, under mild regularity conditions (slow updates to the policy).
\begin{theorem}[CQL learns lower-bounded Q-values]
\label{thm:cql_underestimation}
Assume that $D_{\mathrm{TV}}(\policy^{k+1}, \exp(\hat{Q}^k)) \leq \varepsilon$ (i.e., $\policy^k$ changes slowly w.r.t to $Q^k$). Then, the expected Q-value, $\hat{Q}^k$, lower-bounds the expected actual policy Q-value, that is, $\hat{V}^k(\bs) = \E_{ \policy^{k+1}(\ba|\bs)}[\hat{Q}^{k+1}(\bs, \ba)] \leq \E_{\policy^{k+1}(\ba|\bs)}[Q^{k+1}(\bs, \ba)] = V^k(\bs)$ if
\begin{equation*}
\small{\sum_{\ba} \exp(\hat{Q}^k(\bs, \ba)) \left[ \frac{\exp(\hat{Q}^k(\bs, \ba))}{\behavior(\ba|\bs)} -1 \right] \geq \max_{\ba \text{~s.t.~} \behavior(\ba|\bs) > 0} \left(\frac{\exp(\hat{Q}^k(\bs, \ba))}{\behavior(\ba|\bs)} \right) \cdot \varepsilon}. 
\end{equation*}
\end{theorem}
The LHS of the inequality above is equal to amount of underestimation caused in the value, $\hat{V}^{k+1}(s)$ if we the learned policy was equal to soft-optimal policy, $\exp(\hat{Q}^k)$, corresponding to the current Q-function, however, since the actual policy in an actor critic algorithm, $\policy^{k+1}$ may be different from this Boltzmann distribution, the amount of potential overestimation caused due to this difference is shown in the RHS. In order to obtain a lower bound, the amount of underestimation should be higher.  
%%SL.5.27: This is pretty opaque. Is there any way to provide a bit more intuition for what this inequality actually means? Or some way to define this inequality in terms of some variables that we'll define, so that it's simpler? Right now I really can't tell whether it's reasonable to surmise that this inequality actually holds or not.
% done, although it is a bit verbose.

Our next result shows that CQL Q-function update is ``gap-expanding,'' by which we informally mean that the difference in Q-values at in-distribution actions and over-optimistically erroneous out-of-distribution actions is higher than the corresponding difference under the actual Q-function. This is the reason why lower-bounded CQL values can especially help combat the detrimental effects of overestimation caused due to the distribution shift in the action distribution.
\begin{theorem}[CQL is gap-expanding] 
\label{thm:gap_amplify}
At any iteration $k$, CQL expands the difference in expected Q-values under the behavior policy $\behavior(\ba|\bs)$ and $\mu_k$, i.e., $\forall \bs, ~\E_{\behavior(\ba|\bs)}[\hat{Q}^k(\bs, \ba)] - \E_{\mu_k(\ba|\bs)}[\hat{Q}^k(\bs, \ba)] > \E_{\behavior(\ba|\bs)}[{Q}^k(\bs, \ba)] - \E_{\mu_k(\ba|\bs)}[{Q}^k(\bs, \ba)]$, for appropriate choices of $\alpha_k$.
\end{theorem}
We restate this theorem formally with a proof in Appendix~\ref{app:missing_proofs}. Theorem~\ref{thm:gap_amplify} shows that the suboptimality of the in-distribution actions ($\ba \sim \behavior$) under the learned Q-values is reduced. In scenarios where function approximation or stochastic approximation error makes OOD actions have higher learned Q-values, CQL backups that are more robust to Q-function estimation error by virtue of the gap-expanding property is appealing. 
%%SL.5.17: Above sentence is very hard to parse because it has too many clauses, split it into two sentences.
As we will discuss in Section~\ref{sec:related} and Appendix~\ref{app:gap_amplify}, prior offline RL methods, that do not explicitly constrain or regularize the Q-function, may not enjoy such robustness properties, indicating the benefit of having a simple CQL regularizer. 

\vspace{-7pt}
\section{Practical Algorithm and Implementation Details}
\label{sec:practical_alg}
\vspace{-5pt}
We now describe two practical offline deep reinforcement learning methods based on CQL: a Q-learning variant and an actor-critic variant. Pseudocode is shown in Algorithm~\ref{alg:practical_alg}, with differences from conventional actor-critic algorithms (e.g., SAC~\citep{haarnoja}) and deep Q-learning algorithms (e.g., DQN~\citep{mnih2013playing}) in red.
Our algorithm uses the CQL($\mathcal{H}$) (or CQL($\mathcal{R}$) in general) objective from the CQL framework for training the Q-function $Q_\theta$, which is parameterized by a neural network with parameters $\theta$. For the actor-critic algorithm, a policy $\pi_\phi$ is trained as well. Our algorithm modifies the objective for the Q-function (swaps out Bellman error with CQL($\mathcal{H}$)) or CQL($\rho$)
\begin{wrapfigure}{r}{0.6\textwidth}
\begin{small}
\vspace{-23pt}
\begin{minipage}[t]{0.99\linewidth}
\begin{algorithm}[H]
\small
\caption{Conservative Q-Learning (both variants)}
\label{alg:practical_alg}
\begin{algorithmic}[1]
    \STATE Initialize Q-function, $Q_\theta$, and optionally a policy, $\pi_\phi$.
    \FOR{step $t$ in \{1, \dots, N\}}
        \STATE Train the Q-function using $G_Q$ gradient steps on objective from Equation~\ref{eqn:practical_objective} \\
        \mbox{$\theta_t := \theta_{t-1} - \eta_Q \nabla_\theta \textcolor{red}{\text{CQL}(\mathcal{R})(\theta)}$}\\
        (Use $\bellman^*$ for Q-learning, $\bellman^{\policy_{\phi_t}}$ for actor-critic)
        \STATE \underline{(only with actor-critic)} Improve policy $\pi_\phi$ via $G_\pi$ gradient steps on $\phi$ with SAC-style entropy regularization:\\
        \mbox{$\phi_{t} := \phi_{t-1} + \eta_\pi \mathbb{E}_{\bs \sim \mathcal{D}, \ba \sim \pi_\phi(\cdot|\bs)}[Q_\theta(\bs, \ba)\! -\! \log \pi_\phi(\ba|\bs)] $}
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-15pt}
\end{small}
\end{wrapfigure}
in a standard actor-critic or Q-learning setting, as shown in Line 3. As discussed in Section~\ref{sec:framework}, due to the explicit penalty on the Q-function, CQL methods do not require a policy constraint,
unlike prior offline RL methods~\citep{kumar2019stabilizing,wu2019behavior,siegel2020keep,levine2020offline}.
Hence, no fitting of a behavior policy $\behavior$ is required. This prevents the detrimental effects using an inaccurately fitted behavior policy in an offline RL algorithm~\citep{levine2020offline}.

\textbf{Implementation details.} Our algorithm requires an addition of only \textbf{20} lines of code on top of standard implementations of soft actor-critic (SAC)~\citep{haarnoja} for continuous control experiments and on top of QR-DQN~\citep{dabney2018distributional} for the discrete control experiments. The tradeoff factor, $\alpha$ is automatically tuned via Lagrangian dual gradient descent for continuous control, and is fixed at constant values described in Appendix~\ref{sec:experimental_details} for discrete control. We use default hyperparameters from SAC, except that the learning rate for the policy is chosen to be smaller. Elaborate details are provided in Appendix~\ref{sec:experimental_details}.  
