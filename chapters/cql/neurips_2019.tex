
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[nonatbib,final]{neurips_2019}
\usepackage[numbers]{natbib}

\usepackage[colorlinks=true,allcolors=blue]{hyperref}


% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usetikzlibrary{positioning}
\usepackage{mathtools}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
% \newtheorem{corollary}{Corollary}[proposition]
\newtheorem{definition1}{Definition}[section]

\include{NeuRIPS2019/defs}
\let\svthefootnote\thefootnote

\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}

\title{Conservative Q-Learning\\ for Offline Reinforcement Learning}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Aviral Kumar$^1$, Aurick Zhou$^1$, George Tucker$^2$, Sergey Levine$^{1,2}$ \\
  $^1$UC Berkeley, $^2$Google Research, Brain Team\\
  \texttt{aviralk@berkeley.edu}
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}


\part*{Representative Publications}

% \part*{\Large{Significant Publication 1: \\ Conservative Q-Learning for Offline Reinforcement Learning}}
In this document, two of my representative publications are attached one after the other.

\section*{Publication 1:} 
Conservative Q-Learning for Offline Reinforcement Learning, NeurIPS 2020.

\section*{Publication 2:}
DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization, ICLR 2022.

% \section*{Author List} Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine

% \section*{Significance of the Paper} 

% {\textbf{Summary.}} This paper presents a method directly address offline reinforcement learning by learning a \emph{pessimistic} value function that estimates a lower bound on the actual utility of the learned policy (instead of constraining the policy). The key insight is to add a regularizer inspired by min-max formulations for adversarial training to the training loss in an off-the-shelf RL algorithm. 

% This paper has \textbf{500+ citations} till date (paper released in June 2020).  

% {\textbf{Theoretical impact.}} My theoretical analysis of CQL provided a framework for studying pessimistic algorithms under the lens of safe policy improvement, and this framework was used to make substantial progress on many aspects of offline reinforcement learning including the development of state-of-the-art model-based reinforcement learning algorithms and data sharing in multi-task reinforcement learning. Other works have also shown that CQL attains strong performance guarantees compared to other offline RL approaches. 

% {\textbf{Real-world application impact.}} CQL has served as a fundamental building block of many applications, I list some of them below:
% \begin{itemize}
%     \item In my follow-up work, I show that CQL is an effective way to leverage diverse robot datasets to boost generalization in {robot learning}, along with several works from others. 
%     \item At LinkedIn, CQL policies trained on historical user-interaction data are deployed in {mobile notification systems} to \textbf{hundreds of millions of users} and lead to higher notification quality.
%     \item CQL is also deployed in {digital marketing systems} (DMS), where it reduces costs and improves over contextual bandit and other RL methods.
%     \item CQL is also used for optimizing treatment regimes for chemotherapy.
% \end{itemize}
% References for each of these applications can be found in my research statement.






\newpage

\maketitle


\begin{abstract}
Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose \emph{conservative Q-learning (CQL)}, which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. 
% This conservatism reduces the effect of errors introduced by distributional shift. 
% CQL lower-bounds the value, thus preventing erroneously high values at unobserved actions, which can give rise to poor policy performance.     
We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions\footnote{Our follow-up on CQL for vision-based robotic manipulation can be found here: \url{https://arxiv.org/abs/2010.14500}.}.
%%SL: rewrote some of the material below a bit
%based on Q-function estimation, that alleviates these limitations. Our algorithm simply augments a traditional Q-learning algorithm with an additional penalty the Q-function during training. We theoretically show that this method obtains provably lower-bounded value estimates, and as a result, avoids a number of problems that arise in offline settings. We empirically demonstrate its superior performance over existing methods, especially in harder settings with complex data distributions. We also show how our approach can be used for conservative off-policy policy evaluation, and empirically demonstrate its efficacy in providing meaningful value estimates on a number of complex domains.      
\end{abstract}

\vspace{-0.3cm}

\input{introduction.tex}
\input{background.tex}
\input{method_new.tex}
\input{NeuRIPS2019/method_part2}
\input{related.tex}
% \input{theory.tex}
\input{NeuRIPS2019/experiments_shortened}

\vspace{-9pt}
\section{Discussion}
% summary
\vspace{-9pt}
We proposed conservative Q-learning (CQL), an algorithmic framework for offline RL that learns a lower bound on the policy value.
%%SL: commenting out for length
%We show that the expected values of the Q-functions learned via CQL bound the expected values of the true Q-function. The resulting evaluation and learning algorithms are simple and easy to implement on top of existing Q-learning and actor-critic methods, using only a small modification to the Q-function objective.
Empirically, we demonstrate that CQL outperforms prior offline RL methods on a wide range of offline RL benchmark tasks, including complex control tasks and tasks with raw image observations. In many cases, the performance of CQL is substantially better than the best-performing prior methods, exceeding their final returns by 2-5x.
% implications
The simplicity and efficacy of CQL make it a promising choice for a wide range of real-world offline RL problems. However, a number of challenges remain. While we prove that CQL learns lower bounds on the Q-function in the tabular, linear, and a subset of non-linear function approximation cases, a rigorous theoretical analysis of CQL with deep neural nets, is left for future work. Additionally, offline RL methods are liable to suffer from overfitting in the same way as standard supervised methods, so another important challenge for future work is to devise simple and effective early stopping methods, analogous to validation error in supervised learning.
%We hope that the efficacy of CQL on complex datasets, which combine data from multiple sources (e.g., good and mediocre, real-world behavior policies), its performance with high-dimensional states and actions, and its simplicity and compactness will make it an appealing and promising choice for a wide range of real-world RL problems that must utilize previously collected datasets.

% limitations + future work
%While CQL empirically outperforms prior methods across a wide range of tasks, a number of technical questions still remain. While we prove that CQL learns lower bounds on the Q-function in the tabular and linear case, a rigorous theoretical analysis of CQL with non-linear function approximators, such as deep networks, is left for future work. Additionally, offline RL methods are liable to suffer from overfitting in the same way as standard supervised methods, so another important challenge for future work is to devise simple and effective early stopping methods, analogous to validation error in supervised learning. While these avenues for future promise to further improve the effectiveness of offline RL methods, we believe that CQL represents an important step toward offline RL algorithms that are simple, practical, and highly effective. Such algorithms, when scaled up and applied to large real-world datasets, have the potential to make it possible to bring RL to bear on a much wider range of problems than has been possible with traditional active RL methods.     

% \section*{Broader Impact}
% % very quick summary of the work
% Offline RL offers the promise to scale autonomous learning-based methods for decision-making to large-scale, real-world sequential decision making problems. Such methods can effectively leverage prior datasets without any further interaction, thus avoiding the exploration bottleneck and alleviating many of the safety and cost constraints associated with online reinforcement learning.
% In this work, we proposed conservative Q-learning (CQL), an algorithmic framework for offline reinforcement learning that learns a Q-function such that the expected policy value under this learned Q-function lower-bounds the actual policy value. This mitigates value function over-estimation issues due to distributional shift, which in practice are one of the major challenges in offline reinforcement learning. We analyzed algorithms derived from the CQL framework and demonstrated their performance empirically.

% %impacts in terms of applications where this could be used
% Our primary aim behind this work is to develop simple and effective offline RL algorithms, and we believe that CQL makes an important step in that direction. CQL can be applied directly to several problems of practical interest where large-scale datasets are abundant: autonomous driving, robotics, and software systems (such as recommender systems). We believe that a strong offline RL algorithm, coupled with highly expressive and powerful deep neural networks, will provide us the ability successfully apply end-to-end learning based approaches to such problems, providing considerable societal benefits. Of course, autonomous decision-making agents have a wide range of applications, and technology that enables more effective autonomous decision-making has both positive and negative societal effects. While effective autonomous decision-making can have considerable positive economic effects, it can also enable applications with complex implications in regard to privacy (e.g., in regard to autonomous agents on the web, recommendation agents, advertising, etc.), as well as complex economic effects due to changing economic conditions (e.g., changing job requirements, loss of jobs in some sectors and growth in others, etc.). Such implications apply broadly to technologies that enable automation and decision making, and are largely not unique to this specific work.

\vspace{-0.2cm}
\section*{Acknowledgements and Funding}
\vspace{-0.2cm}
We thank Mohammad Norouzi, Oleh Rybkin, Anton Raichuk, Avi Singh, Vitchyr Pong and anonymous reviewers from the Robotic AI and Learning Lab at UC Berkeley and NeurIPS for their feedback on an earlier version of this paper. We thank Rishabh Agarwal for help with the Atari QR-DQN/REM codebase and for sharing baseline results. This research was funded by the DARPA Assured Autonomy program, and compute support from Google and Amazon.


\bibliography{main.bib}
\bibliographystyle{plainnat}

\newpage
\appendix
\input{NeuRIPS2019/appendix}

\end{document}
