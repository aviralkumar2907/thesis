\documentclass[../thesis.tex]{subfiles}

\usepackage{wrapfig}
\usepackage{cite}
% \usepackage{natbib}
% \usepackage[round]{natbib}

\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usetikzlibrary{positioning}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
% \newtheorem{corollary}{Corollary}[proposition]
\newtheorem{definition1}{Definition}[section]



% \include{chapters/cql/defs}


\begin{document}


% \blfootnote{This chapter is based on \cite{kumar2020conservative}, published at NeurIPS 2020, which is joint work with Aurick Zhou, George Tucker, and Sergey Levine.}

\vspace{-0.4cm}
\begin{AIbox}{\large{\textbf{Abstract}}}
\vspace{4mm}
Thus far, we have developed algorithms for learning policies from offline data. In practice, we would often want to instantiate these algorithms in a way that allow them to ingest larger amounts of broad and diverse data to produce performant policies. One way to do so is to train high-capacity neural network value functions and policies with broad offline datasets. In support is the observation that this sort of a recipe is quite effective in supervised learning: despite the massive over-parameterization, deep networks trained via supervised learning are easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent (SGD), which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this chapter, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning (TD-learning), the resulting derived regularizer favors degenerate solutions with excessive ``aliasing'', in stark contrast to the supervised learning case. We back up these findings empirically and propose a simple and effective \emph{explicit} regularizer, called \drmethodname, that counteracts this undesirable effect. We then build on this technique to enable, for the first time, a recipe that can train 100 million parameter ResNet models entirely via offline TD-learning to outperform variants of behavioral cloning with heterogeneous data.
\vspace{2mm}
\end{AIbox}


\input{chapters/dr3/intro}

\section{DR3: Explicit Regularization For Value-Based Offline RL}

\input{chapters/dr3/problem_final2}
\input{chapters/dr3/method}
\input{chapters/dr3/experiments}
% \input{chapters/dr3/analysis}


\section{Scaled Q-Learning: Large-Scale Study of Offline Q-Learning}

% To systematically evaluate the impact of these changes on scaling and generalization, we train a single policy to play 40 Atari games~\citep{bellemare2013arcade, agarwal2019optimistic}, similarly to \citet{lee2022multi}, and evaluate performance when the training dataset contains expert trajectories \emph{and} when the data is sub-optimal. This problem is especially challenging because of the diversity of games with their own unique dynamics, reward, visuals, and agent embodiments. Furthermore, the sub-optimal data setting requires the learning algorithm to ``stitch together'' useful segments of sub-optimal trajectories to perform well. 
% To investigate generalization of learned representations, we evaluate offline fine-tuning to \emph{never-before-seen} games and fast online adaptation on new \emph{variants} of training games~(Section~\ref{sec:ft_off_on}).
% With our modifications, 
% \begin{itemize}
%     \item Offline Q-learning learns policies that attain more than 100\% human-level performance on most of these games, about \textbf{2x} better than prior supervised learning~(SL) approaches for learning from sub-optimal offline data (51\% human-level performance). 
%     \item Akin to scaling laws in SL~\citep{kaplan2020scaling}, offline Q-learning performance scales favorably with model capacity~(Figure~\ref{fig:scaling}). 
%     \item Representations learned by offline Q-learning give rise to more than 80\% better performance when fine-tuning on new games compared to representations from state-of-the-art return-conditioned supervised~\citep{lee2022multi} and self-supervised methods~\citep{he2111masked,oord2018representation}. 
% \end{itemize}
% By scaling Q-learning, we realize the promise of offline RL: learning policies that broadly generalize and exceed the capabilities demonstrated in the training dataset. We hope that this work encourages large-scale offline RL applications, especially in domains with large sub-optimal datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{chapters/scaled_ql/figures/fig_overview.pdf}
    \vspace{-0.3cm}
    \caption{\footnotesize{An overview of the training and evaluation setup. Models are trained offline with potentially sub-optimal data. We adapt CQL to the multi-task setup via a multi-headed architecture. The pre-trained visual encoder is reused in fine-tuning (the weights are either frozen or fine-tuned), whereas the downstream fully-connected layers are reinitialized and trained.}}
    \label{fig:overview}
    \vspace{-0.5cm}
\end{figure}


\vspace{-0.1cm}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{chapters/scaled_ql/figures/training_fig_legend_20p.pdf}
    \vspace{-0.1cm}
    \includegraphics[width=0.48\linewidth]{chapters/scaled_ql/figures/suboptimal_data_results.pdf}
    ~~~
    \includegraphics[width=0.45\linewidth]{chapters/scaled_ql/figures/perf_profile_20p.pdf}
    \vspace{-0.3cm}
    \caption{\footnotesize{Offline multi-task performance on 40 games with sub-optimal data. \textbf{Left}. Scaled QL significantly outperforms the previous state-of-the-art method, DT, attaining about a \textbf{2.5x} performance improvement in normalized IQM score. To contextualize the absolute numbers, we include online multi-task Impala DQN~\citep{espeholt2018impala} trained on 5x as much data.
    \textbf{Right}. Performance profiles~\citep{agarwal2021deep} showing the distribution of normalized scores across all 40 training games (higher is better). 
    Scaled QL stochastically dominates other offline RL algorithms and achieves superhuman performance in 40\% of the games. ``Behavior policy'' corresponds to the score of the dataset trajectories. {Online MT DQN (5X), taken directly from \citet{lee2022multi}, corresponds to running multi-task online RL for 5x more data with IMPALA (details in Appendix~\ref{sec:online_mt_dqn}).}}}
    \label{fig:suboptimal_offline}
    \vspace{-0.3cm}
\end{figure}


\input{chapters/scaled_ql/prelim}
\input{chapters/scaled_ql/method}
\input{chapters/scaled_ql/experiments}
\input{chapters/scaled_ql/related_work}



\section{Discussion and Limitations}
% summary

In the first part of this chapter, we attempted to characterize the implicit preference of TD-learning towards solutions that maximally co-adapt gradients (or features) at consecutive state-action tuples that appear in Bellman backup. This regularization effect is exacerbated when out-of-sample
state-action samples are used for the Bellman backup and it can lead to poor policy performance. Inspired by the theory, we propose a practical explicit regularizer, \methodname, that yields substantial improvements in stability and performance on a wide range of offline RL problems. We believe that thi sort of an understanding the learning dynamics of offline deep Q-learning will lead to more robust and stable deep RL algorithms and enable predicting such instability issues, well in advance, which can inspire cross-validation and model selection strategies. This is an important, open challenge in offline RL, for which existing off-policy evaluation techniques are not practically sufficient~\citep{fu2021benchmarks,kumar2022workflow}. 
A limitation of our analysis of implicit regularization stems from the simplifying assumptions needed to tractably analyze the learning dynamics. For instance, our analysis holds in the neighborhood of fixed points of the temporal-difference backup, but it does not comment on properties of intermediate solutions of offline TD-learning methods, when the network is far away from a fixed point. We hypothesize that this limitation can be addressed by building on techniques in \citet{damian2021label} (or subsequent follow-ups of this work).   

The second part of this chapter shows, for the first time (to the best of our knowledge), that offline conservative Q-learning can scale to high-capacity models when trained on large, diverse datasets. By scaling up capacity, we unlocked analogous trends to those observed in vision and NLP. We found that scaled Q-learning trains policies that exceed the average dataset performance and prior methods, especially when the dataset does not contain expert trajectories. Furthermore, by training a large-capacity model on diverse tasks, we show that Q-learning is sufficient to recover general-purpose representations that enable rapid learning of novel tasks. Although we described an approach that is sufficient to scale Q-learning, this is by no means optimal. The scale of the experiments limited the number of alternatives we could explore, and we expect that future work will greatly improve performance.
For example, contrary to decision transformers (DT)~\citep{lee2022multi}, we did not use data augmentation in our experiments, which we believe can provide significant benefits. While we did a preliminary attempt to perform online fine-tuning on an entirely new game~($\textsc{SpaceInvaders}$), we found that this did not work well for any of the pretrained representations~(see Figure~\ref{fig:lr_curves_online_ft}). Addressing this is an important direction for future work. We speculate that this challenge is related to designing methods for learning better exploration from offline data, which is not required for offline fine-tuning. Overall, we believe that scaled Q-learning could serve as a starting point for RL-trained foundation models.

%%AK: fix this to include both CQL and COMBO paper people
\section*{Acknowledgements and Funding}
We thank Dibya Ghosh, Xinyang Geng, Dale Schuurmans, Marc Bellemare, Pablo Castro, Ofir Nachum, Ross Goroshin and Aleksandra Faust for informative discussions. We thank Sherry Yang, Ofir Nachum, and Kuang-Huei Lee for help with the multi-game decision transformer codebase; Anurag Arnab for help with the Scenic ViT codebase. We thank Zoubin Ghahramani and Douglas Eck for providing compute support. We thank the members of RAIL at UC Berkeley for their support and suggestions. We thank anonymous reviewers for feedback on an early version of this paper. This research is funded in part by the DARPA Assured Autonomy Program and in part, by compute resources from Microsoft Azure and Google Cloud. TM acknowledges support of Google Faculty Award, NSF IIS 2045685, the Sloan fellowship, and JD.com.

\end{document}
