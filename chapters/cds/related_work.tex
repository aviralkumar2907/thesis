\section{Related Work}

\textbf{Offline RL.} Offline RL~\citep{ernst2005tree, riedmiller2005neural, LangeGR12, levine2020offline} has shown promise in domains such as robotic manipulation~\citep{kalashnikov2018scalable, mandlekar2020iris, Rafailov2020LOMPO,singh2020cog,kalashnikov2021mt}, NLP~\citep{jaques2019way,jaques2020human}, recommender systems \& advertising~\citep{strehl2010learning,garcin2014offline,charles2013counterfactual,theocharous2015ad,thomas2017predictive}, and healthcare~\citep{shortreed2011informing, Wang2018SupervisedRL}. The major challenge in offline RL is distribution shift~\citep{fujimoto2018off,kumar2019stabilizing,kumar2020conservative}, where the learned policy might generate out-of-distribution actions, resulting in erroneous value backups. Prior offline RL methods address this issue by regularizing the learned policy to be ``close`` to the behavior policy~\citep{fujimoto2018off,liu2020provably,jaques2019way,wu2019behavior, zhou2020plas,kumar2019stabilizing,siegel2020keep, peng2019advantage}, through variants of importance sampling~\citep{precup2001off, sutton2016emphatic, LiuSAB19, SwaminathanJ15, nachum2019algaedice}, via uncertainty quantification on Q-values~\citep{agarwal2020optimistic, kumar2019stabilizing, wu2019behavior, levine2020offline}, by learning conservative Q-functions~\citep{kumar2020conservative,kostrikov2021offline}, and with model-based training with a penalty on out-of-distribution states~\citep{kidambi2020morel, yu2020mopo,matsushima2020deployment,argenson2020model,swazinna2020overcoming,Rafailov2020LOMPO,lee2021representation,yu2021combo}. While current benchmarks in offline RL~\citep{fu2020d4rl,gulcehre2020rl} contain datasets that involve multi-task structure, existing offline RL methods do not leverage the shared structure of multiple tasks and instead train each individual task from scratch. In this paper, we exploit the shared structure in the offline multi-task setting and train a general policy that can acquire multiple skills.

%%CF.5.17: I would consider mentioning meta-RL methods somewhere, since they also address multi-task RL and especially since there are some that aren't conflicted I think (e.g. VariBAD, meta-Q-learning). Some of them even reuse data
%%TY.5.21: I think meta-RL methods might be a bit orthogonal since they aim for generalization to new tasks. I can cite some multi-task RL works that are less conflicted.
\textbf{Multi-task RL algorithms.} Multi-task RL algorithms~\citep{wilson2007multi,parisotto2015actor,teh2017distral,espeholt2018impala,hessel2019popart,yu2020gradient, xu2020knowledge, yang2020multi, kalashnikov2021mt,sodhani2021multi}
%%CF.5.17: there are online MTRL methods that are more recent than this. For example, there's one on soft modules from USC or UCSD. You can look at papers that cite PCGrad or meta-world and/or look on google scholar for more.
%%TY.5.21: added several more papers.
focus on solving multiple tasks jointly in an efficient way. While multi-task RL methods seem to provide a promising way to build general-purpose agents~\citep{kalashnikov2021mt}, prior works have observed major challenges in multi-task RL, in particular, the optimization challenge~\citep{hessel2019popart,schaul2019ray,yu2020gradient}.
Beyond the optimization challenge, how to perform effective representation learning via weight sharing is another major challenge in multi-task RL. Prior works have considered distilling per-task policies into a single policy that solves all tasks~\citep{rusu2015policy,teh2017distral,ghosh2017divide,xu2020knowledge}, separate shared and task-specific modules with theoretical guarantees~\citep{d2019sharing}, and incorporating additional supervision~\citep{sodhani2021multi}. Finally, sharing data across tasks emerges as a challenge in multi-task RL, especially in the off-policy setting, as na\"{i}vely sharing data across all tasks turns out to hurt performance in certain scenarios~\citep{kalashnikov2021mt}. Unlike most of these prior works, we focus on the offline setting where the challenges in data sharing are most relevant. Methods that study optimization and representation learning issues are complementary and can be readily combined with our approach.
%%CF.5.17: how does your method & analysis contrast with all of these methods? need to explicitly state what is different. (ie things like - we focus on the offline setting where some of these issues are less severe & just different; we focus on data sharing & methods that look at optimization & representation are complementary, something about the analysis contributing to our understanding in a complementary way, etc)
%%CF.5.17: I also wonder if we should include a comparison that runs only one of these methods to show some evidence that they don't solve the problem.
%%TY.5.21: Added the discussion above. We can also perform the empirical analysis on HIPI to see if it solves the problem.
% We will next survey methods in data sharing in multi-task off-policy RL.

%%CF.5.17: Other papers that do some form of data sharing:
% the REPAINT paper - includes a less naive data sharing approach
% model-based RL methods (e.g. visual foresight, some experiments in Danijar's papers, MBOLD) - these share everything
% Dave Held paper on goal-conditioned RL from images - not sure how they share
% probably other GCRL papers? eg Yevgen's actionable models, distributional planning networks, but probably others that aren't conflicted
%%TY.5.21: The REPAINT paper and model-based RL methods do not seem to study the multi-task problem, which might be less relevant? I added references to more GCRL papers.
%%CF.8.1: The model-based RL papers that I mention above *do* include some multi-task experiments (and share data across tasks, and often don't recompute rewards because often only the model is what is sharing cross-task data.) Also, both visual foresight and MBOLD operate in the fully offline setting.
\textbf{Data sharing in multi-task RL.} Prior works~\citep{andrychowicz2017hindsight,kaelbling1993learning,pong2018temporal,schaul2015universal,eysenbach2020rewriting,li2020generalized,kalashnikov2021mt,chebotar2021actionable} have found it effective to reuse data across tasks by recomputing the rewards of data collected for one task and using such relabeled data for other tasks, which effectively augments the amount of data available for learning each task and boosts performance. These methods perform relabeling either uniformly~\citep{kalashnikov2021mt} or based on metrics such as estimated Q-values~\citep{eysenbach2020rewriting,li2020generalized}, domain knowledge~\citep{kalashnikov2021mt}, the distance to states or images in goal-conditioned settings~\citep{andrychowicz2017hindsight,pong2018temporal,nair2018visual,liu2019competitive,sun2019policy,lin2019reinforcement,huang2019mapping,lynch2020grounding,yang2021bias,chebotar2021actionable}, \arxiv{and metric learning for robust inference in the offline meta-RL setting~\citep{li2019multi}. All of these methods either require online data collection and do not consider data sharing in a fully offline setting, or only consider offline goal-conditioned or meta-RL problems~\citep{chebotar2021actionable,li2019multi}.} \arxiv{While these prior works empirically find that data sharing helps, we believe that our analysis in Section~\ref{sec:analysis} provides the first analytical understanding of why and when data sharing can help in multi-task offline RL and why it hurts in some cases.} 
\arxiv{Specifically, our analysis reveals the effect of distributional shift introduced during data sharing, which is not taken into account by these prior works. Our proposed approach, CDS, tackles the challenge of distributional shift in data sharing by intelligently sharing data across tasks and improves multi-task performance by effectively trading off between the benefits of data sharing and the harms of excessive distributional shift.}
%%SL.8.1: I think this paragraph kind of buries the main point: it makes it sound like we are just (rather naively) extending the ideas from these past papers to the offline multi-task setting, which really undersells the contribution. It's not like we're just doing what they already did but extending beyond goals, we are actually addressing a challenge that these methods did not address (and indeed that they suffer from).
%%TY.8.1: I revised the above paragraph to say that our method addresses the challenge that prior works didn't address.
%%SL.5.15: I think it's important to expand the discussion of prior multi-task RL methods and better cover other methods that aim to understand why multi-task RL is hard, empirically observe that it's hard, and offer various solutions. Right now the above citations seem to focus more or less exclusively on "applications" of multi-task RL, whereas we need to survey prior work on analysis and solutions (maybe in a separate paragraph). This includes things like ray interference, pcgrad, and other papers you can find that cite those or are cited by them
%%TY.5.16: I added a paragraph discussing challenges in multi-task RL and then use the above paragraph to survey relabeling methods in the off-policy setting.
