\section{Related Work}
\label{sec:related}
%
Optimizing hardware accelerators has become more important recently. Prior works~\citep{bo:frontiers:2020,flexibo:arxiv:2020,cnn_gen:cyber:2020,prac_dse:mascots:2019,accel_gen:dac:2018,spatial:pldi:2018,automomml:hpc:2016,opentuner:pact:2014,hegdemind,magnet,autodnnchip} mainly rely on expensive-to-query hardware simulators to navigate the search space \review{and/or target single-application accelerators}. For example, HyperMapper~\citep{prac_dse:mascots:2019} targets compiler optimization for FPGAs by continuously interacting with the simulator in a design space with relatively few infeasible points. Mind Mappings~\citep{hegdemind}, optimizes software mappings to a fixed hardware provided access to millions of feasible points and throws away infeasible points during learning. {MAGNet~\citep{magnet} uses a combination of pruning heuristics and online Bayesian optimization to generate accelerators for image classification models in a single-application setting.} {AutoDNNChip~\citep{autodnnchip} uses two-level online optimization to generate customized  accelerators for ASIC and FPAG platforms.} In contrast, \primemethodname~, does not only learn a conservative model of the objective function from offline data but can also leverage information from infeasible points and can work with just a few thousand feasible points. {In addition, we devise a contextual version of \primemethodname\ that is effective in designing accelerators that are jointly optimized for multiple applications, different from prior work.} Finally, to our knowledge, our work, is the first to demonstrate generalization to unseen applications for accelerator design, outperforming state-of-the-art online methods.

A popular approach for solving black-box optimization problems is model-based optimization (MBO)~\citep{snoek15scalable,shahriari2016TakingTH,snoek2012practical}. Most of these methods fail to scale to high-dimensions, and have been extended with neural networks~\citep{snoek15scalable,snoek2012practical,kim2018attentive,garnelo18neural,garnelo18conditional,p3bo:arxiv:2020,angermueller2019model,mirhoseini2020chip}. While these methods work well in the active setting, they are susceptible to out-of-distribution inputs~\citep{trabucco2021designbench} in the offline, data-driven setting. To prevent this, many methods for offline model-based optimization constrain the optimizer to the manifold of valid, in-distribution inputs~\citep{brookes19a,fannjiang2020autofocused,kumar2019model}. However, modeling the manifold of valid inputs can be challenging for accelerators. \primemethodname\ dispenses with the need for generative modeling, while still avoiding out-of-distribution inputs. \primemethodname\  takes a conservative value estimation approach to offline RL, similar \citet{trabucco2021conservative}. However, unlike these approaches, \primemethodname\ can handle constraints by learning from infeasible data and utilizes a better optimizer (See Appendix Table~\ref{table:coms_vs_prime} for a comparison). In addition, while prior works area mostly restricted to a single application, we show that \primemethodname\ is effective in multi-task optimization and zero-shot generalization.