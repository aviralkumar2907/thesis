% \vspace{-0.2cm}
\section{Discussion}
\label{sec:discussion}
\vspace{-0.1cm}
%
In this work, we present a data-driven offline optimization method, \primemethodname\ to automatically architect hardware accelerators. Our method learns a conservative surrogate of the objective function by leveraging  infeasible data points to better model the desired objective function of the accelerator using a one-time collected dataset of accelerators, thereby alleviating the need for time-consuming simulation.
%
Our results show that, on average, our method outperforms the best designs observed in the logged data by 2.46$\times$ and improves over the best simulator-driven approach by about 1.54$\times$. 
%
In the more challenging setting of designing accelerators jointly optimal for multiple applications or for new, unseen applications, zero-shot, \primemethodname\ outperforms simulator-driven methods by 1.2$\times$, while reducing the total simulation time by 99\%.
%
{The efficacy of \primemethodname\ highlights the potential for utilizing the logged offline data in an accelerator design pipeline. While \primemethodname\ outperforms the online methods we utilize, in principle, a strong online method can be devised by running \primemethodname\ in the inner loop. Our goal is to not advocate that offline methods must replace online methods, but that training a strong offline optimization algorithm on offline datasets of low-performing designs can be a highly effective ingredient in hardware accelerator design.}


\section*{Acknowledgements}
%
We thank the ``Learn to Design Accelerators'' team at Google Research and the Google EdgeTPU team for their invaluable feedback and suggestions.
%
In addition, we extend our gratitude to the Vizier team, Christof Angermueller, Sheng-Chun Kao, Samira Khan, Stella Aslibekyan, and Xinyang Geng for their help with experiment setups and insightful comments. 