\begin{table*}[t!]
\small
\centering
\vspace*{0.1cm}
\caption{\label{table:results_single_task}Optimized objective values (i.e., latency in milliseconds) obtained by various methods for the task of learning accelerators specialized to a given application. Lower latency is better. \textbf{From left to right}: our method, online Bayesian optimization (``Bayes Opt''), online evolutionary algorithm (``Evolutionary''), and the best design in the training dataset. On average (last row), \methodname\ improves over the best in the dataset by 2.46$\times$ (up to 6.69$\times$ in t-RNN Dec) and outperforms best online optimization methods by 1.54$\times$ (up to 6.62$\times$ in t-RNN Enc). The best accelerator configurations identified is highlighted in bold.}
\resizebox{0.95\textwidth}{!}{% <------ Don't forget this %
\begin{tabular}{l|l|l|l|l|l}
\toprule
&&\multicolumn{3}{c|}{\textbf{Online Optimization}}&\\\cline{3-5}
\textbf{Application} & \textbf{\methodname} & \textbf{Bayes Opt} & \textbf{Evolutionary}&\textbf{MBO}&$\mathcal{D}$ \textbf{(Best in Training)}\\\midrule
{MobileNetEdgeTPU}&\textbf{298.50}&319.00&320.28&332.97&354.13\\\hline 
{MobileNetV2}&\textbf{207.43}&240.56&238.58&244.98&410.83\\\hline
{MobileNetV3}&\textbf{454.30}&534.15&501.27&535.34&938.41\\\hline
{\mfour}&\textbf{370.45}&396.36&383.58&405.60&779.98\\\hline
{\mfive}&208.21&201.59&\textbf{198.86}&219.53&449.38\\\hline
{\msix}&131.46&121.83&120.49&\textbf{119.56}&369.85\\\hline
{U-Net}&\textbf{740.27}&872.23&791.64&888.16&1333.18\\\hline
{t-RNN Dec}&\textbf{132.88}&771.11&770.93&771.70&890.22\\\hline
{t-RNN Enc}&\textbf{130.67}&865.07&865.07&866.28&584.70\\\bottomrule
\CC \textbf{Geomean of \methodname's Improvement}&\CC~1.0$\times$&\CC~\texttt{\textbf{1.58$\times$}}&\CC~\texttt{\textbf{1.54$\times$}}&\CC~\texttt{\textbf{1.61$\times$}}&\CC~\texttt{\textbf{2.46$\times$}}\\
\bottomrule
\end{tabular}% <------ Don't forget this %
}
\vspace{-0.2cm}
\end{table*}