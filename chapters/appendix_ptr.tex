\documentclass[../thesis.tex]{subfiles}

\usepackage{wrapfig}
\usepackage{cite}
% \usepackage{natbib}
% \usepackage[round]{natbib}

\begin{document}

% \blfootnote{This chapter is based on \cite{fu2019diagnosing}, published at ICML 2019, which is joint work with Justin Fu, Matthew Soh, and Sergey Levine.}

% \input{chapters/bear/text/abstract.tex}

% \input{chapters/bear/text/introduction.tex}
% \input{chapters/bear/text/related.tex}
% \input{chapters/bear/text/background.tex}
% \input{chapters/bear/text/method.tex}
% \input{chapters/bear/text/experiments.tex}
% \input{chapters/bear/text/conclusions.tex}

% Q-learning methods are a common class of algorithms used in reinforcement learning (RL). However, their behavior with function approximation, especially with neural networks, is poorly understood theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a "unit testing" framework where we can utilize oracles to disentangle sources of error. 
% Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with deep RL methods.
% We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains. 

% \vspace{-0.4cm}
% \begin{AIbox}{\large{\textbf{Abstract}}}
% \vspace{4mm}
% In this chapter, we seek to understand challenges in the offline RL problem setting. To this end, we empirically study the behavior of off-policy RL methods when only training on a static, offline dataset. In principle, off-policy RL methods such as those based on approximate dynamic programming should succeed at leveraging experience collected from prior policies for sample-efficient learning. However, as we illustrate in this chapter, in practice, commonly used off-policy methods based on Q-learning and actor-critic are incredibly sensitive to \emph{both} the dataset distribution and quantity. Building on the insights from our empirical observations, we identify two main issues with learning value functions in the offline setting: \emph{distributional shift} and \emph{sampling error}. Later, we also demonstrate that these challenges also inhibit performance for other classes of off-policy RL methods such as model-based approaches. Finally, in subsequent chapters, we develop techniques to handle distributional shift, progressing towards reliable and easy-to-use offline RL algorithms.  
% \vspace{2mm}
% \end{AIbox}
    
% \input{chapters/diagnosing_q/sections/1_introduction.tex}
% % \input{chapters/diagnosing_q/sections/3_background.tex}
% \input{chapters/diagnosing_q/sections/4_setup.tex}
% % \input{chapters/diagnosing_q/sections/5_approx.tex}
% \input{chapters/diagnosing_q/sections/8_weighting_distributions.tex}
% \input{chapters/diagnosing_q/sections/6_sampling.tex}
% \input{chapters/diagnosing_q/sections/7_nonstationarity.tex}


\section{Diagnostic Study in Simulation}
\label{app:sim_diagnostic}

We perform a diagnostic study in simulation to verify some of the insights observed in our real-world experiments. We created a bin sort task, where a WidowX250 robot is placed in front two bins and is provided with two objects (more details in Appendix~\ref{app:exp_setup}). The task is to sort each object in the correct bin associated with that object. The pre-training data provided to this robot is pick-place data, which only demonstrates how to pick \emph{one} of the objects and place it in one of the bins, but does not demonstrate the compound task of placing both objects. In order to succeed at this such a compound task, a robot must learn an abstract representation of the skill of sorting an object during the pre-training phase and then figure out that it needs to apply this skill multiple times in a trajectory to succeed at the task from just \emph{five} demonstrations of the desired sorting behavior.

The performance numbers (along with 95\%-confidence intervals) are shown in Table~\ref{tab:sim_complete}. Observe that \ptrmethodname improves upon prior methods in a statistically significant manner, outperforming the BC and COG baselines by a significant margin. This validates the efficacy of \ptrmethodname in simulation and corroborates our real-world results. 


\begin{table}[h]
\centering
\resizebox{0.4\textwidth}{!}{\begin{tabular}{l|r}
\toprule
\textbf{Method} & \textbf{Success rate}  \\ \midrule
BC (joint training) & 7.00 $\pm$ 0.00 \% \\
COG (joint training) & 8.00 $\pm$ 1.00 \% \\
BC (finetune) & 4.88 $\pm$ 4.07 \% \\ \midrule
\textbf{\ptrmethodname (Ours)} & \textbf{17.41 $\pm$ 1.77 \%} \\
\bottomrule
\end{tabular}}
\caption{\label{tab:sim_complete} \footnotesize{\textbf{Performance of \ptrmethodname in comparison with other methods} on the simulated bin sorting task, trained for many more gradient steps for all methods until each one of them converges. Observe that \ptrmethodname substantially outperforms other prior methods, including joint training on the same data with BC or CQL. Training on target data only is unable to recover a non-zero performance, so we do not report it in this table. Since the 95\%-confidence intervals do not overlap between \ptrmethodname and other methods, it indicates that \ptrmethodname improves upon baselines in a statistically significant manner.}}
\end{table}



\section{Details of Our Experimental Setup}
\label{app:exp_setup}

\vspace{0.1cm}
\subsection{Real-World Experimental Setup}

A picture of our real-world experimental setup is shown in Figure \ref{fig:setup_overview}. The scenarios considered in our experiments (Section~\ref{sec:result}) are designed to evaluate the performance of our method under a variety of situations and therefore we set up these tasks in different toykitchen domains (see Figure \ref{fig:setup_overview}) on three different WidowX 250 robot arms. We use data from the bridge dataset~\citep{ebert2021bridge} consisting of data collected with many robots in many domains for training but exclude the task/domain that we use for evaluation from the training dataset.  

\begin{figure}[h]
\centering
  \includegraphics[width=\linewidth]{chapters/ptr/setup_overview.pdf}
  \caption{\footnotesize{\textbf{Setup Overview}: Following \citet{ebert2021bridge}, we use a toykitchen setup described in that prior work for our experiments. This utilizes a 6-DoF WidowX 250 robot. \textbf{(1):}  Held-out toykitchen used for experiments in Scenario 3 (denoted ``toykitchen 6''), \textbf{(2):}  Re-targeting toykitchen used for experiments in Scenario 2 (denoted ``toykitchen 2''), \textbf{(3):} target objects used in the experiments of scenario 3.}, \textbf{(4):} the held-out kitchen setup used for door opening (``toykitchen 1'').}
  \label{fig:setup_overview}
  \vspace{-0.3cm}
\end{figure}



\subsection{Simulation Setup}
\label{sec:sim_appendix}

\begin{figure}{}
\centering
    \includegraphics[width=0.4\linewidth]{chapters/ptr/binsort_figure.jpg}
  \caption{\footnotesize{\textbf{Bin-Sorting task used for our simulated evaluations.} The task requires sorting the cylinder into the left bin and the teapot into the right bin.}}
  \label{app:sim_setup}
\end{figure}

We evaluate our approach in a simulated bin-sorting task on the simulated WidowX 250 platform, aimed to mimic the setup we use for our real-world evaluations. This setup is designed in the PyBullet simulation framework provided by \citet{singh2020cog}. A picture is shown in Figure \ref{app:sim_setup}. In this task, two different bins and two different objects are placed in front of the WidowX robot. The goal of the robot is to correctly sort each of the two objects to their designated bin (e.g the cylinder is supposed to be placed in the left bin and the teapot should be placed in the right bin. We refer to this task as a \emph{compound} task since it requires successfully combining behaviors of two different pick-and-place skills one after the other in a single trajectory while also adequately identifying the correct bin associated with each object. Success is counted only when the robot can accurately sort \emph{both} of the objects into their corresponding bins.

\textbf{Offline pre-training dataset.} The dataset provided for offline pre-training only consists of demonstrations that show how the robot should pick one of the two objects and place it into one of the two bins. Each episode in the pre-training dataset is about 30-40 timesteps long. A picture showing some trajectories from the pre-training dataset is shown in Figure \ref{fig:app_pretrain_rollout_sim}. While the downstream task only requires solving this sorting task with two specific objects (shown in Figure \ref{fig:app_targ_rollout_sim}), the pre-training data consists of 10 unique objects (some shown in Figure \ref{fig:app_pretrain_rollout_sim}). The two target objects that appear together in the downstream target scene are never seen together in the pre-training data. Since the pre-training data only demonstrates how the robot must pick up one of the objects and place it in one of the two bins (not necessarily in the target bin that the target task requires), it neither consists of any behavior that places objects into bins sequentially nor does it consist of any behavior where one of the objects is placed one of the bins while the other one is not. This is what makes this  task particularly challenging.

\textbf{Target demonstration data.} The target task data provided to the algorithm consists of only \textbf{\emph{five}} demonstrations that show how the robot must complete both the stages of placing both objects (see Figure \ref{fig:app_targ_rollout_sim}). Each episode in the target demonstration data is 80 timesteps long, which is substantially longer than any trajectory in the pre-training data, though one would hope that good representations learned from the pick and place tasks are still useful for this target task. While all methods are able to generally solve the first segment of placing the first object into the correct bin, the primary challenge in this task is to effectively sort the second object, and we find that \ptrmethodname attains a substantially better success rate than other baselines in this exact step.  

\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{chapters/ptr/BinsortPretrainTrajAppendix.pdf}
  \caption{\label{fig:app_pretrain_rollout_sim} \footnotesize {Some trajectories from the pre-training data used in the simulated bin-sort task.}}
\end{figure*}

\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{chapters/ptr/BinsortTargetTrajAppendix.pdf}
  \caption{\label{fig:app_targ_rollout_sim} \footnotesize {The five demonstration trajectories used for Phase 2 of \ptrmethodname.}}
\end{figure*}

\section{Description of the Real-World Evaluation Scenarios}
\label{app:tasks}
In this section, we describe the real-world evaluation scenarios considered in Section~\ref{sec:result}. We additionally include a much more challenging version of Scenario 3, for which we present results in Appendix~\ref{app:exp_results}. These harder test cases evaluate the fine-tuning performance on four different tasks, starting from the same initialization trained on bridge data except the toykitchen 6 domain in which these four tasks were set up. In the following sections, the nomenclature for the toy kitchens is drawn from \citet{ebert2021bridge} and as described in the caption of Figure~\ref{fig:setup_overview}.

\subsection{Scenario 1: Re-targeting skills for existing to solve new tasks}

\begin{figure}
\centering
  \includegraphics[width=0.83\linewidth]{chapters/ptr/scenario1_overview.pdf}
  \caption{\footnotesize \textbf{Illustration of pre-training data and finetuning data used for Scenario 1}: re-targeting the put sushi in metal-pot behavior to put the object in the metal pot instead of the orange transparent pot.}
  \label{fig:retargeting_setup}
\end{figure}


\textbf{Pre-training data.} The pre-training data comprises all of the pick and place data from the bridge dataset~\citep{ebert2021bridge} from toykitchen 2. This includes data corresponding to the task of putting the sushi in the transparent orange pot (Figure \ref{fig:retargeting_setup}).  

\textbf{Target task and data.} Since our goal in this scenario is to re-target the skill for putting the sushi in the transparent orange pot to the task of putting the sushi in the metallic pot, we utilize a dataset of 20 demonstrations that place the sushi in a metallic pot as our target task data that we fine-tune with (shown in Figure~\ref{fig:retargeting_setup}). 

\textbf{Quantitative evaluation protocol.} For our quantitative evaluations in Table \ref{tab:retarget}, we run 10 controlled evaluation rollouts that place the sushi and the metallic pot in different locations of the workspace. In all runs, the arm starts at up to 10 cm distance above the target object. The initial object and arm poses and positions are matched as closely as possible for different methods.

\subsection{Scenario 2: Generalizing to Previously Unseen Domains}

\begin{figure}
\centering
  \includegraphics[width=0.83\linewidth]{chapters/ptr/scenario2_overview.pdf}
  \caption{\footnotesize \textbf{Illustration of pre-training data and fine-tuning data used for Scenario 2 (door opening)}: transferring a behavior to a held-out domain.}
  \vspace{-0.5cm}
  \label{fig:door_open_setup}
\end{figure}


\textbf{Pre-training data.} The pre-training data in Scenario 2 consists of 800 door-opening demonstrations on 12 different doors across 3 different toykitchen domains.

\textbf{Target task and data.} The target task requires opening the door of an unseen microwave in toykitchen 1 using a target dataset of only 15 demonstrations.

\textbf{Quantitative evaluation protocol.} We run 20 rollouts with each method, counting successes when the robot opened the door by at least 45 degrees. To perform this successfully, there is a degree of complexity as the robot has to initially open the door till it's open to about 30 degrees. Then due to physical constraints, the robot needs to wrap around the door and push it open from the inside. To begin an evaluation rollout, we reset the robot to randomly sampled poses obtained from held-out demonstrations on the target door.  This is a compound task requiring the robot to first grab the door by the handle, next move around the door, and finally push the door open. As before, we match the initial pose of the robot as closely as possible for all the methods. 


\subsection{Scenario 3: Learning to Solve New Tasks in New Domains}

\begin{figure}
\centering
  \includegraphics[width=0.83\linewidth]{chapters/ptr/scenario3_overview.pdf}
  \caption{\footnotesize {\textbf{Illustration of pre-training data and fine-tuning data used for the new tasks we have added in Scenario 3}. The goal is to learn to solve new tasks in new domains starting from the same pre-trained initialization and when fine-tuning is only performed using 10-20 demonstrations of the target task.}}
  \label{fig:scenario4_overview}
\end{figure}

\textbf{Pre-training data.} All pick-and-place data in the bridge dataset~\citep{ebert2021bridge} except any demonstration data collected in toykitchen~6, where our evaluations are performed.

\textbf{Target task and data.} The target task requires placing corn in a pot in the sink in the new target domain and the target dataset provides 10 demonstrations for this task. These target demonstrations are sampled from the bridge dataset itself.

\textbf{Quantitative evaluation protocol.} During the evaluation we were unable to exactly match the camera orientation used to collect the target demonstration trajectories, and therefore ran evaluations with a slightly modified camera view. This presents an additional challenge for any method as it must now generalize to a modified camera view of the target toykitchen domain, without having ever observed this domain or this camera view during training. We sampled initial poses for our method by choosing transitions from a held-out dataset of demonstrations of the target task and resetting the robot to those initial poses for each method. We attempted to match the positions of objects across methods as closely as possible.

\subsection{More Tasks in Scenario 3: Learning to Solve Multiple New Tasks in New Domains From the Same Initialization}
\label{app:scenario4}

In Appendix~\ref{app:exp_results}, we have now added results for more tasks in Scenario 3. The details of these tasks are as follows:

\textbf{Pre-training data.} All pick-and-place data from bridge dataset~\citep{ebert2021bridge} except data from toykitchen~6.

\textbf{Target task and data.} We consider four downstream tasks: take croissant from a metallic bowl, put sweet potato on a plate, place the knife in a pot, and put cucumber in a bowl. We collected 10 target demonstrations for the croissant, sweet potato, and put cucumber in bowl tasks, and 20 target demonstrations for the knife in pot task. A picture of these target tasks is shown in Figure \ref{fig:scenario4_overview}.

\textbf{Qualitative evaluation protocol.} For our evaluations, we utilize either 10 or 20 evaluation rollouts. As with all of our other quantitative results, we evaluate all the baseline approaches and \ptrmethodname starting from an identical set of initial poses for the robot. These initial poses are randomly sampled from the poses that appear in the first 10 timesteps of the held-out demonstration trajectories for this target task. For the configuration of objects, we test our policies in a variety of task-specific configurations that we discuss below:

\begin{itemize}
    \item \textbf{Take croissant from metallic bowl:} For this task, we alternate between two kinds of positions for the metallic bowl. In the ``easy'' positions, the metallic bowl is placed roughly vertically beneath the robot's initial starting pose, whereas in the ``hard'' positions, the robot must first move itself to the right location of the bowl and then execute the policy.
    \item \textbf{Put the cucumber in bowl:} We run 10 evaluation rollouts starting from 10 randomly sampled initial poses of the robot for our evaluations. Here we moved the bowl between the two stovetops in each trial. 
    \item \textbf{Put sweet potato on plate:} For this task, we performed 20 evaluation rollouts. We only sampled 10 initial poses for the robot, but for each position, we evaluated every policy on two orientations of the sweet potato (i.e., the sweet potato is placed on the table on its flat face or on its curved face). Each of these orientations presents some unique challenges, and evaluating both of them allows us to gauge how robust the learned policy is to changes in orientation. The demonstration data had a variety of orientations for the sweet potato object that differed for each collected trajectory. 
    \item \textbf{Place knife in pot:} We evaluate this task over 10 evaluation rollouts, where the first five rollouts use a smaller knife, while the other five rollouts use a larger knife (shown in Figure~\ref{fig:setup_overview}). Each knife was seen in the demonstration dataset with equal probability.
\end{itemize}

We will discuss the results obtained on these new tasks in Appendix~\ref{app:exp_results}.


\section{Additional Experimental Results}
\label{app:exp_results}

\begin{figure}[h]
\vspace{-0.3cm}
\centering
  \includegraphics[width=0.9\linewidth]{chapters/ptr/MultViewpoint.pdf}
  \vspace{-0.3cm}
  \caption{\footnotesize \textbf{Sample observations from different camera viewpoints, only used during fine-tuning}. \textbf{Left:} the original camera viewpoint found in Figure~\ref{fig:scenario4_overview}. \textbf{Middle:} an elevated camera viewpoint where the robot and camera have been raised 7 cm. \textbf{Right:} a rotated camera viewpoint where the kitchen has been slightly translated and rotated 15 degrees counterclockwise relative to the camera and robot.}
  \label{fig:multviewpoint}
  \vspace{-0.2cm}
\end{figure}

\textbf{Finetuning to novel camera viewpoints:} Even though Scenario 3 already presents a novel toy-kitchen domain and previously unseen objects during finetuning, we also evaluate \ptrmethodname on a more challenging scenario where we additionally alter the camera viewpoint during finetuning. We apply two kinds of alterations to the camera: \textbf{(a)} we elevate the mounting platform of the camera by 7 cm, which necessitates adapting the way the physical coordinates of the robot end-effector are interpreted by the policy, and \textbf{(b)} we rotate the camera by about 15 degrees to induce a more oblique image observation than what was ever seen during pre-training. Note that in both of these scenarios, the robot has never encountered such camera viewpoints during pre-training, which makes this scenario even more challenging. The original dataset in \citep{ebert2021bridge} had the camera elevated to the same position for each domain and always ensured the kitchen was parallel to the camera platform, with translations being the primary changes in the scene for each domain. In Table \ref{tab:viewpoint_comp}, we present our results comparing \ptrmethodname and BC (finetune). Observe that \ptrmethodname still clearly outperforms BC (finetune), and attains performance close to that of \ptrmethodname in Table~\ref{tab:scenario4}, indicating that such shifts in the camera do not drastically hurt \ptrmethodname.


\begin{table}[h]
% \small{
\centering
\begin{tabular}{l|r|r}
\toprule
\textbf{Method} & \textbf{Elevated Viewpoint} & \textbf{Rotated Viewpoint}  \\ \midrule
BC (finetune) & 2/10  & 3/10  \\
\textbf{\ptrmethodname (Ours)} & \textbf{6/10}  & \textbf{7/10}  \\
\bottomrule
\end{tabular}
\vspace{-0.1cm}
\caption{\footnotesize{\textbf{Comparison of \ptrmethodname and BC (finetune), when evaluated on novel camera viewpoints} with elevated and rotated cameras as shown in Figure~\ref{fig:multviewpoint} for the croissant task. Observe that \ptrmethodname still outperforms BC (finetune) in this setting and attains more than 2x success rate of BC (finetune).}}
\label{tab:viewpoint_comp}
\end{table}

% \subsection{Expanded Discussion: Why Does \ptrmethodname Outperform BC-based methods, Even With Demonstration Data?}

% One natural question to ask given the results in this paper is: why does utilizing an offline RL method for pre-training and finetuning as in \ptrmethodname outperform BC-based methods even though the dataset is quite ``BC-friendly'', consisting of only demonstrations? One might speculate that an answer to this question is that our BC baseline can be tuned to be much better. However, note that our BC baseline is not suboptimally tuned. We utilize the procedure prescribed by prior work~\citep{ebert2021bridge} for tuning BC as we discuss in Appendix~\ref{app:hyperparams}. In addition, the fact that \textbf{BC (joint)} does actually outperform \textbf{CQL (joint)} in many of our experiments, indicates that our BC baselines are well-tuned. To explain the contrast to \citet{ebert2021bridge}, note that the setup in this prior work utilized many more target task demonstrations ($\geq 50$ demonstrations from the target task) compared to our evaluations, which might explain why our BC-baseline numbers are lower in an absolute sense. Therefore, the technical question still remains: why  would we expect \ptrmethodname to perform better than BC? We will attempt to answer this question using some empirical evidence and visualizations. Also, we will aim to provide intuition for why our approach \ptrmethodname outperforms the baseline.

% \begin{figure}[h]
% \centering
% \vspace{-0.4cm}
%   \includegraphics[width=0.83\linewidth]{Comparison.pdf}
%   \vspace{-0.5cm}
%   \caption{\footnotesize \textbf{Qualitative successes of \ptrmethodname visualized alongside failures of BC (finetune).} As an example, observe that while \ptrmethodname is accurately able to reach to the croissant and grasp it to solve the task, BC (finetune) is imprecise and grasps the bowl instead of the croissant resulting in failure.}
%   \label{fig:dumb_behavior2}
%   \vspace{-0.3cm}
% \end{figure}


% \textbf{To begin answering this question,} it is instructive to visualize some failures for a BC-based method and qualitatively attempt to understand why BC is worse than utilizing \ptrmethodname. We visualize some evaluation rollouts for \textbf{BC (finetune)} and \ptrmethodname as film strips in {Figure~\ref{fig:dumb_behavior2}}. Specifically, we visualize evaluation rollouts that present a challenging initial state. For example, for the rollout from the take croissant out of metallic pot task, the robot must first accurately position itself over the croissant before executing the grasping action. Similarly, for the rollout from the cucumber task, the robot must accurately locate the bowl and precisely try to grasp the cucumber. Observe in {Figure~\ref{fig:dumb_behavior}} that \textbf{BC (finetune)} typically fails to accurately reach the objects of interest (croissant and the bowl) and executes the grasping action prematurely. On the other hand, \ptrmethodname is more robust in these situations and is able to accurately reach the object of interest before it executes the grasping action or the releasing action. Why does this happen?  

% \textbf{To understand why this happens}, one mental model is to appeal to the critical states argument from \citet{kumar2022should}. Intuitively, this argument suggests that in tasks where the robot must precisely accomplish actions at only a few specific states (called ``\textbf{critical states}'') to succeed, but the actions at other states (called ``non-critical states'') do not matter as much. Thus, offline RL-style methods can outperform BC-based methods even with demonstration data. This is because learning a value function can enable the robot to reason about which states are more important than others, and the resulting policy optimization can ``focus'' on taking correct actions at such critical states. Our real-world evaluation scenarios exhibit such a structure. The majority of the actions that the robot must take to reach the object do not need to be precise as long as they generally move the robot in the right direction. However, in order to succeed, the robot must critically ensure to position the arm is right above the object in a correct orientation and position itself right above the container in which the object must be placed. These are the critical states and special care must be taken to execute the right action in these states. In such scenarios, the argument of \citet{kumar2022should} would suggest that offline RL should be better. We believe that we observe a similar effect in our experiments: the learned BC policies are often not precise-enough at those critical states where taking the right action is critical to success.  

% \begin{figure}[h]
% \centering
% \vspace{-0.4cm}
%   \includegraphics[width=0.78\linewidth]{FinetuningQvalPlot.pdf}
%   \vspace{-0.5cm}
%   \caption{\footnotesize \textbf{Evolution of Q-values on the target task} over the process of fine-tuning with \ptrmethodname. Observe that while the learned Q-values on \emph{held-out} trajectories from the dataset just at the beginning of Phase 2 (finetuning) do not exhibit a roughly increasing trend, the checkpoint of \ptrmethodname we choose to evaluate exhibits a generally increasing trend in the Q-values despite having access to only 10 demonstrations for these target tasks.}
%   \label{fig:finetunedQvals2}
%   \vspace{-0.3cm}
% \end{figure}

% {As supporting evidence} to the discussion above, we further visualize the Q-values over held-out trajectories from the target demonstration data that were never seen by \ptrmethodname during fine-tuning in {Figure~\ref{fig:finetunedQvals2}}. To demonstrate the contrast, we present the trend in Q-values before fine-tuning and for the checkpoint selected for evaluation after fine-tuning on the target task. Observe that the Q-values for the chosen checkpoint generally increase over the course of the trajectory indicating that the learned Q-function is able to fit well with the target data. Also, the learned Q-function generalizes to held-out trajectories despite the fact that only 10 demonstrations were provided during the fine-tuning phase. This evidence supports the claim that it is reasonable to expect the learned Q-function to be able to focus on the more critical decisions in the trajectory.

% \textbf{To further support our hypothesis that \ptrmethodname outperforms BC-based methods because the learned value function enables us to learn about ``critical'' decisions}, we run an experiment that essentially runs a weighted version of BC during finetuning, where the weights are provided by exponentiated advantage values, where the advantages are defined as $A_\theta(\bs, \mathbf{a}) = Q_\theta(\bs, \mathbf{a}) - \max_{\mathbf{a}'} Q_\theta(\bs, \mathbf{a}')$ under a Q-function learned by \ptrmethodname. This approaches essentially matches BC finetuning in all aspects: the policy parameterization, the loss function (mean-squared error), and the details of the training are kept identical to our BC baselines, with the exception of an additional weight given by $\exp(A_\theta(\bs, \mathbf{a}))$ on a given transition $(\bs, \mathbf{a}, r, \bs')$ observed in the set of limited task-specific demonstrations. We refer to this approach as ``advantage-weighted BC finetuning''.

% In contrast to our BC (finetune) results from Table~\ref{tab:scenario4}, where \ptrmethodname significantly outperformed BC (finetune), observe in Table~\ref{tab:aw_bc}, that advantage-weighted BC (finetune) performs comparably to \ptrmethodname on the two tasks we studied for our analysis. This result is significant since it implies that all other factors kept identical, utilizing the weights given by the Q-function is the crucial factor in improving the performance of BC and avoids the qualitative failure modes associated with BC methods shown in Figure~\ref{fig:dumb_behavior2}.

% \begin{table}[h]
% % \small{
% \centering
% \resizebox{0.99\linewidth}{!}{\begin{tabular}{c|r|r||r}
% \toprule
% \textbf{Task} & \textbf{BC (finetune)} & \textbf{\ptrmethodname (Ours)} &\textbf{Advantage-weighted BC (finetune)}  \\ \midrule
% Put cucumber in pot &  0/10 & 5/10 &  {5/10} \\
% Take croissant from metal bowl & 3/10 &  7/10  & {6/10} \\
% \bottomrule
% \end{tabular}}
% \vspace{0.1cm}
% \caption{\footnotesize{\textbf{Performance of advantage-weighted BC} on two tasks from Table~\ref{tab:scenario4}. Observe that weighting the BC objective using advantage-weights computed using the Q-function learned by \ptrmethodname leads to much better performance than standard BC (finetune), and close to PTR. This test indicates that the Q-function in \ptrmethodname allows us to focus on more critical points, thereby preventing the failures discussed in Figure~\ref{fig:dumb_behavior2}.}}
% \label{tab:aw_bc}
% \end{table}


\subsection{Hyperparameters for \ptrmethodname and Baseline Methods}
\label{app:hyperparams}

In this section, we will present the hyperparameters we use in our experiments and explain how we tune the other hyperparameters for both our method \ptrmethodname and the baselines we consider.  

\textbf{\ptrmethodname.} Since \ptrmethodname utilizes CQL as the base offline RL method, it trains two Q-functions and a separate policy, and maintains a delayed copy of the two Q-functions, commonly referred to as target Q-functions. We utilize completely independent networks to represent each of these five models (2 Q-functions, 2 target Q-functions, and the policy). We also do not share the convolutional encoders among them. As discussed in the main text, we rescaled the action space to $[-1, 1]^{|\mathcal{A}|}$ to match the one used by actor-critic algorithms, and utilized a Tanh squashing function at the end of the policy. We used a CQL $\alpha$ value of 10.0 for our pick-and-place experiments. The rest of the hyperparameters for training the Q-function, the target network updates, and the policy are taken from the standard training for image-based CQL from \citet{singh2020cog} and are presented in Table~\ref{tab:hparams_cql} below for completeness. The hyperparameters we choose are essentially the network design decisions of \textbf{(1)} utilizing group normalization instead of batch normalization, \textbf{(2)} utilizing learned spatial embeddings instead of standard mean pooling, \textbf{(3)} passing in actions at each of the fully connected layers of the Q-network and the hyperparameter $\alpha$ in CQL that must be adjusted since our data consists of demonstrations. We will ablate the new design decisions explicitly in Appendix~\ref{app:design}.

\begin{table*}[h]
% \small{
\centering
\begin{tabular}{l|c}
\toprule
\textbf{Hyperparameter} & \textbf{Value}\\  \midrule
Q-function learning rate & 3e-4 \\
Policy learning rate & 1e-4 \\
Target update rate & 0.005 (soft update with Polyak averaging) \\
Optimizer type & Adam \\
Discount factor $\gamma$ & 0.96 (since trajectories have a length of only about 30-40) \\
Use terminals & True \\
Reward shift and scale & shift = -1, scale = 10.0 \\
CQL $\alpha$ & 10.0 \\
Use Color Jitter & True \\
Use Random Cropping & True \\
\bottomrule
\end{tabular}
\vspace{0.07cm}
\caption{\footnotesize{\textbf{Main hyperparameters for CQL training in our real-world experiments.} In the simulation, we utilize a smaller $\alpha$ for CQL, $\alpha=1.0$, and a larger discount $\gamma = 0.98$ since trajectories in the simulation are about 60-70 timesteps in length. }}
\label{tab:hparams_cql}
% \vspace{-0.5cm}
\end{table*}

The only other hyperparameter used by \ptrmethodname is the mixing ratio $\tau$ that determines the proportion of samples drawn from the pre-training dataset and the target dataset during the offline finetuning phase in \ptrmethodname. We utilize $\tau = 0.7$ for our experiments with \ptrmethodname in the main paper, and use $\tau = 0.9$ for the additional experiments we added in the Appendix. This is because $\tau=0.9$ (more bridge data, and a smaller amount of target data) was helpful in scenarios with very limited target data.  

In order to perform checkpoint selection for \ptrmethodname, we utilized the trends in the learned Q-values over a set of held-out trajectories on the target data as discussed in Section~\ref{sec:design_choices}. We did not tune any other algorithmic hyperparameters for CQL, as these were taken directly from \citep{singh2020cog}.  

\textbf{BC (finetune).}
We trained BC in a similar manner as \citet{ebert2021bridge}, utilizing the design decisions that this prior work found optimal for their experiments. The policy for BC utilizes the very same ResNet 34 backbone as our RL policy since a backbone based on ResNet 34 was found to be quite effective in \citet{ebert2021bridge}. Following the recommendations of \citet{ebert2021bridge} and based on result trends from our own preliminary experiments, we chose to not utilize the tanh squashing function at the end of the policy for any BC-based method, but trained a deterministic BC policy that was trained to regress to the action in the demonstration with a mean-squared error (MSE) objective. 

\begin{table}[h]
% \small{
\centering
\begin{tabular}{l|c}
\toprule
\textbf{Hyperparameter} & \textbf{Value}\\  \midrule
Policy learning rate & 1e-4 \\
Optimizer type & Adam \\
Use Color Jitter & True \\
Use Random Cropping & True \\
Dropout & 0.4 \\
\bottomrule
\end{tabular}
\vspace{0.07cm}
\caption{\footnotesize{\textbf{Main hyperparameters for Behavior Cloning Baseline Training in our real-world and simulation experiments.} Note: architecture design choices follow closely to \ptrmethodname design choices.}}
\label{tab:hparams_cql}
\vspace{-0.4cm}
\end{table}

In order to perform cross-validation, checkpoint, and model selection for our BC policies, we follow guidelines from prior work~\citep{ebert2021bridge,emmons2021rvs} and track the MSE on a held-out validation dataset similar to standard supervised learning. We found that a ResNet 34 BC policy attained the smallest validation MSEs in general, and for our evaluations, we utilized a checkpoint of a ResNet 34 BC policy that attained the smallest MSE.   

Analogous to the case of \ptrmethodname discussed above, we also ablated the performance of BC for a set of varying values of the mixing ratio $\tau$, but found that a large value of $\tau = 0.9$ was the most effective for BC, and hence utilized $\tau = 0.9$ for BC (finetune) and BC (joint).

\textbf{BC (joint) and CQL (joint).} The primary distinction between training \textbf{BC (joint)} and \textbf{BC (finetune)} and correspondingly, \textbf{CQL (joint)} and \ptrmethodname was that in the case of joint training, the target dataset was introduced right at the beginning of Phase 1 (pre-training phase), and we mixed the target data with the pre-training data using the same value of the mixing ratio $\tau$ used in for our fine-tuning experiments to ensure a fair comparison.


\textbf{Few-shot offline meta-RL (MACAW)~\citep{mitchell2021offline}:} We compare to two variants of this algorithm and perform an \textbf{extensive} sweep over several hyperparameters, shown in Table~\ref{tab:hparams_macaw}. 
We trained two different variants of MACAW in our evaluation: \textbf{(1)} Pre-training on the bridge data in Scenario 3 and then fine-tuning on target data of interest, and \textbf{(2)} adapting a set of existing task identifiers to the target task of interest utilizing the same pre-training and fine-tuning domains. We performed early stopping on the meta-training based on validation losses. From there, we started the meta-testing phase, adapting to the target domain of interest. Following \citet{mitchell2021offline}, we use a task mini-batch of 8 tasks at each step of optimization rather than using all of the training tasks. We clipped the advantage weight logits to the scale of 20 and attempted to utilize a policy network with a fixed and learned standard deviation. Additionally, we varied the number of Adaptation steps following prior work. Our evaluation protocol for MACAW entails utilizing the validation losses to choose an initial checkpoint for evaluation. Then, we consider checkpoints in the neighborhood ($\pm$ 50K gradient steps) to for evaluations as well and chose the max over all of these checkpoints as the final evaluation success rate.


{
Quantitatively, as seen in Table~\ref{tab:scenario4}, MACAW was unable to get non-zero success rates on any of the tasks we study. However, we did qualitatively observe nontrivial behavior seen in our evaluation rollouts. For instance, we found that the policies trained via MACAW could consistently grasp the object of interest but were unable to localize where to place the object correctly. Several trials involved hovering around with the object of interest and not placing the object in the container. Other trials involved the agent failing to grasp the object.
}

\begin{table}[h]
% \small{
\centering
\begin{tabular}{l|c}
\toprule
\textbf{Hyperparameter} & \textbf{Value}\\  \midrule
Optimizer & Adam \\
Outer Policy learning rate & 1e-4 \\
Outer Value learning rate & 1e-5, 1e-6 \\
Inner Policy learning rate & 1e-2, 1e-3 \\
Inner Value learning rate & 1e-3, 1e-4 \\
Auxilary Advantage Coefficient & 1e-2, 1e-3, 1e-4 \\
Policy Parameterization & Fixed std, Learned std \\
AWR Policy Temperature & 1, 10, 20 \\
Number of Adaptation Steps & 1, 2, 3 \\
Task Batch Size & 8 \\
Train Adaptation Batch Size & 64 \\
Eval Adaptation Batch Size & 64 \\
Max Advantage Clip & 20 \\
Use Color Jitter & True \\
Use Random Cropping & True \\

\bottomrule
\end{tabular}
\vspace{0.07cm}
\caption{{\footnotesize{\textbf{Main hyperparameters for Training MACAW~\citep{mitchell2021offline} in our real-world experiments.} Note: architecture design choices follow closely to \ptrmethodname design choices but hyperparameter design choices follow closely the suggestions in \citet{mitchell2021offline}.}}}
\label{tab:hparams_macaw}
\vspace{-0.2cm}
\end{table}

\textbf{Pre-trained R3M initialization~\citep{nair2022r3m}:} Next we compare \ptrmethodname to utilizing an off-the-shelf pre-trained representation given by R3M~\citep{nair2022r3m}. We compare two baselines that attempt to train an MLP policy on top of the R3M state representation by using BC (finetuning) and CQL (finetuning) respectively. To ensure that this baseline is well-tuned, we tried a variety of network sizes with 2, 3 or 4 MLP layers and also tuned the hidden dimension sizes in [256, 512, 1024]. We also utilized dropout as regularization to prevent overfitting and tuned a variety of values of dropout probability in [0, 0.1, 0.2, 0.4, 0.6, 0.8]. We observe in Table~\ref{tab:scenario4}, that on the four tasks we evaluate on, \ptrmethodname outperforms R3M, which indicates that training on the bridge dataset can indeed give rise to effective visual representations that are more suited to finetuning in our setting. The numbers we report in the table are the best over each parametric policy corresponding to each hyperparameter in our ablation. Checkpoint selection was done utilizing early stopping which is the last iteration where the validation error stops decreasing. 
% Learning curves for this baseline can be found in our Anonymous Website.

\textbf{Pre-trained MAE initialization~\citep{he2111masked}:}
We took a similar training procedure to R3M for our MAE representation. We used an MAE trained on every image from the bridge dataset \citet{ebert2021bridge}. We then fine-tuned a specific target task with a similar ablation on network size, hidden dimension size, and regularization techniques such as dropout.  We observe in Table~\ref{tab:rep_learning_comparison}, that on the four tasks we evaluate on, \ptrmethodname outperforms R3M, which indicates that training on the bridge dataset can indeed give rise to effective visual representations that are more suited to finetuning in our setting. The numbers we report in the table are the best over each parametric policy corresponding to each hyperparameter in our ablation. Checkpoint selection was done utilizing early stopping which is the last iteration where the validation error stops decreasing. 

\textbf{Policy expressiveness study.}
We considered two policy expressiveness choices for BC to compare with our reference BC implementation that is implemented with a set of MLP layers. The first of the two choices was an \textbf{autoregressive policy} where the 7-dimensional action space was discretized into 100 bins. Each action was then predicted autoregressively conditioned on the observation, task id, and the action component from the previous dimension(s). The second approach was with the BeT Architecture from \citet{shafiullah2022behavior}. We utilized the reference implementation from the paper with the default suggested hyperparameters for this set of ablations. The window size for the MinGPT transformer was ablated over between 1, 2, and 10.

\section{Validating the Design Choices from Section~\ref{sec:design_choices}}
\label{app:design}

In this section, we will present ablation studies aimed to validate the design choices utilized by \ptrmethodname. We found these design choices quite crucial for attaining good performance. The concrete research questions we wish to answer are: \textbf{(1)} How effective is a learned spatial embedding compared to other approaches for aggregating spatial information? \textbf{(2)} Is concatenating actions at each fully-connected layer of the Q-function crucial for good performance?, \textbf{(3)} Is group normalization a good alternative to batch normalization? and \textbf{(4)} How does our choice of creating binary rewards for training affect the performance of \ptrmethodname?. We will answer these questions next.

% \begin{figure}
% % \vspace{-1cm}
% \includegraphics[width=0.9\linewidth]{chapters/ptr/scaling_ptr.pdf}
% % \vspace{-0.1cm}
% \caption{\footnotesize{\label{fig:scaling_ptr2} \textbf{Scaling trends for \ptrmethodname} on the open door task from Scenario 2, and average over two pick and place tasks (take croissant out of the metallic pot and put cucumber in the bowl) from Scenario 3. Note that more high capacity and expressive function approximators lead to the best results.}}
% % \vspace{-0.6cm}
% \end{figure}

% \textbf{Highly expressive Q-networks are essential for good performance.} To assess the importance of highly expressive Q-functions, we evaluate the performance of \ptrmethodname with varying sizes and architectures on three tasks: the open door task from Scenario 2, and the put cucumber in the pot and take croissant out of metallic bowl tasks from Scenario 3. Our choice of architectures is as follows: \textbf{(a)} a standard three-layer convolutional network typically used by prior work for DM-control tasks (see for example, \citet{kostrikov2021offline}), \textbf{(b)} an IMPALA~\citep{espeholt2018impala} ResNet that consists of 15 convolutional layers spread across a stack of 3 residual blocks, \textbf{(c)} ResNet 18 with group normalization and learned spatial embeddings, \textbf{(d)} ResNet 34 that we use in our experiments, and \textbf{(e)} an even bigger ResNet 50 with group normalization and learned spatial embeddings. 

% We present our results in {Figure~\ref{fig:scaling_ptr}}. To obtain more accurate scaling trends, we plot the trend in the average success rates for the pick and place tasks from Scenario 3 along with the trend in the success rate for the open door task separately since these tasks use different pre-training datasets. Observe that the performance of smaller networks (Small, IMPALA) is significantly worse than the ResNet in the door-opening task. For the pick and place tasks that contain a much larger dataset, Small, IMPALA, and ResNet18 all perform much worse than ResNet 34 and ResNet 50. We believe this result is quite exciting since it highlights the possibility of actually benefitting from using highly-expressive neural network models with TD-learning based RL methods trained on lots of diverse multi-task data (contrary to prior work~\citep{lee2022multi}). We believe that this result is a valuable starting point for further scaling and innovation.


\textbf{Learned spatial embeddings are crucial for performance.} Next we study the impact of utilizing the learned spatial embeddings for encoding spatial information when converting the feature maps from the convolutional stack into a vector that is fed into the fully-connected part of the Q-function. We compare our choice to utilizing a spatial softmax as in \citet{ebert2021bridge}, and also global average pooling (GAP) that simply averages over the spatial information, typically utilized in supervised learning with ResNets.


\begin{table}[h]
% \small{
\centering
% \vspace*{0.1cm}
\begin{tabular}{l|r}
\toprule
\textbf{Method} & \textbf{Success rate}\\  \midrule
PTR with spatial softmax & 4/10 \\
PTR with global average pooling & 4/10 \\
\midrule
PTR with learned spatial embeddings \textbf{(Ours)} & \textbf{7/10} \\
\bottomrule
\end{tabular}
\vspace{0.1cm}
\caption{\footnotesize{\textbf{Ablation of \ptrmethodname with spatial softmax and GAP on the croissant task.} Note that \ptrmethodname with learned spatial embeddings performs significantly better than using a spatial softmax or average pooling.}}
\label{tab:spatial}
\end{table}

As shown in {Table~\ref{tab:spatial}} learned spatial embeddings outperform both of these prior approaches on the put croissant in pot task. We suspect that spatial softmax does not perform much better than the GAP approach since the softmax operation can easily get saturated when running gradient descent to fit value targets that are not centered in some range, which would effectively hinder its expressivity. This indicates that the approach of retaining spatial information like in \ptrmethodname is required for attaining good performance.

\textbf{Concatenating actions at each layer is crucial for performance.} Next, we run \ptrmethodname without passing in actions at each fully connected layer of the Q-function on the take croissant out of metallic bowl task and only directly concatenate the actions with the output of the convolutional layers before passing it into the fully-connected component of the network. On the croissant task, we find that not passing in actions at each layer only succeeds in \textbf{2/10} evaluation rollouts, which is significantly worse than the default \ptrmethodname which passes in actions at each layer and succeeds in \textbf{7/10} evaluation rollouts (Table~\ref{tab:action_sep}).

\begin{table}[h]
% \small{
\centering
% \vspace*{0.1cm}
\begin{tabular}{l|r}
\toprule
\textbf{Method} & \textbf{Success rate}\\  \midrule
PTR without actions passed in at each FC layer & 2/10 \\
PTR with actions passed in at each FC layer (Ours) & \textbf{7/10} \\
\bottomrule
\end{tabular}
\vspace{0.1cm}
\caption{\footnotesize{\textbf{Ablation of \ptrmethodname with actions passed in at each layer.} Observe that passing in actions at each fully-connected layer does lead to quite good performance.}}
\label{tab:action_sep}
% \vspace{-0.5cm}
\end{table}

\textbf{Group normalization is more consistent than batch normalization.} Next, we ablate the usage of group normalization over batch normalization in the ResNet 34 Q-functions that \ptrmethodname uses. We found that batch normalization was generally harder to train to attain Q-function plots that exhibit a roughly increasing trend over the course of a trajectory. That said, on some tasks such as the croissant in pot task, we did get a reasonable Q-function, and found that batch normalization can perform well. On the other hand, on the put cucumber in pot task, we found that batch normalization was really ineffective. These results are shown in {Table~\ref{tab:batch_norm}}, and they demonstrate that batch normalization may not be as consistent and reliable with \ptrmethodname as group normalization.

\begin{table}
\centering
\scalebox{0.9}{
\begin{tabular}{l|r|r}
\toprule
\textbf{Method} & \textbf{Croissant out of metallic bowl} & \textbf{Cucumber in pot} \\  \midrule
PTR with batch norm. (relative) & + 28.0\% (7/10 $\rightarrow$ 9/10)& - 60.0\% (5/10 $\rightarrow$ 2/10) \\
\bottomrule
\end{tabular}
}
\vspace{0.1cm}
\caption{\footnotesize{\textbf{Relative performance of \ptrmethodname with batch normalization with respect to \ptrmethodname with group normalization.} Observe that while utilizing batch normalization in \ptrmethodname can be sometimes more effective than using group normalization (e.g., take croissant out of metallic bowl task), it may also be highly ineffective and can reduce success rates significantly in other tasks. The performance numbers to the left of the $\rightarrow$ correspond to the performance of \ptrmethodname with group normalization and the performance to the right of $\rightarrow$ is the performance with batch normalization.}}
\label{tab:batch_norm}
% \vspace{-0.5cm}
\end{table}


\textbf{Choice of the reward function.} Finally, we present some results that ablate the choice of the reward function utilized for training \ptrmethodname from data that entirely consists of demonstrations. In our main set of experiments, we labeled the last three timesteps of every trajectory with a reward of +1 and annotated all other timesteps with a 0 reward. We tried perhaps the most natural choice of labeling only the last timestep with a 0 reward on the croissant task and found that this choice succeeds \textbf{0/10} times, compared to annotating the last three timesteps with a +1 reward which succeeds \textbf{7/10} times. We suspect that this is because only annotating the last timestep with a +1 reward is not ideal for two reasons: first, the task is often completed in the dataset much earlier than the observation shows the task complete, and hence the last-step annotation procedure induces a non-Markovian reward function, and second, only labeling the last step with a +1 leads to overly conservative Q-functions when used with \ptrmethodname, which may not lead to good policies.

\section{More Details For Online Fine-tuning}
\label{app:online_finetuning}

\textbf{Offline pre-training.}
For both PTR and BC baseline, we used 40 open-door demonstrations as target task data and combined them with the Bridge Dataset to pre-train the policy. To reduce the training time in the real system, we used ResNet 18 backbones.

\textbf{Reset policy.}
For the reset policy, we additionally collected 22 close-door demonstrations as the target task data and pre-trained the policy with PTR. Similar to the open-door policy, we used ResNet 18 backbones to save training time.

\textbf{Reward classifier.} We used a ResNet 34 classification model and trained it to detect whether the door is open or closed from visual inputs. For the training data, we manipulated the robot to collect around 20 positive and negative trajectories for both open and closed doors.

\textbf{Method.} As discussed in Chapter~\ref{chapter:calql}, offline value function initializations from CQL may not be effective at fine-tuning if the learned Q-values are not at the same scale as the ground-truth return of the behavior policy. While this property does not affect offline performance, it is crucial to enforce this property during fine-tuning. That said, this property can be ``baked in'' by simply preventing the CQL regularizer from minimizing the learned Q-function if its values fall below the Monte-Carlo return of the trajectories in the dataset. Therefore, for the online fine-tuning experiment, we utilize \methodname\ incorporated into the offline CQL approach.

\textbf{Hyperparameters.} 
For both online fine-tuning with \ptrmethodname and SACfD, we performed the experiment by mixing the Bridge Dataset, offline target data, and the online data in a ratio ($\beta$) of 0.35, 0.15, and 0.5. For \ptrmethodname, we used the CQL alpha value of 5 for the offline phase and 0.5 for the online phase. 

\textbf{Evaluation.} The results shown in Figure~\ref{fig:online_door} were evaluated autonomously every 5K environment step during the online fine-tuning. Each evaluation was assessed with 10 trials, one from each initial position. The results shown in Figure~\ref{tab:online-finetune} were additionally evaluated over 3 trials from each initial position, using the offline initialization and the final checkpoint obtained after 20K environment steps of online fine-tuning.

\begin{figure}
% \vspace{-1cm}
\includegraphics[width=\linewidth]{chapters/ptr/dangerous_actions.jpeg}
% \vspace{-0.1cm}
\caption{\footnotesize{\label{fig:dangerous-actions} \textbf{Example of unsafe behaviors when running SACfD.} The robot collides with the camera during online exploration, resulting in a system crash.}}
% \vspace{-0.6cm}
\end{figure}



\end{document}