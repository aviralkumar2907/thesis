@STRING{aistats = {AISTATS}}
@STRING{cvpr = {CVPR}}
@STRING{emnlp = {EMNLP}}
@STRING{focs = {FOCS}}
@STRING{iccv = {ICCV}}
@STRING{icdr = {ICDR}}
@STRING{icml = {ICML}}
@STRING{ijcv = {IJCV}}
@STRING{jmlr = {JMLR}}
@STRING{nc = {Neural Comput.}}
@STRING{nips = {NeurIPS}}
@STRING{pami = {IEEE Trans. PAMI}}
@STRING{sigir = {SIGIR}}
@STRING{stoc = {STOC}}
@STRING{socg = {SoCG}}
@STRING{uai = {UAI}}
@STRING{vissapp = {VISSAPP}}
@STRING{eccv = {ECCV}}
@STRING{acl = {ACL}}
@STRING{iclr = {ICLR}}
@STRING{icassp = {ICASSP}}
@STRING{asru = {ASRU}}
@STRING{machlearning = {Mach. Learn. J.}}
@STRING{aaai = {AAAI}}
@STRING{naacl = {NAACL}}
@STRING{iros = {IROS}}
@STRING{tacl = {TACL}}
@STRING{corl = {CoRL}}
@STRING{ijcai = {IJCAI}}
@STRING{aamas = {AAMAS}}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={arXiv preprint arXiv:2106.01345},
  year={2021}
}

@article{cao2020heteroskedastic,
  title={Heteroskedastic and imbalanced deep learning with adaptive regularization},
  author={Cao, Kaidi and Chen, Yining and Lu, Junwei and Arechiga, Nikos and Gaidon, Adrien and Ma, Tengyu},
  journal={arXiv preprint arXiv:2006.15766},
  year={2020}
}

@inproceedings{
li2023efficient,
title={Efficient Deep Reinforcement Learning Requires Regulating Overfitting},
author={Qiyang Li and Aviral Kumar and Ilya Kostrikov and Sergey Levine},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=14-kr46GvP-}
}


@article{kostrikov2021offlineb,
  title={Offline reinforcement learning with implicit q-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@ARTICLE{2022arXiv221005178K,
       author = {{Kumar}, Aviral and {Singh}, Anikait and {Ebert}, Frederik and {Yang}, Yanlai and {Finn}, Chelsea and {Levine}, Sergey},
        title = "{Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Robotics, Computer Science - Machine Learning},
         year = 2022,
        month = oct,
          eid = {arXiv:2210.05178},
        pages = {arXiv:2210.05178},
          doi = {10.48550/arXiv.2210.05178},
archivePrefix = {arXiv},
       eprint = {2210.05178},
 primaryClass = {cs.RO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv221005178K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@inproceedings{
mandlekar2021what,
title={What Matters in Learning from Offline Human Demonstrations for Robot Manipulation},
author={Ajay Mandlekar and Danfei Xu and Josiah Wong and Soroush Nasiriany and Chen Wang and Rohun Kulkarni and Fei-Fei Li and Silvio Savarese and Yuke Zhu and Roberto Mart{\'\i}n-Mart{\'\i}n},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
url={https://openreview.net/forum?id=JrsfBJtDFdI}
}

@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={arXiv preprint arXiv:2103.12021},
  year={2021}
}


@article{ebert2021bridge,
  title={Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets},
  author={Ebert, Frederik and Yang, Yanlai and Schmeckpeper, Karl and Bucher, Bernadette and Georgakis, Georgios and Daniilidis, Kostas and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:2109.13396},
  year={2021}
}

@inproceedings{ettinger2021large,
  title={Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset},
  author={Ettinger, Scott and Cheng, Shuyang and Caine, Benjamin and Liu, Chenxi and Zhao, Hang and Pradhan, Sabeek and Chai, Yuning and Sapp, Ben and Qi, Charles R and Zhou, Yin and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9710--9719},
  year={2021}
}

@inproceedings{zheng2022online,
  title={Online decision transformer},
  author={Zheng, Qinqing and Zhang, Amy and Grover, Aditya},
  booktitle={International Conference on Machine Learning},
  pages={27042--27059},
  year={2022},
  organization={PMLR}
}

@article{Rafailov2020LOMPO,
  title={Offline Reinforcement Learning from Images with Latent Space Models},
  author={Rafael Rafailov and Tianhe Yu and A. Rajeswaran and Chelsea Finn},
  journal={Learning for Decision Making and Control (L4DC)},
  year={2021},
}

@article{argenson2020model,
  title={Model-Based Offline Planning},
  author={Argenson, Arthur and Dulac-Arnold, Gabriel},
  journal={arXiv preprint arXiv:2008.05556},
  year={2020}
}

@article{brandfonbrener2021offline,
  title={Offline RL Without Off-Policy Evaluation},
  author={Brandfonbrener, David and Whitney, William F and Ranganath, Rajesh and Bruna, Joan},
  journal={arXiv preprint arXiv:2106.08909},
  year={2021}
}

@article{kidambi2020morel,
  title={MOReL: Model-Based Offline Reinforcement Learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={arXiv preprint arXiv:2005.05951},
  year={2020}
}

@article{matsushima2020deployment,
  title={Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization},
  author={Matsushima, Tatsuya and Furuta, Hiroki and Matsuo, Yutaka and Nachum, Ofir and Gu, Shixiang},
  journal={arXiv preprint arXiv:2006.03647},
  year={2020}
}

@inproceedings{
lee2021representation,
title={Representation Balancing Offline Model-based Reinforcement Learning},
author={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=QpNz8r_Ri2Y}
}

@article{swazinna2020overcoming,
  title={Overcoming Model Bias for Robust Offline Deep Reinforcement Learning},
  author={Swazinna, Phillip and Udluft, Steffen and Runkler, Thomas},
  journal={arXiv preprint arXiv:2008.05533},
  year={2020}
}

@article{emmons2021rvs,
  title={RvS: What is Essential for Offline RL via Supervised Learning?},
  author={Emmons, Scott and Eysenbach, Benjamin and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2112.10751},
  year={2021}
}

@article{rezaeifar2021offline,
  title={Offline reinforcement learning as anti-exploration},
  author={Rezaeifar, Shideh and Dadashi, Robert and Vieillard, Nino and Hussenot, L{\'e}onard and Bachem, Olivier and Pietquin, Olivier and Geist, Matthieu},
  journal={arXiv preprint arXiv:2106.06431},
  year={2021}
}

@article{zhou2020plas,
  title={PLAS: Latent Action Space for Offline Reinforcement Learning},
  author={Zhou, Wenxuan and Bajracharya, Sujay and Held, David},
  journal={arXiv preprint arXiv:2011.07213},
  year={2020}
}


@article{chen2022latent,
  title={Latent-Variable Advantage-Weighted Policy Optimization for Offline RL},
  author={Chen, Xi and Ghadirzadeh, Ali and Yu, Tianhe and Gao, Yuan and Wang, Jianhao and Li, Wenzhe and Liang, Bin and Finn, Chelsea and Zhang, Chongjie},
  journal={arXiv preprint arXiv:2203.08949},
  year={2022}
}

@article{fujimoto2021minimalist,
  title={A Minimalist Approach to Offline Reinforcement Learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2106.06860},
  year={2021}
}

@article{wang2020critic,
  title={Critic regularized regression},
  author={Wang, Ziyu and Novikov, Alexander and {\.Z}o{\l}na, Konrad and Springenberg, Jost Tobias and Reed, Scott and Shahriari, Bobak and Siegel, Noah and Merel, Josh and Gulcehre, Caglar and Heess, Nicolas and others},
  journal={arXiv preprint arXiv:2006.15134},
  year={2020}
}


@article{yu2021combo,
  title={Combo: Conservative offline model-based policy optimization},
  author={Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2102.08363},
  year={2021}
}

@inproceedings{
kumar2022should,
title={Should I Run Offline Reinforcement Learning or Behavioral Cloning?},
author={Aviral Kumar and Joey Hong and Anikait Singh and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=AP1MKT37rJ}
}


@article{zhang2019all,
  title={Are all layers created equal?},
  author={Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01996},
  year={2019}
}

@article{agarwal2021precipice,
  title={Deep Reinforcement Learning at the Edge of the Statistical Precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel
          and Courville, Aaron and Bellemare, Marc G},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}


@inproceedings{snoek2015scalable,
  title={Scalable bayesian optimization using deep neural networks},
  author={Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},
  booktitle={International conference on machine learning},
  pages={2171--2180},
  year={2015}
}


@article{schrittwieser2021online,
  title={Online and offline reinforcement learning by planning with a learned model},
  author={Schrittwieser, Julian and Hubert, Thomas and Mandhane, Amol and Barekatain, Mohammadamin and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:2104.06294},
  year={2021}
}

@article{bouthillier2021accounting,
  title={Accounting for variance in machine learning benchmarks},
  author={Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko and Trofimov, Assya and Nichyporuk, Brennan and Szeto, Justin and Mohammadi Sepahvand, Nazanin and Raff, Edward and Madan, Kanika and Voleti, Vikram and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  year={2021}
}

@inproceedings{li2019towards, 
 title={Towards explaining the regularization effect of initial large learning rate in training neural networks}, 
 author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu}, 
 booktitle={Advances in Neural Information Processing Systems}, 
 pages={11674--11685}, 
 year={2019} 
 }

@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={May},
  pages={815--857},
  year={2008}
}

@article{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  journal={ICML},
  year={2019}
}

@book{hastie2015sparsity,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
title = {Statistical Learning with Sparsity: The Lasso and Generalizations},
year = {2015},
isbn = {1498712169},
publisher = {Chapman &amp; Hall/CRC},
}

@book{bellman1957dynamic,
  author =       "Bellman, Richard",
  title =        "Dynamic Programming",
  publisher =    "Princeton University Press",
  year =         "1957",
}


@inproceedings{boyan1999least,
  title={Least-squares temporal difference learning},
  author={Boyan, Justin A},
  booktitle={ICML},
  pages={49--56},
  year={1999},
  organization={Citeseer}
}

@article{igl2020impact,
  title={The Impact of Non-stationarity on Generalisation in Deep Reinforcement Learning},
  author={Igl, Maximilian and Farquhar, Gregory and Luketina, Jelena and Boehmer, Wendelin and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2006.05826},
  year={2020}
}

@article{xu2007kernel,
  title={Kernel-based least squares policy iteration for reinforcement learning},
  author={Xu, Xin and Hu, Dewen and Lu, Xicheng},
  journal={IEEE Transactions on Neural Networks},
  volume={18},
  number={4},
  pages={973--992},
  year={2007},
  publisher={IEEE}
}

@article{xu2005kernel,
  title={Kernel least-squares temporal difference learning},
  author={Xu, Xin and Xie, Tao and Hu, Dewen and Lu, Xicheng},
  journal={International Journal of Information Technology},
  volume={11},
  number={9},
  pages={54--63},
  year={2005}
}

@article{dabney2020value,
  title={The Value-Improvement Path: Towards Better Representations for Reinforcement Learning},
  author={Dabney, Will and Barreto, Andr{\'e} and Rowland, Mark and Dadashi, Robert and Quan, John and Bellemare, Marc G and Silver, David},
  journal={arXiv preprint arXiv:2006.02243},
  year={2020}
}

@techreport{townsend2016differentiating,
  title={Differentiating the singular value decomposition},
  author={Townsend, James},
  year={2016},
  institution={Technical Report 2016, https://j-towns. github. io/papers/svd-derivative~…}
}

@Inbook{Hlawka1991dirichlet,
author="Hlawka, Edmund
and Taschner, Rudolf
and Schoi{\ss}engeier, Johannes",
title="The Dirichlet Approximation Theorem",
bookTitle="Geometric and Analytic Number Theory",
year="1991",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--18",
isbn="978-3-642-75306-0",
doi="10.1007/978-3-642-75306-0_1",
url="https://doi.org/10.1007/978-3-642-75306-0_1"
}



@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning},
  pages={45--73},
  year={2012},
  publisher={Springer}
}

@book{duffy2015green,
  title={Green's functions with applications},
  author={Duffy, Dean G},
  year={2015},
  publisher={CRC Press}
}

@article{jaderberg2016reinforcement,
  title={Reinforcement learning with unsupervised auxiliary tasks},
  author={Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1611.05397},
  year={2016}
}

@article{dabney2017distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1710.10044},
  year={2017}
}

@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}

@article{yang2019harnessing,
  title={Harnessing structures for value-based planning and reinforcement learning},
  author={Yang, Yuzhe and Zhang, Guo and Xu, Zhi and Katabi, Dina},
  journal={arXiv preprint arXiv:1909.12255},
  year={2019}
}

@article{fedus2020catastrophic,
  title={On Catastrophic Interference in Atari 2600 Games},
  author={Fedus, William and Ghosh, Dibya and Martin, John D and Bellemare, Marc G and Bengio, Yoshua and Larochelle, Hugo},
  journal={arXiv preprint arXiv:2002.12499},
  year={2020}
}

@article{paine2020hyperparameter,
  title={Hyperparameter selection for offline reinforcement learning},
  author={Paine, Tom Le and Paduraru, Cosmin and Michi, Andrea and Gulcehre, Caglar and Zolna, Konrad and Novikov, Alexander and Wang, Ziyu and de Freitas, Nando},
  journal={arXiv preprint arXiv:2007.09055},
  year={2020}
}

@article{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang M and Voloshin, Cameron and Yue, Yisong},
  journal={arXiv preprint arXiv:1903.08738},
  year={2019}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9461--9471},
  year={2018}
}

@inproceedings{luo2020i4r,
  title     = {I4R: Promoting Deep Reinforcement Learning by the Indicator for Expressive Representations},
  author    = {Luo, Xufang and Meng, Qi and He, Di and Chen, Wei and Wang, Yunhong},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  editor    = {Christian Bessiere},	
  pages     = {2669--2675},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/370},
  url       = {https://doi.org/10.24963/ijcai.2020/370},
}


@inproceedings{
Sanyal2020Stable,
title={Stable Rank Normalization for Improved Generalization in Neural Networks and GANs},
author={Amartya Sanyal and Philip H. Torr and Puneet K. Dokania},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1enKkrFDB}
}

@book{puterman1994markov,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L},
  year={1994},
  publisher={John Wiley \& Sons, Inc.}
}

@article{huber1964robust,
  title={Robust estimation of a location parameter},
  author={Huber, PJ},
  journal={Ann. Math. Stat.},
  year={1964}
}


@article{mnih2013playing,
  title={{Playing Atari with deep reinforcement learning}},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv:1312.5602},
  year={2013}
}





@inproceedings{
daskalakis2018training,
title={Training {GAN}s with Optimism},
author={Constantinos Daskalakis and Andrew Ilyas and Vasilis Syrgkanis and Haoyang Zeng},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SJJySbbAZ},
}

@inproceedings{sutton1999PG,
  author    = {Richard S. Sutton and
               David A. McAllester and
               Satinder P. Singh and
               Yishay Mansour},
  editor    = {Sara A. Solla and
               Todd K. Leen and
               Klaus{-}Robert M{\"{u}}ller},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems 12, {[NIPS} Conference,
               Denver, Colorado, USA, November 29 - December 4, 1999]},
}

@PhdThesis{Watkins1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     "May",
  url = "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
  bib2html_rescat = "Parameter",
}

@inproceedings{hou2017novel,
  title={A novel ddpg method with prioritized experience replay},
  author={Hou, Yuenan and Liu, Lifeng and Wei, Qing and Xu, Xudong and Chen, Chunlin},
  booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={316--321},
  year={2017},
  organization={IEEE}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{horgan2018distper,
  author    = {Dan Horgan and
               John Quan and
               David Budden and
               Gabriel Barth{-}Maron and
               Matteo Hessel and
               Hado van Hasselt and
               David Silver},
  title     = {Distributed Prioritized Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1803.00933},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.00933},
  archivePrefix = {arXiv},
  eprint    = {1803.00933},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-00933.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{absention,
author = {Thulasidasan, Sunil and Bhattacharya, Tanmoy and Bilmes, Jeff and Chennupati, Gopinath and Mohd-Yusof, Jamal},
year = {2019},
month = {05},
booktitle = {Proceedings of 35th International Conference on Machine Learning},
title = {Combating Label Noise in Deep Learning Using Abstention}
}

@inproceedings{hassalt10doubleq,
 author = {Hasselt, Hado van},
 title = {Double Q-Learning},
 year = {2010},
 booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 2}
}

@article{ODonoghue2017TheUB,
  title={The Uncertainty Bellman Equation and Exploration},
  author={Brendan O'Donoghue and Ian Osband and R{\'e}mi Munos and Volodymyr Mnih},
  journal={ICML},
  year={2018},
  volume={abs/1709.05380}
}

@inproceedings{hazan2019maxent,
title	= {Provably Efficient Maximum Entropy Exploration},
author	= {Elad Hazan and Sham Kakade and Karan Singh and Abby Van Soest},
year	= {2019},
journal = {ICML},
URL	= {https://arxiv.org/pdf/1812.02690.pdf}
}


@TECHREPORT{Tsitsiklis97ananalysis,
    author = {John N. Tsitsiklis and Benjamin Van Roy},
    title = {An analysis of temporal-difference learning with function approximation},
    institution = {IEEE Transactions on Automatic Control},
    year = {1997}
}

@inproceedings{rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{zhang2017deeper,
  title={A deeper look at experience replay},
  author={Zhang, Shangtong and Sutton, Richard S},
  journal={arXiv preprint arXiv:1712.01275},
  year={2017}
}

@inproceedings{liu2018effects,
  title={The effects of memory replay in reinforcement learning},
  author={Liu, Ruishan and Zou, James},
  booktitle={2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  year={2018},
  organization={IEEE}
}

@inproceedings{suggala2018connecting,
  title={Connecting optimization and regularization paths},
  author={Suggala, Arun and Prasad, Adarsh and Ravikumar, Pradeep K},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10608--10619},
  year={2018}
}

@article{ruhe1975closeness,
  title={On the closeness of eigenvalues and singular values for almost normal matrices},
  author={Ruhe, Axel},
  journal={Linear Algebra and its Applications},
  volume={11},
  number={1},
  pages={87--93},
  year={1975},
  publisher={Elsevier}
}

@article{wu2019behavior,
  title={Behavior Regularized Offline Reinforcement Learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@InProceedings{fujimoto19a,
  title = 	 {Off-Policy Deep Reinforcement Learning without Exploration},
  author = 	 {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019},
}

@incollection{NIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8026--8037},
year = {2019},
}

@inproceedings{tensorflow,
title	= {TensorFlow: A system for large-scale machine learning},
author	= {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year	= {2016},
URL	= {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
booktitle	= {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
pages	= {265--283}
}

@inproceedings{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang and Voloshin, Cameron and Yue, Yisong},
  booktitle={International Conference on Machine Learning},
  pages={3703--3712},
  year={2019},
  organization={PMLR}
}

@inproceedings{
fu2021benchmarks,
title={Benchmarks for Deep Off-Policy Evaluation},
author={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=kWSeGEeHvF8}
}


@inproceedings{gulcehre2020rl,
  title={Rl unplugged: Benchmarks for offline reinforcement learning},
  author={Gulcehre, Caglar and Wang, Ziyu and Novikov, Alexander and Paine, Tom Le and Colmenarejo, Sergio G{\'o}mez and Zolna, Konrad and Agarwal, Rishabh and Merel, Josh and Mankowitz, Daniel and Paduraru, Cosmin and others},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@book{rummery1994line,
  title={On-line Q-learning using connectionist systems},
  author={Rummery, Gavin A and Niranjan, Mahesan},
  volume={37},
  year={1994}
}

@inproceedings{perkins2002api,
author = {Perkins, Theodore J. and Precup, Doina},
title = {A Convergent Form of Approximate Policy Iteration},
year = {2002},
series = {NIPS’02}
}

@inproceedings{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6151--6159},
  year={2017}
}

@article{lstd,
  title={Linear least-squares algorithms for temporal difference learning},
  author={Bradtke, Steven J and Barto, Andrew G},
  journal={Machine learning},
  volume={22},
  number={1-3},
  pages={33--57},
  year={1996},
  publisher={Springer}
}

@article{bellemare2013ale,
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
year = {2013},
issue_date = {May 2013},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {47},
number = {1},
issn = {1076-9757},
journal = {J. Artif. Int. Res.},
month = may,
pages = {253–279},
numpages = {27}
}
  
@article{yu2020gradient,
  title={Gradient Surgery for Multi-Task Learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  journal={arXiv preprint arXiv:2001.06782},
  year={2020}
}

  
@incollection{ntk,
title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
booktitle = {Advances in Neural Information Processing Systems 31},
year = {2018},
url = {}
}

@article{lorraine2019optimizing,
  title={Optimizing Millions of Hyperparameters by Implicit Differentiation},
  author={Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  journal={arXiv preprint arXiv:1911.02590},
  year={2019}
}


@inproceedings{peters2010reps,
author = {Peters, Jan and M\"{u}lling, Katharina and Alt\"{u}n, Yasemin},
title = {Relative Entropy Policy Search},
year = {2010},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
pages = {1607–1612},
numpages = {6},
location = {Atlanta, Georgia},
series = {AAAI’10}
}
  


@article{machado18sticky,
author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
title = {Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {523–562},
numpages = {40}
}
  


@inproceedings{bellemare2017distributional,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={449--458},
  year={2017},
  organization={JMLR. org}
}


@article{castro18dopamine,
  author    = {Pablo Samuel Castro and
               Subhodeep Moitra and
               Carles Gelada and
               Saurabh Kumar and
               Marc G. Bellemare},
  title     = {Dopamine: {A} {R}esearch {F}ramework for {D}eep {R}einforcement {L}earning},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.06110},
  archivePrefix = {arXiv}
}

@article{scherrer15a,
  author  = {Bruno Scherrer and Mohammad Ghavamzadeh and Victor Gabillon and Boris Lesner and Matthieu Geist},
  title   = {Approximate Modified Policy Iteration and its Application to the Game of Tetris},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {49},
  pages   = {1629-1676},
  url     = {http://jmlr.org/papers/v16/scherrer15a.html}
}

@article{Lesner2013TightPB,
  title={Tight Performance Bounds for Approximate Modified Policy Iteration with Non-Stationary Policies},
  author={Boris Lesner and Bruno Scherrer},
  journal={ArXiv},
  year={2013},
  volume={abs/1304.5610}
}

@inproceedings{scherrer_comparison,
author = {Scherrer, Bruno},
title = {Approximate Policy Iteration Schemes: A Comparison},
year = {2014},
publisher = {JMLR.org},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–1314–II–1322},
numpages = {9},
location = {Beijing, China},
series = {ICML’14}
}
  


@inproceedings{
du2020is,
title={Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
author={Simon S. Du and Sham M. Kakade and Ruosong Wang and Lin F. Yang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1genAVKPB}
}

@inproceedings{munos2003api,
author = {Munos, R\'{e}mi},
title = {Error Bounds for Approximate Policy Iteration},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {560–567},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML’03}
}
  


@inproceedings{yu2019meta,
  title={Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning},
  author={Tianhe Yu and Deirdre Quillen and Zhanpeng He and Ryan Julian and Karol Hausman and Chelsea Finn and Sergey Levine},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2019},
  eprint={1910.10897},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1910.10897},
}

@article{kumar19bear,
  author       = {Aviral Kumar and Justin Fu and George Tucker and Sergey Levine},
  title        = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
  conference   = {NeurIPS 2019},
  year = {2019},
  url          = {http://arxiv.org/abs/1906.00949},
}

@article{farias_fixed_points,
author = {Farias, D. and Roy, B.},
year = {2000},
month = {06},
pages = {589-608},
title = {On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning},
volume = {105},
journal = {Journal of Optimization Theory and Applications},
doi = {10.1023/A:1004641123405}
}

@inproceedings{Krantz2002TheIF,
  title={The Implicit Function Theorem: History, Theory, and Applications},
  author={Steven G. Krantz and Harold R. Parks},
  year={2002}
}

@InProceedings{ahmed19understanding,
  title = 	 {Understanding the Impact of Entropy on Policy Optimization},
  author = 	 {Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  publisher = 	 {PMLR},
  year = {2019},
  url = 	 {http://proceedings.mlr.press/v97/ahmed19a.html},
  abstract = 	 {Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.}
}

@book{rockafellar-1970a,
  added-at = {2008-03-02T02:12:02.000+0100},
  address = {Princeton, N. J.},
  author = {Rockafellar, R. Tyrrell},
  biburl = {https://www.bibsonomy.org/bibtex/223aa07ea525f6dd11585fc2037a0daf1/dmartins},
  callnumber = {UniM Maths 516.08 R59},
  description = {robotica-bib},
  interhash = {30830becb0a2c5ebca5946b895d9740a},
  intrahash = {23aa07ea525f6dd11585fc2037a0daf1},
  keywords = {imported},
  notes = {A SRL reference.},
  publisher = {Princeton University Press},
  series = {Princeton Mathematical Series},
  timestamp = {2008-03-02T02:14:11.000+0100},
  title = {Convex analysis},
  year = 1970
}


@article{Achiam2019TowardsCD,
  title={Towards Characterizing Divergence in Deep Q-Learning},
  author={Joshua Achiam and Ethan Knight and Pieter Abbeel},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.08894}
}

@Article{Tsitsiklis1994,
author="Tsitsiklis, John N.",
title="Asynchronous stochastic approximation and Q-learning",
journal="Machine Learning",
year="1994",
month="Sep",
day="01",
volume="16",
number="3",
pages="185--202",
abstract="We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.",
issn="1573-0565",
doi="10.1007/BF00993306",
url="https://doi.org/10.1007/BF00993306"
}



  @inproceedings{devraj2017zap,
 author = {Devraj, Adithya M. and Meyn, Sean P.},
 title = {Zap Q-Learning},
 year = {2017},
 isbn = {9781510860964},
 publisher = {Curran Associates Inc.},
 address = {Red Hook, NY, USA},
 booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
 pages = {2232–2241},
 numpages = {10},
 location = {Long Beach, California, USA},
 series = {NIPS’17}
}


@inproceedings{maei09nonlineargtd,
 author = {Maei, Hamid R. and Szepesv\'{a}ri, Csaba and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S.},
 title = {Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation},
 year = {2009},
 booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems}
}


@article{Hasselt2018DeepRL,
  title={Deep Reinforcement Learning and the Deadly Triad},
  author={Hado van Hasselt and Yotam Doron and Florian Strub and Matteo Hessel and Nicolas Sonnerat and Joseph Modayil},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.02648}
}

@inproceedings{Kolter2011TheFP,
  title={The Fixed Points of Off-Policy TD},
  author={J. Zico Kolter},
  booktitle={NIPS},
  year={2011}
}

@inproceedings{du2019distributioncheck,
author = {Du, Simon and Luo, Yuping and Wang, Ruosong and Zhang, Hanrui},
year = {2019},
month = {06},
booktitle={NeurIPS},
title = {Provably Efficient $Q$-learning with Function Approximation via Distribution Shift Error Checking Oracle}
}

@article{sutton16emphatic,
author = {Sutton, Richard S. and Mahmood, A. Rupam and White, Martha},
title = {An Emphatic Approach to the Problem of Off-Policy Temporal-Difference Learning},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2603–2631},
numpages = {29},
keywords = {function approximation, temporal-difference learning, off-policy learning, convergence, stability}
}

@InProceedings{fu19diagnosing,
  title = 	 {Diagnosing Bottlenecks in Deep Q-learning Algorithms},
  author = 	 {Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey},
  year = {2019},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  publisher = 	 {PMLR},
}


@phdthesis{konda_ac,
author = {Konda, Vijaymohan and Tsitsiklis, John N.},
title = {Actor-Critic Algorithms},
year = {2002},
publisher = {Massachusetts Institute of Technology},
address = {USA},
note = {AAI0804543}
}


@article{schaul2019ray,
  author    = {Tom Schaul and
               Diana Borsa and
               Joseph Modayil and
               Razvan Pascanu},
  title     = {Ray Interference: a Source of Plateaus in Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1904.11455},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.11455},
  archivePrefix = {arXiv},
  eprint    = {1904.11455},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-11455},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout


@inproceedings{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2010}
}

@article{tsitsiklis1994asynchronous,
  title={Asynchronous stochastic approximation and Q-learning},
  author={Tsitsiklis, John N},
  journal={Machine learning},
  volume={16},
  number={3},
  pages={185--202},
  year={1994},
  publisher={Springer}
}

@inproceedings{
yaz2018the,
title={The Unusual Effectiveness of Averaging in {GAN} Training},
author={Yasin Yaz{\i}c{\i} and Chuan-Sheng Foo and Stefan Winkler and Kim-Hui Yap and Georgios Piliouras and Vijay Chandrasekhar},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJgw_sRqFQ},
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2018}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@inproceedings{Kakade2002,
author = {Kakade, Sham and Langford, John},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Approximately Optimal Approximate Reinforcement Learning}},
year = {2002}
}

@inproceedings{Maei2010,
author = {Maei, Hamid Reza and Szepesv{\'{a}}ri, Csaba and Bhatnagar, Shalabh and Sutton, Richard S},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Toward off-policy learning control with function approximation}},
year = {2010}
}

@inproceedings{Baird1995,
annote = {Residual gradient = differentiating through target},
author = {Baird, Leemon},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Residual Algorithms : Reinforcement Learning with Function Approximation}},
year = {1995}
}

@article{Mnih2015,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}

@article{Lillicrap2015,
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Continuous control with deep reinforcement learning}},
year = {2015}
}


@inproceedings{Precup2001,
author = {Precup, Doina and Sutton, Richard S. and Dasgupta, Sanjoy},
booktitle = {International Conference on Machine Learning (ICML)},
title = {{Off-policy temporal-difference learning with function approximation}},
year = {2001}
}

@article{Schulman2017,
  author    = {John Schulman and
               Pieter Abbeel and
               Xi Chen},
  title     = {Equivalence Between Policy Gradients and Soft Q-Learning},
  journal   = {ArXiv Preprint},
  volume    = {abs/1704.06440},
  year      = {2017},
}

@article{Gu2016,
  author    = {Shixiang Gu and
               Ethan Holly and
               Timothy Lillicrap and
               Sergey Levine},
  title     = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  journal   = {IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2017},
}

@article{Watkins92,
  author    = {Christopher J.C.H. Watkins and Peter Dayan},
  title     = {Q-learning},
  journal   = {Machine Learning},
  year      = {1992},
  volume    = {8},
  pages     = {279-292},
  issue     = {3},
}

@incollection{Kakade2001,
title = {A Natural Policy Gradient},
author = {Sham M. Kakade},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2001},
}

@incollection{Munos2016,
title = {Safe and Efficient Off-policy Reinforcement Learning},
author = {Remi Munos and Thomas Stepleton and Anna Harutyunyan and Marc G. Bellemare},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2016},
}

@inproceedings{Sutton09a,
title = {A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation},
author = {Sutton, Richard S. and Hamid Reza Maei and Csaba Szepesv\'{a}ri},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2009},
}

@inproceedings{Sutton09b,
title = {Fast Gradient-Descent Methods for Temporal-Difference Learning
with Linear Function Approximation},
author = {
Richard S. Sutton and Hamid Reza Maei and Doina Precup and Shalabh Bhatnagar and David Silver and Csaba Szepesv\'{a}ri and
Eric Wiewiora},
booktitle = {International Conference on Machine Learning (ICML)},
year = {2009},
}

@article{Bertsekas96,
title = {Neuro-Dynamic Programming},
author = {Bertsekas, Dimitri P., and Tsitsiklis, John N.},
journal = {Athena Scientific},
year = {1996},
}


@inproceedings{Haarnoja2017,
author = {Tuomas Haarnoja and
               Haoran Tang and
               Pieter Abbeel and
               Sergey Levine},
booktitle = {International Conference on Machine Learning (ICML)},
title = {Reinforcement Learning with Deep Energy-Based Policies},
year = {2017}
}

@inproceedings{Hausknecht2016,
author = {Matthew Hausknecht and Peter Stone},
booktitle = {Deep Reinforcement Learning: Frontiers and Challenges Workshop, IJCAI},
title = {On-Policy vs. Off-Policy Updates for Deep Reinforcement Learning},
year = {2016}
}

@inproceedings{Farrell95,
author = {Jay A. Farrell and T. Berger},
booktitle = {American Control Conference},
title = {On the effects of the training sample density in passive learning control},
year = {1995}
}

@inproceedings{Boyan94,
author = {Justin A. Boyan and Andrew W. Moore},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
title = {Generalization in Reinforcement Learning:
Safely Approximating the Value Function},
year = {1994}
}

@article{Haarnoja18,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  archivePrefix = {arXiv},
  eprint    = {1801.01290},
}

@inproceedings{kalashnikov18,
  author    = {Dmitry Kalashnikov and
               Alex Irpan and
               Peter Pastor and
               Julian Ibarz and
               Alexander Herzog and
               Eric Jang and
               Deirdre Quillen and
               Ethan Holly and
               Mrinal Kalakrishnan and
               Vincent Vanhoucke and
               Sergey Levine},
  title     = {QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  booktitle = {CoRL},
  year      = {2018}
}

@article{Ernst05,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Apr},
  pages={503--556},
  year={2005}
}

@book{suttonrlbook,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  edition = {Second}
}

@inproceedings{Dai2018,
  title={Sbeed: Convergent reinforcement learning with nonlinear function approximation},
  author={Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
  booktitle={International Conference on Machine Learning},
  pages={1133--1142},
  year={2018}
}

@article{VanHesselt2018,
  title={Deep Reinforcement Learning and the Deadly Triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv preprint arXiv:1812.02648},
  year={2018}
}

@inproceedings{Tsitsiklis1997,
  title={Analysis of temporal-diffference learning with function approximation},
  author={Tsitsiklis, John N and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={1075--1081},
  year={1997}
}

@article{Hallak2017,
  title={Consistent on-line off-policy evaluation},
  author={Hallak, Assaf and Mannor, Shie},
  journal={arXiv preprint arXiv:1702.07121},
  year={2017}
}

@article{Sutton2016,
  title={An emphatic approach to the problem of off-policy temporal-difference learning},
  author={Sutton, Richard S and Mahmood, A Rupam and White, Martha},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2603--2631},
  year={2016},
  publisher={JMLR. org}
}

@article{henderson2017deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  journal={arXiv preprint arXiv:1709.06560},
  year={2017}
}

@article{dalal2017finite,
  title={Finite sample analyses for TD (0) with function approximation},
  author={Dalal and Gal , Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Thoppe, Gugan and Mannorand Shie},
  journal={arXiv preprint arXiv:1704.01161},
  year={2017}
}

@inproceedings{bhatnagar2009convergent,
  title={Convergent temporal-difference learning with arbitrary smooth function approximation},
  author={Maei, Hamid R, and Szepesv{\'a}ri, Csaba, and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S },
  booktitle={Advances in Neural Information Processing Systems},
  pages={1204--1212},
  year={2009}
}

@inproceedings{Riedmiller2005,
  title={Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method},
  author={Riedmiller, Martin},
  booktitle={European Conference on Machine Learning},
  pages={317--328},
  year={2005},
  organization={Springer}
}

@article{Watkins1992,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{Gu2017,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={Robotics and Automation (ICRA), 2017 IEEE International Conference on},
  pages={3389--3396},
  year={2017},
  organization={IEEE}
}

@article{fedus2020revisiting,
  title={Revisiting fundamentals of experience replay},
  author={Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  journal={arXiv preprint arXiv:2007.06700},
  year={2020}
}

@misc{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@misc{approxstack,
  Author = {Robert Johnson},
  Title = {Approximate Irrational Numbers by Rational Numbers},
  Year = {2016},
  url = {https://math.stackexchange.com/questions/1829743/},
}

@article{Schaul2015,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={International Conference on Learning Representations (ICLR)},
  year={2015}
}

@book{Shalev2014,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@incollection{NIPS2017_6913,
title = {Is the Bellman residual a bad proxy?},
author = {Geist, Matthieu and Piot, Bilal and Pietquin, Olivier},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
pages = {3205--3214},
year = {2017},
}

@inproceedings{Pritzel2017,
  title={Neural Episodic Control},
  author={Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Badia, Adri{\`a} Puigdom{\`e}nech and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2827--2836},
  year={2017}
}

@InProceedings{pmlr-v80-fujimoto18a,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author = 	 {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {1587--1596},
  year = 	 {2018},
}

@inproceedings{Gu2017ipg,
  title={Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning},
  author={Gu, Shixiang Shane and Lillicrap, Timothy and Turner, Richard E and Ghahramani, Zoubin and Sch{\"o}lkopf, Bernhard and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={3846--3855},
  year={2017}
}

@inproceedings{maillard2010finite,
  title={Finite-sample analysis of Bellman residual minimization},
  author={Maillard, Odalric-Ambrym and Munos, R{\'e}mi and Lazaric, Alessandro and Ghavamzadeh, Mohammad},
  booktitle={Asian Conference on Machine Learning (ACML)},
  pages={299--314},
  year={2010}
}

@inproceedings{munos2005error,
  title={Error bounds for approximate value iteration},
  author={Munos, R{\'e}mi},
  booktitle={AAAI Conference on Artificial intelligence (AAAI)},
  pages={1006--1011},
  year={2005},
  organization={AAAI Press}
}

@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={May},
  pages={815--857},
  year={2008}
}

@article{zhang2018deeper,
  author    = {Shangtong Zhang and
               Richard S. Sutton},
  title     = {A Deeper Look at Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1712.01275},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01275},
  archivePrefix = {arXiv},
  eprint    = {1712.01275},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01275},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{antos2007fitted,
 author = {Antos, Andr\'{a}s and Munos, R{\'e}mi and Szepesv\'{a}ri, Csaba},
 title = {Fitted Q-iteration in Continuous Action-space MDPs},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 series = {NIPS'07},
 year = {2007},
 location = {Vancouver, British Columbia, Canada},
} 

@phdthesis{de2002alp,
  title={The linear programming approach to approximate dynamic programming: Theory and application},
  author={De Farias, Daniela Pucci},
  year={2002}
}

@article{antos2008concentrability,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@article{metelli2018nips,
  author    = {Alberto Maria Metelli and
               Matteo Papini and
               Francesco Faccio and
               Marcello Restelli},
  title     = {Policy Optimization via Importance Sampling},
  journal   = {CoRR},
  volume    = {abs/1809.06098},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.06098},
  archivePrefix = {arXiv},
  eprint    = {1809.06098},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1809-06098},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author = 	 {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {214--223},
  year = 	 {2017},
}

@techreport{haarnoja2018sacapps,
  title={Soft Actor-Critic Algorithms and Applications},
  author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{hazan2018,
  title={Provably Efficient Maximum Entropy Exploration},
  author={Hazan, Elad and Kakade, Sham M and Singh, Karan and Van Soest, Abby},
  journal={arXiv preprint arXiv:1812.02690},
  year={2018}
}

@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@inproceedings{scherrer2010residual,
  title={Should one compute the temporal difference fix point or minimize the Bellman Residual? The unified oblique projection view},
  author={Scherrer, Bruno},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={959--966},
  year={2010},
}

@inproceedings{tosatto2017boosted,
  title={Boosted fitted q-iteration},
  author={Tosatto, Samuele and Pirotta, Matteo and D'Eramo, Carlo and Restelli, Marcello},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={3434--3443},
  year={2017},
  organization={JMLR. org}
}

@article{lin1992replay,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@article{yarats2022don,
  title={Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning},
  author={Yarats, Denis and Brandfonbrener, David and Liu, Hao and Laskin, Michael and Abbeel, Pieter and Lazaric, Alessandro and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2201.13425},
  year={2022}
}

@inproceedings{munos2016safe,
  title={Safe and efficient off-policy reinforcement learning},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={1054--1062},
  year={2016}
}

@article{sriperum2009ipms,
  author    = {Bharath K. Sriperumbudur and
               Arthur Gretton and
               Kenji Fukumizu and
               Gert R. G. Lanckriet and
               Bernhard Sch{\"{o}}lkopf},
  title     = {A note on integral probability metrics and {\textdollar}{\textbackslash}phi{\textdollar}-divergences},
  journal   = {CoRR},
  volume    = {abs/0901.2698},
  year      = {2009},
  url       = {http://arxiv.org/abs/0901.2698},
  archivePrefix = {arXiv},
  eprint    = {0901.2698},
  timestamp = {Mon, 13 Aug 2018 16:48:04 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-0901-2698},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kumar2020discor,
  title={Discor: Corrective feedback in reinforcement learning via distribution correction},
  author={Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2003.07305},
  year={2020}
}

@inproceedings{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7413--7424},
  year={2019}
}

@inproceedings{yang2020theoretical,
  title={A theoretical analysis of deep Q-learning},
  author={Yang, Zhuoran and Xie, Yuchen and Wang, Zhaoran},
  booktitle={Learning for Dynamics and Control},
  pages={486--489},
  year={2020},
  organization={PMLR}
}

@article{cai2019neural,
  title={Neural Temporal-Difference and Q-Learning Provably Converge to Global Optima},
  author={Cai, Qi and Yang, Zhuoran and Lee, Jason D and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1905.10027},
  year={2019}
}

@article{mahadevan2007proto,
  title={Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes},
  author={Mahadevan, Sridhar and Maggioni, Mauro},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={Oct},
  pages={2169--2231},
  year={2007}
}

@article{wu2018laplacian,
  title={The Laplacian in RL: Learning representations with efficient approximations},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1810.04586},
  year={2018}
}

@article{machado2017eigenoption,
  title={Eigenoption discovery through the deep successor representation},
  author={Machado, Marlos C and Rosenbaum, Clemens and Guo, Xiaoxiao and Liu, Miao and Tesauro, Gerald and Campbell, Murray},
  journal={arXiv preprint arXiv:1710.11089},
  year={2017}
}

@article{van2019use,
  title={When to use parametric models in reinforcement learning?},
  author={van Hasselt, Hado and Hessel, Matteo and Aslanides, John},
  journal={arXiv preprint arXiv:1906.05243},
  year={2019}
}

@article{tian2021understanding,
  title={Understanding self-supervised learning dynamics without contrastive pairs},
  author={Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
  journal={arXiv preprint arXiv:2102.06810},
  year={2021}
}

@article{tian2020understanding,
  title={Understanding self-supervised learning with dual deep networks},
  author={Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  journal={arXiv preprint arXiv:2010.00578},
  year={2020}
}

@article{chen2020exploring,
  title={Exploring Simple Siamese Representation Learning},
  author={Chen, Xinlei and He, Kaiming},
  journal={arXiv preprint arXiv:2011.10566},
  year={2020}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent: A new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and others},
  journal={arXiv preprint arXiv:2006.07733},
  year={2020}
}

@article{fakoor2021continuous,
  title={Continuous Doubly Constrained Batch Reinforcement Learning},
  author={Fakoor, Rasool and Mueller, Jonas and Chaudhari, Pratik and Smola, Alexander J},
  journal={arXiv preprint arXiv:2102.09225},
  year={2021}
}

@article{kaiser2019model,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  year={2019}
}

@article{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:2002.09277},
  year={2020}
}

@inproceedings{lyle2021effect,
  title={On The Effect of Auxiliary Tasks on Representation Dynamics},
  author={Lyle, Clare and Rowland, Mark and Ostrovski, Georg and Dabney, Will},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1--9},
  year={2021},
  organization={PMLR}
}

@article{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason and Liu, Qiang and Ma, Tengyu},
  year={2019}
}

@article{sedghi2018singular,
  title={The singular values of convolutional layers},
  author={Sedghi, Hanie and Gupta, Vineet and Long, Philip M},
  journal={arXiv preprint arXiv:1805.10408},
  year={2018}
}

@article{zhang2020can,
  title={Can Temporal-Difference and Q-Learning Learn Representation? A Mean-Field Theory},
  author={Zhang, Yufeng and Cai, Qi and Yang, Zhuoran and Chen, Yongxin and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2006.04761},
  year={2020}
}

@article{xu2019finite,
  title={A finite-time analysis of Q-learning with neural network function approximation},
  author={Xu, Pan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1912.04511},
  year={2019}
}

@article{voloshin2019empirical,
  title={Empirical study of off-policy policy evaluation for reinforcement learning},
  author={Voloshin, Cameron and Le, Hoang M and Jiang, Nan and Yue, Yisong},
  journal={arXiv preprint arXiv:1911.06854},
  year={2019}
}

@article{ghosh2020representations,
  title={Representations for Stable Off-Policy Reinforcement Learning},
  author={Ghosh, Dibya and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2007.05520},
  year={2020}
}

@article{mobahi2020self,
  title={Self-distillation amplifies regularization in hilbert space},
  author={Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2002.05715},
  year={2020}
}

@inproceedings{agarwal2019optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2020}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{kumar2020conservative,
  title={Conservative Q-Learning for Offline Reinforcement Learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@article{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  journal={arXiv preprint arXiv:1802.06509},
  year={2018}
}

@inproceedings{boyan1999least,
  title={Least-squares temporal difference learning},
  author={Boyan, Justin A},
  booktitle={ICML},
  pages={49--56},
  year={1999},
  organization={Citeseer}
}

@inproceedings{melo2008analysis,
  title={An analysis of reinforcement learning with function approximation},
  author={Melo, Francisco S and Meyn, Sean P and Ribeiro, M Isabel},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={664--671},
  year={2008}
}

@inproceedings{devraj2017zap,
  title={Zap Q-learning},
  author={Devraj, Adithya M and Meyn, Sean},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2235--2244},
  year={2017}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@article{bengio2020interference,
  title={Interference and Generalization in Temporal Difference Learning},
  author={Bengio, Emmanuel and Pineau, Joelle and Precup, Doina},
  journal={arXiv preprint arXiv:2003.06350},
  year={2020}
}

@inproceedings{precup2001offpol,
  title={Off-Policy Temporal Difference Learning with Function Approximation},
  author={Precup, Doina and Sutton, Richard S and Dasgupta, Sanjoy},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={417--424},
  year={2001},
}

@article{dabney2018implicit,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1806.06923},
  year={2018}
}

@inproceedings{hausknecht2016policy,
  title={On-policy vs. off-policy updates for deep reinforcement learning},
  author={Hausknecht, Matthew and Stone, Peter},
  booktitle={Deep Reinforcement Learning: Frontiers and Challenges, IJCAI},
  year={2016}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{martha2018sparse,
  author    = {Vincent Liu and
               Raksha Kumaraswamy and
               Lei Le and
               Martha White},
  title     = {The Utility of Sparse Representations for Control in Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/1811.06626},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.06626},
  archivePrefix = {arXiv},
  eprint    = {1811.06626},
  timestamp = {Sun, 25 Nov 2018 18:57:12 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-06626},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{jaques2019way,
  title={Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
  author={Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  journal={arXiv preprint arXiv:1907.00456},
  year={2019}
}

@article{shani2005recommender,
  title={An MDP-based recommender system},
  author={Shani, Guy and Heckerman, David and Brafman, Ronen I},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Sep},
  pages={1265--1295},
  year={2005}
}

@inproceedings{szepesvari1998asymptotic,
  title={The asymptotic convergence-rate of Q-learning},
  author={Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1064--1070},
  year={1998}
}

@inproceedings{ijcai2020-370,
  title     = {I4R: Promoting Deep Reinforcement Learning by the Indicator for Expressive Representations},
  author    = {Luo, Xufang and Meng, Qi and He, Di and Chen, Wei and Wang, Yunhong},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {2669--2675},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/370},
  url       = {https://doi.org/10.24963/ijcai.2020/370},
}

@article{yoshida2017spectral,
  title={Spectral norm regularization for improving the generalizability of deep learning},
  author={Yoshida, Yuichi and Miyato, Takeru},
  journal={arXiv preprint arXiv:1705.10941},
  year={2017}
}

@inproceedings{Parr2008AnAO,
  title={An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning},
  author={Ronald Parr and Lihong Li and Gavin Taylor and Christopher Painter-Wakefield and Michael L. Littman},
  booktitle={ICML '08},
  year={2008}
}


@book{koller_friedman,
 author = {Koller, D. and Friedman, N.},
 title = {Probabilistic Graphical Models: Principles and Techniques},
 year = {2009},
 isbn = {0262013193, 9780262013192},
 publisher = {The MIT Press},
} 

@inproceedings{chebotar2019closing,
  title={Closing the sim-to-real loop: Adapting simulation randomization with real world experience},
  author={Chebotar, Yevgen and Handa, Ankur and Makoviychuk, Viktor and Macklin, Miles and Issac, Jan and Ratliff, Nathan and Fox, Dieter},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8973--8979},
  year={2019},
  organization={IEEE}
}

@article{tan2018sim,
  title={Sim-to-real: Learning agile locomotion for quadruped robots},
  author={Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
  journal={arXiv preprint arXiv:1804.10332},
  year={2018}
}

@inproceedings{sadeghi2016cad2rl,
  title={{CAD2RL}: Real single-image flight without a single real image},
  author={Sadeghi, Fereshteh and Levine, Sergey},
  booktitle={Robotics: Science and Systems},
  year={2017}
}

@inproceedings{gae,
title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
author = {J. Schulman and P. Moritz and S. Levine and M. Jordan and P. Abbeel},
booktitle = {International Conference on Learning Representations (ICLR)},
year  = 2016
}

@misc{kumar_blog,
title = {Data-Driven Deep Reinforcement Learning},
author = {Aviral Kumar},
note = {{BAIR} Blog},
year = {2019},
howpublished  = {\url{https://bair.berkeley.edu/blog/2019/12/05/bear/}}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{levine2013guided,
  title={Guided policy search},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle={International Conference on Machine Learning},
  pages={1--9},
  year={2013}
}

@article{luo2019learning,
  title={Learning self-correctable policies and value functions from demonstrations with negative sampling},
  author={Luo, Yuping and Xu, Huazhe and Ma, Tengyu},
  journal={arXiv preprint arXiv:1907.05634},
  year={2019}
}

@inproceedings{mulayoff2020unique,
  title={Unique Properties of Flat Minima in Deep Networks},
  author={Mulayoff, Rotem and Michaeli, Tomer},
  booktitle={International Conference on Machine Learning},
  pages={7108--7118},
  year={2020},
  organization={PMLR}
}

@article{van2018deep,
  title={Deep reinforcement learning and the deadly triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv preprint arXiv:1812.02648},
  year={2018}
}

@article{damian2021label,
  title={Label Noise SGD Provably Prefers Flat Global Minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason},
  journal={arXiv preprint arXiv:2106.06530},
  year={2021}
}

@article{mahmood2015emphatic,
  title={Emphatic temporal-difference learning},
  author={Mahmood, A Rupam and Yu, Huizhen and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1507.01569},
  year={2015}
}

@article{chatterji2019intriguing,
  title={The intriguing role of module criticality in the generalization of deep networks},
  author={Chatterji, Niladri S and Neyshabur, Behnam and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1912.00528},
  year={2019}
}

@article{bjorck2021towards,
  title={Towards Deeper Deep Reinforcement Learning},
  author={Bjorck, Johan and Gomes, Carla P and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:2106.01151},
  year={2021}
}

@article{ota2021training,
  title={Training Larger Networks for Deep Reinforcement Learning},
  author={Ota, Kei and Jha, Devesh K and Kanezaki, Asako},
  journal={arXiv preprint arXiv:2102.07920},
  year={2021}
}

@article{yang2020feature,
  title={Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@article{sinha2020d2rl,
  title={D2rl: Deep dense architectures in reinforcement learning},
  author={Sinha, Samarth and Bharadhwaj, Homanga and Srinivas, Aravind and Garg, Animesh},
  journal={arXiv preprint arXiv:2010.09163},
  year={2020}
}

@inproceedings{kakade2002natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  booktitle={Advances in neural information processing systems},
  pages={1531--1538},
  year={2002}
}


@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@article{ernst2005tree,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Apr},
  pages={503--556},
  year={2005}
}

@InProceedings{thomas,
  title = 	 {Bias in Natural Actor-Critic Algorithms},
  author = 	 {Philip Thomas},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year = 	 {2014},
  month = 	 {22--24 Jun},
}

@inproceedings{toussaint06,
 author = {Toussaint, M. and Storkey, A.},
 title = {Probabilistic Inference for Solving Discrete and Continuous State Markov Decision Processes},
 booktitle = {International Conference on Machine Learning (ICML)},
 year = {2006},
} 

@INPROCEEDINGS{attias,
    author = {H. Attias},
    title = {Planning by Probabilistic Inference},
    booktitle = {Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics},
    year = {2003}
}

@article{hafner2011reinforcement,
  title={Reinforcement learning in feedback control},
  author={Hafner, Roland and Riedmiller, Martin},
  journal={Machine learning},
  volume={84},
  number={1-2},
  pages={137--169},
  year={2011},
  publisher={Springer}
}

@article{tesauro1994td,
  title={{TD-Gammon}, a self-teaching backgammon program, achieves master-level play},
  author={Tesauro, Gerald},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={215--219},
  year={1994},
  publisher={MIT Press}
}

@InProceedings{ebrl,
  title = 	 {Actor-Critic Reinforcement Learning with Energy-Based Policies},
  author = 	 {N. Heess and D. Silver and Y. W. Teh},
  booktitle = 	 {European Workshop on Reinforcement Learning (EWRL)},
  year = 	 {2013},
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={627--635},
  year={2011}
}

@inproceedings{ross2010efficient,
  title={Efficient reductions for imitation learning},
  author={Ross, St{\'e}phane and Bagnell, Drew},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={661--668},
  year={2010}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={International Conference on Machine Learning (ICML)},
  volume={2},
  year={2002}
}

@inproceedings{ep,
 author = {Minka, T. P.},
 title = {Expectation Propagation for Approximate Bayesian Inference},
 booktitle = {Uncertainty in Artificial Intelligence (UAI)},
 year = {2001},
}

@inproceedings{mpo,
title = {Maximum a Posteriori Policy Optimisation},
author = {A. Abdolmaleki and J. T. Springenberg and Y. Tassa and R. Munos and N. Heess and M. Riedmiller},
booktitle = {International Conference on Learning Representations (ICLR)},
year  = 2018
}

@article{williams,
 author = {Williams, R. J.},
 title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
 journal = {Machine Learning},
 issue_date = {May 1992},
 volume = {8},
 number = {3-4},
 month = may,
 year = {1992},
 pages = {229--256}
} 

@article{WilliamsPeng91,
  author = {Williams, R. J. and Peng, J.},
  journal = {Connection Science},
  number = 3,
  pages = {241-268},
  title = {Function optimization using connectionist reinforcement learning
	algorithms},
  volume = 3,
  year = 1991
}

@book{sb-irl-98,
 author = {Sutton, R. S. and Barto, A. G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{suttonadp,
    author = {R. S. Sutton},
    title = {Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {1990},
}

@inproceedings{pgq,
author = {O'Donoghue, B. and Munos, R. and Kavukcuoglu, K. and Mnih, V.},
year = {2017},
title = {PGQ: Combining policy gradient and Q-learning},
booktitle = {International Conference on Learning Representations (ICLR)}
}

@article{sallans,
 author = {Sallans, B. and Hinton, G. E.},
 title = {Reinforcement Learning with Factored States and Actions},
 journal = {Journal of Machine Learning Research},
 volume = {5},
 month = dec,
 year = {2004}
 },

@article{KLMSurvey,
    author = "L. P. Kaelbling and M. L. Littman and A. P. Moore",
    title = "Reinforcement Learning: A Survey",
    journal = "Journal of Artificial Intelligence Research",
    volume = "4",
    pages = "237-285",
    year = "1996",
url={http://people.csail.mit.edu/lpk/papers/rl-survey.ps}}

@inproceedings{todorov_kalman_duality, 
author={E. Todorov}, 
booktitle={Conference on Decision and Control (CDC)},
title={General duality between optimal control and estimation}, 
year={2008},
}

@inproceedings{covariant,
author = {J. A. Bagnell and J. Schneider},
title = {Covariant Policy Search},
booktitle = {International Joint Conference on Artifical Intelligence (IJCAI)},
year = {2003},
}

@InProceedings{reps,
  author =       "Peters, J. and  M{\"u}lling, K. and Alt{\"u}n, Y.",
  title =        "Relative Entropy Policy Search",
  booktitle =    "AAAI Conference on Artificial Intelligence (AAAI)",
  year =         "2010",
}

@inproceedings{mfgps,
title = {Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics},
author = {Levine, S. and Abbeel, P.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2014},
}

@inproceedings{lmdp_policy,
  author    = {E. Todorov},
  title     = {Policy gradients in linearly-solvable MDPs},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year      = {2010},
}

@inproceedings{ldmp_irl,
 author = {Dvijotham, K. and Todorov, E.},
 title = {Inverse Optimal Control with Linearly-solvable MDPs},
 booktitle = {International Conference on International Conference on Machine Learning (ICML)},
 year = {2010},
} 

@inproceedings{bear,
title = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
author = {Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
booktitle = {Neural Information Processing Systems (NeurIPS)},
year = {2019}
}

@article{lerer2016learning,
  title={Learning physical intuition of block towers by example},
  author={Lerer, Adam and Gross, Sam and Fergus, Rob},
  journal={arXiv preprint arXiv:1603.01312},
  year={2016}
}

@article{ebert2018visual,
  title={Visual foresight: Model-based deep reinforcement learning for vision-based robotic control},
  author={Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.00568},
  year={2018}
}

@inproceedings{finn2017deep,
  title={Deep visual foresight for planning robot motion},
  author={Finn, Chelsea and Levine, Sergey},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2786--2793},
  year={2017},
  organization={IEEE}
}

@inproceedings{battaglia2016interaction,
  title={Interaction networks for learning about objects, relations and physics},
  author={Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo Jimenez and others},
  booktitle={Advances in neural information processing systems},
  pages={4502--4510},
  year={2016}
}

@inproceedings{sagawa2019distributionally,
  title={Distributionally Robust Neural Networks},
  author={Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B and Liang, Percy},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@article{sinha2017certifying,
  title={Certifying some distributional robustness with principled adversarial training},
  author={Sinha, Aman and Namkoong, Hongseok and Duchi, John},
  journal={arXiv preprint arXiv:1710.10571},
  year={2017}
}

@inproceedings{kingma2014semi,
  title={Semi-supervised learning with deep generative models},
  author={Kingma, Durk P and Mohamed, Shakir and Rezende, Danilo Jimenez and Welling, Max},
  booktitle={Advances in neural information processing systems},
  pages={3581--3589},
  year={2014}
}

@inproceedings{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  booktitle={Advances in neural information processing systems},
  pages={5574--5584},
  year={2017}
}

@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}

@article{scholkopf2019causality,
  title={Causality for Machine Learning},
  author={Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:1911.10500},
  year={2019}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{pi2,
  title = {Learning Policy Improvements with Path Integrals},
  author = {Theodorou, E. A. and Buchli, J. and Schaal, S.},
  booktitle = {International Conference on  Artificial Intelligence and Statistics (AISTATS 2010)},
  year = {2010},
}

@InProceedings{trpo,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {J. Schulman and S. Levine and P. Moritz and M. I. Jordan and P. Abbeel},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year = 	 {2015},
}

@phdthesis{levine_thesis,
author = {Levine, S.},
title = {Motor skill learning with local trajectory methods},
school = {Stanford University},
year = {2014},
}

@phdthesis{ziebart_thesis,
author = {Ziebart, B.},
title = {Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
school = {Carnegie Mellon University},
year = {2010},
}

@article{kappen_oc,
  author    = {H. J. Kappen},
  title     = {Optimal control theory and the linear bellman equation},
  journal   = {Inference and Learning in Dynamic Models},
  year      = {2011},
  pages     = {363-387},
}

@inproceedings{heess2015learning,
  title={Learning continuous control policies by stochastic value gradients},
  author={Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2944--2952},
  year={2015}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1352--1361},
  year={2017},
  organization={JMLR. org}
}

@article{kappen_pgm,
  author    = {H. J. Kappen and
               V. G{\'o}mez and
               M. Opper},
  title     = {Optimal control as a graphical model inference problem},
  journal   = {Machine Learning},
  volume    = {87},
  number    = {2},
  year      = {2012},
  pages     = {159-182},
}

@inproceedings{rawlik_soc,
  author    = {K. Rawlik and
               M. Toussaint and
               S. Vijayakumar},
  title     = {On Stochastic Optimal Control and Reinforcement Learning
               by Approximate Inference},
  year = {2013},
  booktitle = {Robotics: Science and Systems (RSS)},
}

@inproceedings{toussaint_soc,
  author    = {M. Toussaint},
  title     = {Robot trajectory optimization using approximate inference},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2009},
}

@inproceedings{toussaint_pgm,
  title={Hierarchical POMDP Controller Optimization by Likelihood Maximization},
  author={Toussaint, M. and Charlin, L. and Poupart, P.},
  booktitle={Uncertainty in Artificial Intelligence (UAI)},
  volume={24},
  pages={562--570},
  year={2008}
}

@article{kalman_filter,
  author={R. Kalman},
  title={A new approach to linear filtering and prediction problems},
  journal={ASME Transactions journal of basic engineering},
  volume={82},
  number={1},
  pages={35-45},
  year={1960}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000}
}

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{todorov_lmdp,
  author    = {E. Todorov},
  title     = {Linearly-solvable Markov decision problems},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2006},
}

@inproceedings{rwr,
 author = {Peters, J. and Schaal, S.},
 title = {Reinforcement Learning by Reward-weighted Regression for Operational Space Control},
 booktitle = {International Conference on Machine Learning (ICML)},
 year = {2007},
} 


@inproceedings{neumann,
  author    = {G. Neumann},
  title     = {Variational Inference for Policy Search in changing situations},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2011},
}

@inproceedings{levine_vgps,
  author    = {S. Levine and V. Koltun},
  title     = {Variational policy search via trajectory optimization},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2013},
}

@inproceedings{em_policy_search,
  title = {Efficient Sample Reuse in EM-Based Policy Search},
  author = {Hachiya, H. and Peters, J. and Sugiyama, M.},
  booktitle = {European Conference on Machine Learning (ECML)},
  year = {2009},
}

@article{vmp,
    title={Variational message passing},
    author={Winn, J. and Bishop, C.},
    journal={Journal of Machine Learning Research},
    volume={6},
    year={2005},
    pages={661-694}
}

@inproceedings{haarnoja,
  title={Reinforcement Learning with Deep Energy-Based Policies},
  author={Haarnoja, T. and Tang, H. and Abbeel, P. and Levine, S.},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}

@inproceedings{sac,
title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
author  = {T. Haarnoja and A. Zhou and P. Abbeel and S. Levine},
year  = {2018},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1801.01290.pdf}
}

@inproceedings{d4rl,
title = {D4RL: Datasets for Deep Data-Driven Reinforcement Learning},
author  = {J. Fu and A. Kumar and O. Nachum and G. Tucker and S. Levine},
year  = {2020},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/2004.07219}
}

@inproceedings{pcl,
title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
author  = {O. Nachum and M. Norouzi and K. Xu and D. Schuurmans},
year  = {2017},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1702.08892.pdf}
}

@inproceedings{gps,
 author = {Levine, S. and Koltun, V.},
 title = {Guided Policy Search},
 booktitle = {International Conference on International Conference on Machine Learning (ICML)},
 year = {2013},
} 

@inproceedings{maxentirl,
 author = {Ziebart, B. D. and Maas, A. and Bagnell, J. A. and Dey, A. K.},
 title = {Maximum Entropy Inverse Reinforcement Learning},
 booktitle = {International Conference on Artificial Intelligence (AAAI)},
 year = {2008},
} 

@inproceedings{haarnoja_composable,
author = {Haarnjoa, T. and Pong, V. and Zhou, A. and Dalal, M. and Abbeel, P. and Levine, S.},
title = {Composable Deep Reinforcement Learning for Robotic Manipulation},
booktitle = {International Conference on Robotics and Automation (ICRA)},
year = {2018},
}

@article{trust_pcl,
  author    = {O. Nachum and
               M. Norouzi and
               K. Xu and
               D. Schuurmans},
  title     = {Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1707.01891},
  year      = {2017},
}

@inproceedings{gpirl,
 author = {Levine, S. and Popovi\'{c}, Z. and Koltun, V.},
 title = {Nonlinear Inverse Reinforcement Learning with Gaussian Processes},
 booktitle = {Neural Information Processing Systems (NIPS)},
 year = {2011},
}

@inproceedings{localioc,
    author = {S. Levine and V. Koltun},
    title = {Continuous Inverse Optimal Control with Locally Optimal Examples},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {2012},
}

@inproceedings{legibility,
  author    = {A. D. Dragan and
               K. C. T. Lee and
               S. S. Srinivasa},
  title     = {Legibility and predictability of robot motion},
  booktitle = {International Conference on Human-Robot Interaction (HRI)},
  year      = {2013},
}

@inproceedings{maxentdeepirl,
author = {M. Wulfmeier and
P. Ondruska and
I. Posner},
title = {Maximum Entropy Deep Inverse Reinforcement Learning},
Booktitle = {Neural Information Processing Systems Conference, Deep Reinforcement Learning Workshop},
year = {2015},
}

@inproceedings{maxcausalent,
  author    = {B. D. Ziebart and
               J. A. Bagnell and
               A. K. Dey},
  title     = {Modeling Interaction via the Principle of Maximum Causal Entropy},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2010},
}

@inproceedings{kitani1,
  author    = {D. Huang and
               K. M. Kitani},
  title     = {Action-Reaction: Forecasting the Dynamics of Human Interaction},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2014},
}

@inproceedings{kitani2,
  author    = {D. Huang and
               A. Farahmand and
               K. M. Kitani and
               J. A. Bagnell},
  title     = {Approximate {MaxEnt} Inverse Optimal Control and Its Application for
               Mental Simulation of Human Interactions},
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
  year      = {2015},
}

@article{realnvp,
title={Latent Space Policies for Hierarchical Reinforcement Learning},
author={T. Haarnoja and K. Hartikainen and P. Abbeel and S. Levine},
journal   = {CoRR},
volume    = {abs/1804.02808},
year      = {2018},
}

@inproceedings{Javdani, 
    AUTHOR    = {S. Javdani and S. Srinivasa and J. A. Bagnell}, 
    TITLE     = {Shared Autonomy via Hindsight Optimization}, 
    BOOKTITLE = {Robotics: Science and Systems (RSS)}, 
    YEAR      = {2015}, 
}

@inproceedings{airl,
title={Learning Robust Rewards with Adversarial Inverse Reinforcement Learning},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
author={Fu, J. and Luo, K. and Levine, S.},
}

@inproceedings{hausman,
title={Learning an Embedding Space for Transferable Robot Skills},
author={K. Hausman and J. T. Springenberg and Z. Wang and N. Heess and M. Riedmiller},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
}

@article{learning_to_explore,
  author    = {A. Gupta and
               R. Mendonca and
               Y. Liu and
               P. Abbeel and
               S. Levine},
  title     = {Meta-Reinforcement Learning of Structured Exploration Strategies},
  journal   = {CoRR},
  volume    = {abs/1802.07245},
  year      = {2018},
}

@article{endtoend,
 author = {Levine, S. and Finn, C. and Darrell, T. and Abbeel, P.},
 title = {End-to-end Training of Deep Visuomotor Policies},
 journal = {Journal of Machine Learning Research},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 month = jan,
 year = {2016},
} 

@inproceedings{cgps,
    author = {Sergey Levine and Vladlen Koltun},
    title = {Learning Complex Neural Network Policies with Trajectory Optimization},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {2014},
}
@inproceedings{schulman,
title = {Equivalence Between Policy Gradients and Soft Q-Learning},
author  = {J. Schulman and X. Chen and P. Abbeel},
year  = {2017},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1704.06440}
}

@inproceedings{nvil,
  author    = {A. Mnih and
               K. Gregor},
  title     = {Neural Variational Inference and Learning in Belief Networks},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2014}
}

@inproceedings{gcl,
  author    = {Chelsea Finn and
               Sergey Levine and
               Pieter Abbeel},
  title     = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},
    booktitle   = {International Conference on Machine Learning (ICML)},
  year      = {2016},
}

@inproceedings{gan,
title = {Generative Adversarial Nets},
author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2014},
}

@inproceedings{gail,
title = {Generative Adversarial Imitation Learning},
author = {Ho, J. and Ermon, S.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2016},
}

@article{finn_adversarial,
  author    = {Chelsea Finn and
               Paul Christiano and
               Pieter Abbeel and
               Sergey Levine},
  title     = {A Connection between Generative Adversarial Networks, Inverse Reinforcement
               Learning, and Energy-Based Models},
  journal   = {CoRR},
  volume    = {abs/1611.03852},
  year      = {2016},
}

@inproceedings{bornschein2016bidirectional,
  title={Bidirectional helmholtz machines},
  author={Bornschein, Jorg and Shabanian, Samira and Fischer, Asja and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={2511--2519},
  year={2016}
}

@article{marino2018iterative,
  title={Iterative amortized inference},
  author={Marino, Joseph and Yue, Yisong and Mandt, Stephan},
  journal={arXiv preprint arXiv:1807.09356},
  year={2018}
}

@inproceedings{rasmus2015semi,
  title={Semi-supervised learning with ladder networks},
  author={Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  booktitle={Advances in neural information processing systems},
  pages={3546--3554},
  year={2015}
}

@inproceedings{sonderby2016ladder,
  title={Ladder variational autoencoders},
  author={S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  booktitle={Advances in neural information processing systems},
  pages={3738--3746},
  year={2016}
}

@inproceedings{zhao2017learning,
  title={Learning hierarchical features from deep generative models},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={4091--4099},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{hsu2017unsupervised,
  title={Unsupervised learning of disentangled and interpretable representations from sequential data},
  author={Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  booktitle={Advances in neural information processing systems},
  pages={1878--1889},
  year={2017}
}


@inproceedings{kalashnikov2018qtopt,
  title={Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  booktitle={Conference on Robot Learning},
  pages={651--673},
  year={2018}
}

@article{cabi2019framework,
  title={A Framework for Data-Driven Robotics},
  author={Cabi, Serkan and Colmenarejo, Sergio G{\'o}mez and Novikov, Alexander and Konyushkova, Ksenia and Reed, Scott and Jeong, Rae and {\.Z}o{\l}na, Konrad and Aytar, Yusuf and Budden, David and Vecerik, Mel and others},
  journal={arXiv preprint arXiv:1909.12200},
  year={2019}
}

 @article{Mo18AdobeIndoorNav,
        Author = {Kaichun Mo and Haoxiang Li and Zhe Lin and Joon-Young Lee},
        Title = {The AdobeIndoorNav Dataset: Towards Deep Reinforcement Learning based Real-world Indoor Robot Visual Navigation},
        Year = {2018},
        Eprint = {arXiv:1802.08824},
    }
    
@article{kandasamy2017batch,
  title={Batch policy gradient methods for improving neural conversation models},
  author={Kandasamy, Kirthevasan and Bachrach, Yoram and Tomioka, Ryota and Tarlow, Daniel and Carter, David},
  journal={arXiv preprint arXiv:1702.03334},
  year={2017}
}

@inproceedings{sun2018fast,
  title={A fast integrated planning and control framework for autonomous driving via imitation learning},
  author={Sun, Liting and Peng, Cheng and Zhan, Wei and Tomizuka, Masayoshi},
  booktitle={Dynamic Systems and Control Conference},
  volume={51913},
  pages={V003T37A012},
  year={2018},
  organization={American Society of Mechanical Engineers}
}

@article{zhou2017end,
  title={End-to-end offline goal-oriented dialog policy learning via policy gradient},
  author={Zhou, Li and Small, Kevin and Rokhlenko, Oleg and Elkan, Charles},
  journal={arXiv preprint arXiv:1712.02838},
  year={2017}
}

@article{pan2017agile,
  title={Agile autonomous driving using end-to-end deep imitation learning},
  author={Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
  journal={arXiv preprint arXiv:1709.07174},
  year={2017}
}

@article{nie2019learning,
  title={Learning when-to-treat policies},
  author={Nie, Xinkun and Brunskill, Emma and Wager, Stefan},
  journal={arXiv preprint arXiv:1905.09751},
  year={2019}
}

@article{bojarski2016end,
  title={End to end learning for self-driving cars},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv preprint arXiv:1604.07316},
  year={2016}
}

@article{yu2018bdd100k,
  title={Bdd100k: A diverse driving video database with scalable annotation tooling},
  author={Yu, Fisher and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Liao, Mike and Madhavan, Vashisht and Darrell, Trevor},
  journal={arXiv preprint arXiv:1805.04687},
  year={2018}
}

@article{sallab2017deep,
  title={Deep reinforcement learning framework for autonomous driving},
  author={Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil},
  journal={Electronic Imaging},
  volume={2017},
  number={19},
  pages={70--76},
  year={2017},
  publisher={Society for Imaging Science and Technology}
}

@article{yurtsever2020survey,
  title={A survey of autonomous driving: Common practices and emerging technologies},
  author={Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
  journal={IEEE Access},
  year={2020},
  publisher={IEEE}
}

@inproceedings{kendall2019learning,
  title={Learning to drive in a day},
  author={Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8248--8254},
  year={2019},
  organization={IEEE}
}

@article{maddern2017robotcar,
  title={1 year, 1000 km: The Oxford RobotCar dataset},
  author={Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
  journal={The International Journal of Robotics Research},
  volume={36},
  number={1},
  pages={3--15},
  year={2017},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{codevilla2018end,
  title={End-to-end driving via conditional imitation learning},
  author={Codevilla, Felipe and Miiller, Matthias and L{\'o}pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1--9},
  year={2018},
  organization={IEEE}
}

@inproceedings{tassa2012synthesis,
  title={Synthesis and stabilization of complex behaviors through online trajectory optimization},
  author={Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={4906--4913},
  year={2012},
  organization={IEEE}
}

@inproceedings{pets,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4754--4765},
  year={2018}
}

@inproceedings{nagabandi2018neural,
  title={Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author={Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7559--7566},
  year={2018},
  organization={IEEE}
}

@inproceedings{gu2017deep,
  title={Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author={Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={3389--3396},
  year={2017},
  organization={IEEE}
}

@article{pietquin2011sample,
  title={Sample-efficient batch reinforcement learning for dialogue management optimization},
  author={Pietquin, Olivier and Geist, Matthieu and Chandramohan, Senthilkumar and Frezza-Buet, Herv{\'e}},
  journal={ACM Transactions on Speech and Language Processing (TSLP)},
  volume={7},
  number={3},
  pages={1--21},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@article{gottesman2019guidelines,
  title={Guidelines for reinforcement learning in healthcare},
  author={Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
  journal={Nat Med},
  volume={25},
  number={1},
  pages={16--18},
  year={2019}
}

@inproceedings{swaminathan2015self,
  title={The self-normalized estimator for counterfactual learning},
  author={Swaminathan, Adith and Joachims, Thorsten},
  booktitle={advances in neural information processing systems},
  pages={3231--3239},
  year={2015}
}

@book{rockafellar1970convex,
  title={Convex analysis},
  author={Rockafellar, R Tyrrell},
  number={28},
  year={1970},
  publisher={Princeton university press}
}

@article{nachum2020reinforcement,
  title={Reinforcement Learning via Fenchel-Rockafellar Duality},
  author={Nachum, Ofir and Dai, Bo},
  journal={arXiv preprint arXiv:2001.01866},
  year={2020}
}

@inproceedings{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2315--2325},
  year={2019}
}

@article{eysenbach2017leave,
  title={Leave no trace: Learning to reset for safe and autonomous reinforcement learning},
  author={Eysenbach, Benjamin and Gu, Shixiang and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1711.06782},
  year={2017}
}

@article{levine2018reinforcement,
  title={Reinforcement learning and control as probabilistic inference: Tutorial and review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}

@inproceedings{osband2018randomized,
  title={Randomized prior functions for deep reinforcement learning},
  author={Osband, Ian and Aslanides, John and Cassirer, Albin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8617--8629},
  year={2018}
}


@inproceedings{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle={Advances in neural information processing systems},
  pages={6402--6413},
  year={2017}
}

@article{achiam2019towards,
  title={Towards characterizing divergence in deep q-learning},
  author={Achiam, Joshua and Knight, Ethan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1903.08894},
  year={2019}
}

@article{gupta2019relay,
  title={Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning},
  author={Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
  journal={arXiv preprint arXiv:1910.11956},
  year={2019}
}

@inproceedings{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4026--4034},
  year={2016}
}

@article{lagoudakis2003least,
  title={Least-squares policy iteration},
  author={Lagoudakis, Michail G and Parr, Ronald},
  journal={Journal of machine learning research},
  volume={4},
  number={Dec},
  pages={1107--1149},
  year={2003}
}

@article{sutton1991dyna,
  title={Dyna, an integrated architecture for learning, planning, and reacting},
  author={Sutton, Richard S},
  journal={ACM Sigart Bulletin},
  volume={2},
  number={4},
  pages={160--163},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@inproceedings{deisenroth2011pilco,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={Proceedings of the 28th International Conference on machine learning (ICML-11)},
  pages={465--472},
  year={2011}
}

@article{johnson2016mimic,
  title={MIMIC-III, a freely accessible critical care database},
  author={Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Li-wei, H Lehman and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark, Roger G},
  journal={Scientific data},
  volume={3},
  pages={160035},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{guez2008adaptive,
  title={Adaptive Treatment of Epilepsy via Batch-mode Reinforcement Learning.},
  author={Guez, Arthur and Vincent, Robert D and Avoli, Massimo and Pineau, Joelle},
  booktitle={AAAI},
  pages={1671--1678},
  year={2008}
}

@article{pipps,
  title={PIPPS: Flexible model-based policy search robust to the curse of chaos},
  author={Parmas, Paavo and Rasmussen, Carl Edward and Peters, Jan and Doya, Kenji},
  journal={arXiv preprint arXiv:1902.01240},
  year={2019}
}

@article{simpl,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  year={2019}
}

@inproceedings{mbpo,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12498--12509},
  year={2019}
}

@article{raghu2017deep,
  title={Deep reinforcement learning for sepsis treatment},
  author={Raghu, Aniruddh and Komorowski, Matthieu and Ahmed, Imran and Celi, Leo and Szolovits, Peter and Ghassemi, Marzyeh},
  journal={arXiv preprint arXiv:1711.09602},
  year={2017}
}

@article{prasad2017reinforcement,
  title={A reinforcement learning approach to weaning of mechanical ventilation in intensive care units},
  author={Prasad, Niranjani and Cheng, Li-Fang and Chivers, Corey and Draugelis, Michael and Engelhardt, Barbara E},
  journal={arXiv preprint arXiv:1704.06300},
  year={2017}
}

@inproceedings{swaminathan2017off,
  title={Off-policy evaluation for slate recommendation},
  author={Swaminathan, Adith and Krishnamurthy, Akshay and Agarwal, Alekh and Dudik, Miro and Langford, John and Jose, Damien and Zitouni, Imed},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3632--3642},
  year={2017}
}

@inproceedings{osband2017posterior,
  title={Why is posterior sampling better than optimism for reinforcement learning?},
  author={Osband, Ian and Van Roy, Benjamin},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2701--2710},
  year={2017},
  organization={JMLR. org}
}

@article{peng2019awr,
  title={Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}

@article{sriperumbudur2009integral,
  title={On integral probability metrics,$\backslash$phi-divergences and binary classification},
  author={Sriperumbudur, Bharath K and Fukumizu, Kenji and Gretton, Arthur and Sch{\"o}lkopf, Bernhard and Lanckriet, Gert RG},
  journal={arXiv preprint arXiv:0901.2698},
  year={2009}
}

@article{fujimoto2018off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  journal={arXiv preprint arXiv:1812.02900},
  year={2018}
}

@inproceedings{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11761--11771},
  year={2019}
}

@article{siegel2020keep,
  title={Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning},
  author={Siegel, Noah Y and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Riedmiller, Martin},
  journal={arXiv preprint arXiv:2002.08396},
  year={2020}
}

@inproceedings{nowozin2016f,
  title={f-gan: Training generative neural samplers using variational divergence minimization},
  author={Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  booktitle={Advances in neural information processing systems},
  pages={271--279},
  year={2016}
}

@article{wu2019domain,
  title={Domain adaptation with asymmetrically-relaxed distribution alignment},
  author={Wu, Yifan and Winston, Ezra and Kaushik, Divyansh and Lipton, Zachary},
  journal={arXiv preprint arXiv:1903.01689},
  year={2019}
}

@inproceedings{sutton2009fastgtd,
  title={Fast gradient-descent methods for temporal-difference learning with linear function approximation},
  author={Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={993--1000},
  year={2009}
}


@article{alphastar,
  title={Mastering the Real-Time Strategy Game StarCraft II},
  author={AlphaStar, DeepMind},
  journal={URL: https://deepmind. com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii}
}

@article{lu2018general,
  title={A General Family of Robust Stochastic Operators for Reinforcement Learning},
  author={Lu, Yingdong and Squillante, Mark S and Wu, Chai Wah},
  journal={arXiv preprint arXiv:1805.08122},
  year={2018}
}

@inproceedings{
Zhang2020GenDICE:,
title={GenDICE: Generalized Offline Estimation of Stationary Values},
author={Ruiyi Zhang and Bo Dai and Lihong Li and Dale Schuurmans},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HkxlcnVFwB}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{bellemare2016increasing,
  title={Increasing the action gap: New operators for reinforcement learning},
  author={Bellemare, Marc G and Ostrovski, Georg and Guez, Arthur and Thomas, Philip S and Munos, R{\'e}mi},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@inproceedings{rajeswaran2018dapg,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  booktitle={Robotics: Science and Systems},
  year={2018}
}

@inproceedings{hester2018deep,
  title={Deep q-learning from demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{vecerik2017leveraging,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017}
}

@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}

@inproceedings{dabney2018distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@misc{
sachdeva2020offpolicy,
title={Off-policy Bandits with Deficient Support},
author={Noveen Sachdeva and Yi Su and Thorsten Joachims},
year={2020},
url={https://openreview.net/forum?id=SklcyJBtvB}
}

@inproceedings{li2010contextual,
  title={A contextual-bandit approach to personalized news article recommendation},
  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={661--670},
  year={2010}
}

@article{o2018variational,
  title={Variational Bayesian reinforcement learning with regret bounds},
  author={O'Donoghue, Brendan},
  journal={arXiv preprint arXiv:1807.09647},
  year={2018}
}

@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in neural information processing systems},
  pages={2613--2621},
  year={2010}
}

@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}

@InProceedings{fujimoto2018addressing,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author = 	 {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  pages = 	 {1587--1596},
  year = 	 {2018},
}

@misc{d4rl_repo,
title = {D4RL: Datasets for Data-Driven Deep Reinforcement Learning},
author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
note = {Github repository},
year = {2020},
howpublished  = {\url{https://github.com/rail-berkeley/d4rl/wiki/New-Franka-Kitchen-Tasks}}
}

@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{furuichi2004fundamental,
  title={Fundamental properties of Tsallis relative entropy},
  author={Furuichi, Shigeru and Yanagi, Kenjiro and Kuriyama, Ken},
  journal={Journal of Mathematical Physics},
  volume={45},
  number={12},
  pages={4868--4877},
  year={2004},
  publisher={American Institute of Physics}
}

@inproceedings{namkoong2017variance,
  title={Variance-based regularization with convex objectives},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in neural information processing systems},
  pages={2971--2980},
  year={2017}
}

@inproceedings{tamar2014scaling,
  title={Scaling up robust MDPs using function approximation},
  author={Tamar, Aviv and Mannor, Shie and Xu, Huan},
  booktitle={International Conference on Machine Learning},
  pages={181--189},
  year={2014}
}

@article{nair2020accelerating,
  title={Accelerating Online Reinforcement Learning with Offline Datasets},
  author={Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@inproceedings{nilim2004robustness,
  title={Robustness in Markov decision problems with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  booktitle={Advances in neural information processing systems},
  pages={839--846},
  year={2004}
}

@article{russel2018tight,
  title={Tight bayesian ambiguity sets for robust MDPs},
  author={Russel, Reazul Hasan and Petrik, Marek},
  journal={arXiv preprint arXiv:1811.06512},
  year={2018}
}

@article{iyengar2005robust,
  title={Robust dynamic programming},
  author={Iyengar, Garud N},
  journal={Mathematics of Operations Research},
  volume={30},
  number={2},
  pages={257--280},
  year={2005},
  publisher={INFORMS}
}

@inproceedings{ghavamzadeh2016safe,
  title={Safe policy improvement by minimizing robust baseline regret},
  author={Petrik, Marek and Ghavamzadeh, Mohammad and Chow, Yinlam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2298--2306},
  year={2016}
}

@inproceedings{strehl2010learning,
  title={Learning from logged implicit exploration data},
  author={Strehl, Alex and Langford, John and Li, Lihong and Kakade, Sham M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2217--2225},
  year={2010}
}

@article{simao2019safe,
  title={Safe Policy Improvement with an Estimated Baseline Policy},
  author={Sim{\~a}o, Thiago D and Laroche, Romain and Combes, R{\'e}mi Tachet des},
  journal={arXiv preprint arXiv:1909.05236},
  year={2019}
}

@article{laroche2017safe,
  title={Safe policy improvement with baseline bootstrapping},
  author={Laroche, Romain and Trichelair, Paul and Combes, R{\'e}mi Tachet des},
  journal={arXiv preprint arXiv:1712.06924},
  year={2017}
}

@article{nachum2019algaedice,
  title={AlgaeDICE: Policy Gradient from Arbitrary Experience},
  author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1912.02074},
  year={2019}
}

@article{brac,
  title={Behavior Regularized Offline Reinforcement Learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1563--1600},
  year={2010}
}

@inproceedings{o2018uncertainty,
  title={The Uncertainty Bellman Equation and Exploration},
  author={O’Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
  booktitle={International Conference on Machine Learning},
  pages={3836--3845},
  year={2018}
}

@inproceedings{gilotte2018offline,
  title={Offline a/b testing for recommender systems},
  author={Gilotte, Alexandre and Calauz{\`e}nes, Cl{\'e}ment and Nedelec, Thomas and Abraham, Alexandre and Doll{\'e}, Simon},
  booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
  pages={198--206},
  year={2018}
}

@article{bottou2013counterfactual,
  title={Counterfactual reasoning and learning systems: The example of computational advertising},
  author={Bottou, L{\'e}on and Peters, Jonas and Qui{\~n}onero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={3207--3260},
  year={2013},
  publisher={JMLR. org}
}

@inproceedings{theocharous2015personalized,
  title={Personalized ad recommendation systems for life-time value optimization with guarantees},
  author={Theocharous, Georgios and Thomas, Philip S and Ghavamzadeh, Mohammad},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015}
}


@article{henderson2008hybrid,
  title={Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets},
  author={Henderson, James and Lemon, Oliver and Georgila, Kallirroi},
  journal={Computational Linguistics},
  volume={34},
  number={4},
  pages={487--511},
  year={2008},
  publisher={MIT Press}
}


@inproceedings{tesauro2006hybrid,
  title={A hybrid reinforcement learning approach to autonomic resource allocation},
  author={Tesauro, Gerald and Jong, Nicholas K and Das, Rajarshi and Bennani, Mohamed N},
  booktitle={2006 IEEE International Conference on Autonomic Computing},
  pages={65--73},
  year={2006},
  organization={IEEE}
}

@inproceedings{nazari2018reinforcement,
  title={Reinforcement learning for solving the vehicle routing problem},
  author={Nazari, Mohammadreza and Oroojlooy, Afshin and Snyder, Lawrence and Tak{\'a}c, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9839--9849},
  year={2018}
}

@article{balaji2010urban,
  title={Urban traffic signal control using reinforcement learning agents},
  author={Balaji, PG and German, X and Srinivasan, Dipti},
  journal={IET Intelligent Transport Systems},
  volume={4},
  number={3},
  pages={177--188},
  year={2010},
  publisher={IET}
}

@inproceedings{hein2017industrial,
  title={Batch reinforcement learning on the industrial benchmark: First experiences},
  author={Hein, Daniel and Udluft, Steffen and Tokic, Michel and Hentschel, Alexander and Runkler, Thomas A and Sterzing, Volkmar},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={4214--4221},
  year={2017},
  organization={IEEE}
}

@article{degris2012off,
  title={Off-policy actor-critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1205.4839},
  year={2012}
}

@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@article{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  journal={arXiv preprint arXiv:1802.01561},
  year={2018}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={22--31},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}

@article{fu2019diagnosing,
  title={Diagnosing bottlenecks in deep {Q}-learning algorithms},
  author={Fu, Justin and Kumar, Aviral and Soh, Matthew and Levine, Sergey},
  journal={arXiv preprint arXiv:1902.10250},
  year={2019}
}

@inproceedings{imani2018off,
  title={An off-policy policy gradient theorem using emphatic weightings},
  author={Imani, Ehsan and Graves, Eric and White, Martha},
  booktitle={Advances in Neural Information Processing Systems},
  pages={96--106},
  year={2018}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year={2014}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{gu2017interpolated,
  title={Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning},
  author={Gu, Shixiang Shane and Lillicrap, Timothy and Turner, Richard E and Ghahramani, Zoubin and Sch{\"o}lkopf, Bernhard and Levine, Sergey},
  booktitle={Advances in neural information processing systems},
  pages={3846--3855},
  year={2017}
}

@inproceedings{gelada2019off,
  title={Off-policy deep reinforcement learning by bootstrapping the covariate shift},
  author={Gelada, Carles and Bellemare, Marc G},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3647--3655},
  year={2019}
}

@article{gabel2008adaptive,
  title={Adaptive reactive job-shop scheduling with reinforcement learning agents},
  author={Gabel, Thomas and Riedmiller, Martin},
  journal={International Journal of Information Technology and Intelligent Computing},
  volume={24},
  number={4},
  pages={14--18},
  year={2008},
  publisher={Citeseer}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@article{peshkin2002learning,
  title={Learning from scarce experience},
  author={Peshkin, Leonid and Shelton, Christian R},
  journal={arXiv preprint cs/0204043},
  year={2002}
}

@inproceedings{precup2001off,
  title={Off-policy temporal-difference learning with function approximation},
  author={Precup, Doina and Sutton, Richard S and Dasgupta, Sanjoy},
  booktitle={ICML},
  pages={417--424},
  year={2001}
}

@inproceedings{hallak2017consistent,
  title={Consistent on-line off-policy evaluation},
  author={Hallak, Assaf and Mannor, Shie},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1372--1383},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016}
}

@inproceedings{wang2017optimal,
  title={Optimal and adaptive off-policy evaluation in contextual bandits},
  author={Wang, Yu-Xiang and Agarwal, Alekh and Dudik, Miroslav},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3589--3597},
  year={2017},
  organization={JMLR. org}
}

@article{jiang2015doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  journal={arXiv preprint arXiv:1511.03722},
  year={2015}
}

@article{farajtabar2018more,
  title={More robust doubly robust off-policy evaluation},
  author={Farajtabar, Mehrdad and Chow, Yinlam and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:1802.03493},
  year={2018}
}

@inproceedings{thomas2015high,
  title={High confidence policy improvement},
  author={Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={2380--2388},
  year={2015}
}


@inproceedings{scherrer2014approximate,
  title={Approximate policy iteration schemes: a comparison},
  author={Scherrer, Bruno},
  booktitle={International Conference on Machine Learning},
  pages={1314--1322},
  year={2014}
}

@article{kumar2022pre,
  title={Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials},
  author={Kumar, Aviral and Singh, Anikait and Ebert, Frederik and Yang, Yanlai and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:2210.05178},
  year={2022}
}

@article{ajay2020opal,
  title={Opal: Offline primitive discovery for accelerating offline reinforcement learning},
  author={Ajay, Anurag and Kumar, Aviral and Agrawal, Pulkit and Levine, Sergey and Nachum, Ofir},
  journal={arXiv preprint arXiv:2010.13611},
  year={2020}
}

@inproceedings{hanna2017bootstrapping,
  title={Bootstrapping with models: Confidence intervals for off-policy evaluation},
  author={Hanna, Josiah P and Stone, Peter and Niekum, Scott},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@inproceedings{thomas2015higheval,
  title={High-confidence off-policy evaluation},
  author={Thomas, Philip S and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}


@article{gu2016q,
  title={Q-prop: Sample-efficient policy gradient with an off-policy critic},
  author={Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.02247},
  year={2016}
}

@article{nadjahi2019safe,
  title={Safe Policy Improvement with Soft Baseline Bootstrapping},
  author={Nadjahi, Kimia and Laroche, Romain and Combes, R{\'e}mi Tachet des},
  journal={arXiv preprint arXiv:1907.05079},
  year={2019}
}

@article{cheng2019trajectory,
  title={Trajectory-wise control variates for variance reduction in policy gradient methods},
  author={Cheng, Ching-An and Yan, Xinyan and Boots, Byron},
  journal={arXiv preprint arXiv:1908.03263},
  year={2019}
}

@article{pankov2018reward,
  title={Reward-estimation variance elimination in sequential decision processes},
  author={Pankov, Sergey},
  journal={arXiv preprint arXiv:1811.06225},
  year={2018}
}

@article{huang2019importance,
  title={From Importance Sampling to Doubly Robust Policy Gradient},
  author={Huang, Jiawei and Jiang, Nan},
  journal={arXiv preprint arXiv:1910.09066},
  year={2019}
}

@article{sutton2016emphatic,
  title={An emphatic approach to the problem of off-policy temporal-difference learning},
  author={Sutton, Richard S and Mahmood, A Rupam and White, Martha},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2603--2631},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{yu2015convergence,
  title={On convergence of emphatic temporal-difference learning},
  author={Yu, Huizhen},
  booktitle={Conference on Learning Theory},
  pages={1724--1751},
  year={2015}
}

@inproceedings{hallak2016generalized,
  title={Generalized emphatic temporal difference learning: Bias-variance analysis},
  author={Hallak, Assaf and Tamar, Aviv and Munos, R{\'e}mi and Mannor, Shie},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@inproceedings{zhang2019generalized,
  title={Generalized off-policy actor-critic},
  author={Zhang, Shangtong and Boehmer, Wendelin and Whiteson, Shimon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1999--2009},
  year={2019}
}

@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{ghasemipour2021emaq,
  title={Emaq: Expected-max q-learning operator for simple yet effective offline and online rl},
  author={Ghasemipour, Seyed Kamyar Seyed and Schuurmans, Dale and Gu, Shixiang Shane},
  booktitle={International Conference on Machine Learning},
  pages={3682--3691},
  year={2021},
  organization={PMLR}
}

@article{hallak2015emphatic,
  title={Emphatic TD Bellman Operator is a Contraction},
  author={Hallak, Assaf and Tamar, Aviv and Mannor, Shie},
  journal={arXiv preprint arXiv:1508.03411},
  year={2015}
}

@inproceedings{liu2018breaking,
  title={Breaking the curse of horizon: Infinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5356--5366},
  year={2018}
}

@article{liu2019off,
  title={Off-policy policy gradient with state distribution correction},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:1904.08473},
  year={2019}
}

@article{mousavi2020black,
  title={Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning},
  author={Mousavi, Ali and Li, Lihong and Liu, Qiang and Zhou, Denny},
  journal={arXiv preprint arXiv:2003.11126},
  year={2020}
}

@article{kallus2019efficiently,
  title={Efficiently breaking the curse of horizon: Double reinforcement learning in infinite-horizon processes},
  author={Kallus, Nathan and Uehara, Masatoshi},
  journal={arXiv preprint arXiv:1909.05850},
  year={2019}
}

@inproceedings{kallus2019intrinsically,
  title={Intrinsically efficient, stable, and bounded off-policy evaluation for reinforcement learning},
  author={Kallus, Nathan and Uehara, Masatoshi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3320--3329},
  year={2019}
}

@article{uehara2019minimax,
  title={Minimax Weight and Q-Function Learning for Off-Policy Evaluation},
  author={Uehara, Masatoshi and Jiang, Nan},
  journal={arXiv preprint arXiv:1910.12809},
  year={2019}
}

@article{tang2019doubly,
  title={Doubly robust bias reduction in infinite horizon off-policy estimation},
  author={Tang, Ziyang and Feng, Yihao and Li, Lihong and Zhou, Dengyong and Liu, Qiang},
  journal={arXiv preprint arXiv:1910.07186},
  year={2019}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{dai2017boosting,
  title={Boosting the actor with dual critic},
  author={Dai, Bo and Shaw, Albert and He, Niao and Li, Lihong and Song, Le},
  journal={arXiv preprint arXiv:1712.10282},
  year={2017}
}

@article{wang2021instabilities,
  title={Instabilities of Offline RL with Pre-Trained Neural Representation},
  author={Wang, Ruosong and Wu, Yifan and Salakhutdinov, Ruslan and Kakade, Sham M},
  journal={arXiv preprint arXiv:2103.04947},
  year={2021}
}

@article{kostrikov2021offline,
  title={Offline Reinforcement Learning with Fisher Divergence Critic Regularization},
  author={Kostrikov, Ilya and Tompson, Jonathan and Fergus, Rob and Nachum, Ofir},
  journal={arXiv preprint arXiv:2103.08050},
  year={2021}
}

@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline rl?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@article{dai2017sbeed,
  title={SBEED: Convergent reinforcement learning with nonlinear function approximation},
  author={Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
  journal={arXiv preprint arXiv:1712.10285},
  year={2017}
}

@article{chen2016stochastic,
  title={Stochastic primal-dual methods and sample complexity of reinforcement learning},
  author={Chen, Yichen and Wang, Mengdi},
  journal={arXiv preprint arXiv:1612.02516},
  year={2016}
}

@inproceedings{wang2016online,
  title={An online primal-dual method for discounted markov decision processes},
  author={Wang, Mengdi and Chen, Yichen},
  booktitle={2016 IEEE 55th Conference on Decision and Control (CDC)},
  pages={4516--4521},
  year={2016},
  organization={IEEE}
}

@inproceedings{
kumar2021implicit,
title={Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning},
author={Aviral Kumar and Rishabh Agarwal and Dibya Ghosh and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=O9bnihsFfXU}
}

@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}

@article{durugkar2018td,
  title={TD learning with constrained gradients},
  author={Durugkar, Ishan and Stone, Peter},
  year={2018}
}

@article{singh2020cog,
  title={COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning},
  author={Singh, Avi and Yu, Albert and Yang, Jonathan and Zhang, Jesse and Kumar, Aviral and Levine, Sergey},
  journal={arXiv preprint arXiv:2010.14500},
  year={2020}
}

@inproceedings{dong2020expressivity,
  title={On the expressivity of neural networks for deep reinforcement learning},
  author={Dong, Kefan and Luo, Yuping and Yu, Tianhe and Finn, Chelsea and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={2627--2637},
  year={2020},
  organization={PMLR}
}

@inproceedings{savarese2019infinite,
  title={How do infinite width bounded norm networks look in function space?},
  author={Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={2667--2690},
  year={2019},
  organization={PMLR}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{
kumar2021workflow,
title={A Workflow for Offline Model-Free Robotic Reinforcement Learning},
author={Aviral Kumar and Anikait Singh and Stephen Tian and Chelsea Finn and Sergey Levine},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
url={https://openreview.net/forum?id=fy4ZBWxYbIo}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@inproceedings{ghiassian2020gradient,
  title={Gradient temporal-difference learning with regularized corrections},
  author={Ghiassian, Sina and Patterson, Andrew and Garg, Shivam and Gupta, Dhawal and White, Adam and White, Martha},
  booktitle={International Conference on Machine Learning},
  pages={3524--3534},
  year={2020},
  organization={PMLR}
}

@inproceedings{gogianu2021spectral,
  title={Spectral normalisation for deep reinforcement learning: an optimisation perspective},
  author={Gogianu, Florin and Berariu, Tudor and Rosca, Mihaela C and Clopath, Claudia and Busoniu, Lucian and Pascanu, Razvan},
  booktitle={International Conference on Machine Learning},
  pages={3734--3744},
  year={2021},
  organization={PMLR}
}

@article{lee2021versions,
  title={Versions of Gradient Temporal Difference Learning},
  author={Lee, Donghwan and Lim, Han-Dong and Park, Jihoon and Choi, Okyong},
  journal={arXiv preprint arXiv:2109.04033},
  year={2021}
}

@article{cheng2022adversarially,
  title={Adversarially Trained Actor Critic for Offline Reinforcement Learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2202.02446},
  year={2022}
}

@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{kostrikov2020image,
  title={Image augmentation is all you need: Regularizing deep reinforcement learning from pixels},
  author={Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  journal={arXiv preprint arXiv:2004.13649},
  year={2020}
}

@article{kumar2021dr3,
  title={{DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization}},
  author={Kumar, Aviral and Agarwal, Rishabh and Ma, Tengyu and Courville, Aaron and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2112.04716},
  year={2021}
}

@article{pohlen2018observe,
  title={Observe and look further: Achieving consistent performance on atari},
  author={Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and Van Hasselt, Hado and Quan, John and Ve{\v{c}}er{\'\i}k, Mel and others},
  journal={arXiv preprint arXiv:1805.11593},
  year={2018}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zanette2020exponential,
  title={Exponential Lower Bounds for Batch Reinforcement Learning: Batch RL can be Exponentially Harder than Online RL},
  author={Zanette, Andrea},
  journal={arXiv preprint arXiv:2012.08005},
  year={2020}
}

@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={arXiv preprint arXiv:2005.13239},
  year={2020}
}

@inproceedings{
wang2021what,
title={What are the Statistical Limits of Offline {\{}RL{\}} with Linear Function Approximation?},
author={Ruosong Wang and Dean Foster and Sham M. Kakade},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=30EvkP2aQLD}
}

@article{liu2020provably,
  title={Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{duan2020minimax,
  title={Minimax-optimal off-policy evaluation with linear function approximation},
  author={Duan, Yaqi and Jia, Zeyu and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={2701--2709},
  year={2020},
  organization={PMLR}
}

@article{lee2018stochastic,
  title={Stochastic primal-dual Q-learning},
  author={Lee, Donghwan and He, Niao},
  journal={arXiv preprint arXiv:1810.08298},
  year={2018}
}

@inproceedings{swaminathan2015counterfactual,
  title={Counterfactual risk minimization: Learning from logged bandit feedback},
  author={Swaminathan, Adith and Joachims, Thorsten},
  booktitle={International Conference on Machine Learning},
  pages={814--823},
  year={2015}
}

@article{hayes2005large,
  title={A large-deviation inequality for vector-valued martingales},
  author={Hayes, Thomas P},
  journal={Combinatorics, Probability and Computing},
  year={2005}
}


@ARTICLE{2019arXiv190600949K,
       author = {{Kumar}, Aviral and {Fu}, Justin and {Tucker}, George and {Levine}, Sergey},
        title = "{Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2019,
        month = jun,
          eid = {arXiv:1906.00949},
        pages = {arXiv:1906.00949},
archivePrefix = {arXiv},
       eprint = {1906.00949},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190600949K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2021arXiv211001548A,
       author = {{An}, Gaon and {Moon}, Seungyong and {Kim}, Jang-Hyun and {Song}, Hyun Oh},
        title = "{Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
         year = 2021,
        month = oct,
          eid = {arXiv:2110.01548},
        pages = {arXiv:2110.01548},
archivePrefix = {arXiv},
       eprint = {2110.01548},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv211001548A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{AWAC,
  author    = {Ashvin Nair and
               Murtaza Dalal and
               Abhishek Gupta and
               Sergey Levine},
  title     = {Accelerating Online Reinforcement Learning with Offline Datasets},
  journal   = {CoRR},
  volume    = {abs/2006.09359},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.09359},
  eprinttype = {arXiv},
  eprint    = {2006.09359},
  timestamp = {Wed, 17 Jun 2020 14:28:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-09359.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{one_step_RL,
  author    = {David Brandfonbrener and
               William F. Whitney and
               Rajesh Ranganath and
               Joan Bruna},
  title     = {Offline {RL} Without Off-Policy Evaluation},
  journal   = {CoRR},
  volume    = {abs/2106.08909},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.08909},
  eprinttype = {arXiv},
  eprint    = {2106.08909},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08909.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{td3bc,
  author    = {Scott Fujimoto and
               Shixiang Shane Gu},
  title     = {A Minimalist Approach to Offline Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2106.06860},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.06860},
  eprinttype = {arXiv},
  eprint    = {2106.06860},
  timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-06860.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
xiao2023the,
title={The In-Sample Softmax for Offline Reinforcement Learning},
author={Chenjun Xiao and Han Wang and Yangchen Pan and Adam White and Martha White},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=u-RuvyDYqCM}
}


%%%%%%%%%%%%%%%%%%%%
%%% Below are bib for the ROLLIN paper
%%%%%%%%%%%%%%%%%%%%
@inproceedings{sidford2018variance,
  title={Variance reduced value iteration and faster algorithms for solving markov decision processes},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={770--787},
  year={2018},
  organization={SIAM}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving discounted Markov decision process with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  journal={arXiv preprint arXiv:1806.01492},
  year={2018}
}
@article{haarnoja2018soft,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{
goyal2018transfer,
title={Transfer and Exploration via the Information Bottleneck},
author={Anirudh Goyal and Riashat Islam and DJ Strouse and Zafarali Ahmed and Hugo Larochelle and Matthew Botvinick and Sergey Levine and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJg8yhAqKm},
}

@inproceedings{
Goyal2020The,
title={The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget},
author={Anirudh Goyal and Yoshua Bengio and Matthew Botvinick and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hye1kTVFDS}
}

@inproceedings{alemi2016deep,
title	= {Deep Variational Information Bottleneck},
author	= {Alex Alemi and Ian Fischer and Josh Dillon and Kevin Murphy},
year	= {2017},
URL	= {https://arxiv.org/abs/1612.00410},
booktitle	= {ICLR}
}



@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020}
}

@article{yang2019sample,
  title={Sample-optimal parametric q-learning using linearly additive features},
  author={Yang, Lin F and Wang, Mengdi},
  journal={arXiv preprint arXiv:1902.04779},
  year={2019}
}

@article{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={arXiv preprint arXiv:2005.12900},
  year={2020}
}

@article{wang2020randomized,
  title={Randomized linear programming solves the Markov decision problem in nearly linear (sometimes sublinear) time},
  author={Wang, Mengdi},
  journal={Mathematics of Operations Research},
  volume={45},
  number={2},
  pages={517--546},
  year={2020},
  publisher={INFORMS}
}

@article{azar2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  number={3},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@inproceedings{kearns1999finite,
  title={Finite-sample convergence rates for Q-learning and indirect algorithms},
  author={Kearns, Michael J and Singh, Satinder P},
  booktitle={Advances in neural information processing systems},
  pages={996--1002},
  year={1999}
}

@inproceedings{yang2020harnessing,
  title={Harnessing Structures for Value-Based Planning and Reinforcement Learning},
  author={Yuzhe Yang and Guo Zhang and Zhi Xu and Dina Katabi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=rklHqRVKvH}
}

@inproceedings{dubeyICLRW18human,
    Author = {Dubey, Rachit and Agrawal, Pulkit
    and Pathak, Deepak and Griffiths, Thomas L.
    and Efros, Alexei A.},
    Title = {Investigating Human Priors for
    Playing Video Games},
    Booktitle = {ICML},
    Year = {2018},
}


@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{pananjady2020instance,
  title={Instance-dependent ℓ∞-bounds for policy evaluation in tabular reinforcement learning},
  author={Pananjady, Ashwin and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  year={2020},
  publisher={IEEE}
}

@article{wainwright2019variance,
  title={Variance-reduced $ Q $-learning is minimax optimal},
  author={Wainwright, Martin J},
  journal={arXiv preprint arXiv:1906.04697},
  year={2019}
}


@inproceedings{
Wang2020Exploring,
title={Exploring Model-based Planning with Policy Networks},
author={Tingwu Wang and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1exf64KwH}
}

@article{chua2018deep,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{janner2019trust,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={16--17},
  year={2017}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={ICML},
  volume={99},
  pages={278--287},
  year={1999}
}

@article{even2003learning,
  title={Learning rates for Q-learning},
  author={Even-Dar, Eyal and Mansour, Yishay},
  journal={Journal of machine learning Research},
  volume={5},
  number={Dec},
  pages={1--25},
  year={2003}
}

@inproceedings{jiang2018open,
  title={Open problem: The dependence of sample complexity lower bounds on planning horizon},
  author={Jiang, Nan and Agarwal, Alekh},
  booktitle={Conference On Learning Theory},
  pages={3395--3398},
  year={2018}
}

@inproceedings{forejt2011automated,
  title={Automated verification techniques for probabilistic systems},
  author={Forejt, Vojt{\v{e}}ch and Kwiatkowska, Marta and Norman, Gethin and Parker, David},
  booktitle={International School on Formal Methods for the Design of Computer, Communication and Software Systems},
  pages={53--113},
  year={2011},
  organization={Springer}
}

@inproceedings{
liu2018competitive,
title={Competitive experience replay},
author={Hao Liu and Alexander Trott and Richard Socher and Caiming Xiong},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Sklsm20ctX},
}

@incollection{ng2006autonomous,
  title={Autonomous inverted helicopter flight via reinforcement learning},
  author={Ng, Andrew Y and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
  booktitle={Experimental robotics IX},
  pages={363--372},
  year={2006},
  publisher={Springer}
}

@article{levine2018learning,
  title={Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
  author={Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
  journal={The International Journal of Robotics Research},
  volume={37},
  number={4-5},
  pages={421--436},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{tian2019elf,
  title={Elf opengo: An analysis and open reimplementation of alphazero},
  author={Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1902.04522},
  year={2019}
}

@article{li2018deep,
  title={Deep reinforcement learning},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1810.06339},
  year={2018}
}

@article{guestrin2003efficient,
  title={Efficient solution algorithms for factored MDPs},
  author={Guestrin, Carlos and Koller, Daphne and Parr, Ronald and Venkataraman, Shobha},
  journal={Journal of Artificial Intelligence Research},
  volume={19},
  pages={399--468},
  year={2003}
}

@article{boutilier2000stochastic,
  title={Stochastic dynamic programming with factored representations},
  author={Boutilier, Craig and Dearden, Richard and Goldszmidt, Mois{\'e}s},
  journal={Artificial intelligence},
  volume={121},
  number={1-2},
  pages={49--107},
  year={2000},
  publisher={Elsevier}
}

%%% Q learning convergence paper

@inproceedings{bertsekas1995neuro,
  title={Neuro-dynamic programming: an overview},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  booktitle={Proceedings of 1995 34th IEEE conference on decision and control},
  volume={1},
  pages={560--564},
  year={1995},
  organization={IEEE}
}

@article{tsitsiklis1994asynchronous,
  title={Asynchronous stochastic approximation and Q-learning},
  author={Tsitsiklis, John N},
  journal={Machine learning},
  volume={16},
  number={3},
  pages={185--202},
  year={1994},
  publisher={Springer}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{littman1996generalized,
  title={A generalized reinforcement-learning model: Convergence and applications},
  author={Littman, Michael L and Szepesv{\'a}ri, Csaba},
  booktitle={ICML},
  volume={96},
  pages={310--318},
  year={1996},
  organization={Citeseer}
}

@article{jaakkola1994convergence,
  title={On the convergence of stochastic iterative dynamic programming algorithms},
  author={Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},
  journal={Neural computation},
  volume={6},
  number={6},
  pages={1185--1201},
  year={1994},
  publisher={MIT Press}
}

@article{borkar2000ode,
  title={The ODE method for convergence of stochastic approximation and reinforcement learning},
  author={Borkar, Vivek S and Meyn, Sean P},
  journal={SIAM Journal on Control and Optimization},
  volume={38},
  number={2},
  pages={447--469},
  year={2000},
  publisher={SIAM}
}

%%% Q learning convergence paper

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{liu2020learning,
  title={Learning Abstract Models for Strategic Exploration and Fast Reward Transfer},
  author={Liu, Evan Zheran and Keramati, Ramtin and Seshadri, Sudarshan and Guu, Kelvin and Pasupat, Panupong and Brunskill, Emma and Liang, Percy},
  journal={arXiv preprint arXiv:2007.05896},
  year={2020}
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@misc{Brockman2016OpenAI,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@inproceedings{
gleave2021quantifying,
title={Quantifying Differences in Reward Functions},
author={Adam Gleave and Michael D Dennis and Shane Legg and Stuart Russell and Jan Leike},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=LwEQnp6CYev}
}

@article{kearns2002sparse,
  title={A sparse sampling algorithm for near-optimal planning in large Markov decision processes},
  author={Kearns, Michael and Mansour, Yishay and Ng, Andrew Y},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={193--208},
  year={2002},
  publisher={Springer}
}

@article{boutilier1999decision,
  title={Decision-theoretic planning: Structural assumptions and computational leverage},
  author={Boutilier, Craig and Dean, Thomas and Hanks, Steve},
  journal={Journal of Artificial Intelligence Research},
  volume={11},
  pages={1--94},
  year={1999}
}


@book{powell2007approximate,
  title={Approximate Dynamic Programming: Solving the curses of dimensionality},
  author={Powell, Warren B},
  volume={703},
  year={2007},
  publisher={John Wiley \& Sons}
}

@article{rintanen2001overview,
  title={An overview of recent algorithms for AI planning},
  author={Rintanen, Jussi and Hoffmann, J{\"o}rg},
  journal={KI},
  volume={15},
  number={2},
  pages={5--11},
  year={2001}
}


%%%%%
% Books for MDP
%%%%%

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@book{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  year={1996},
  publisher={Athena Scientific}
}

@book{bertsekas2019reinforcement,
  title={Reinforcement Learning and Optimal Control},
  author={Bertsekas, Dimitri P},
  year = {2019},
  publisher = {Athena Scientific}
}

@book{bertsekas1995dynamic,
  title={Dynamic Programming and Optimal Control},
  author={Bertsekas, Dimitri P},
  year={1995},
  publisher={Athena Scientific}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{szepesvari2010algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

%%%%%
% Hierarchical Robotics Planning
%%%%%

@inproceedings{kaelbling2011hierarchical,
  title={Hierarchical task and motion planning in the now},
  author={Kaelbling, Leslie Pack and Lozano-P{\'e}rez, Tom{\'a}s},
  booktitle={2011 IEEE International Conference on Robotics and Automation},
  pages={1470--1477},
  year={2011},
  organization={IEEE}
}

@article{silver2021learning,
  title={Learning Symbolic Operators for Task and Motion Planning},
  author={Silver, Tom and Chitnis, Rohan and Tenenbaum, Joshua and Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
  journal={arXiv preprint arXiv:2103.00589},
  year={2021}
}

@inproceedings{singh2009rewards,
  title={Where do rewards come from},
  author={Singh, Satinder and Lewis, Richard L and Barto, Andrew G},
  booktitle={Proceedings of the annual conference of the cognitive science society},
  pages={2601--2606},
  year={2009},
  organization={Cognitive Science Society}
}

@article{singh2010intrinsically,
  title={Intrinsically motivated reinforcement learning: An evolutionary perspective},
  author={Singh, Satinder and Lewis, Richard L and Barto, Andrew G and Sorg, Jonathan},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={2},
  number={2},
  pages={70--82},
  year={2010},
  publisher={IEEE}
}


%%%%%%
% Methods that study the hierarchical structures with subgoals
%%%%%%

@article{wen2020efficiency,
  title={On Efficiency in Hierarchical Reinforcement Learning},
  author={Wen, Zheng and Precup, Doina and Ibrahimi, Morteza and Barreto, Andre and Van Roy, Benjamin and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{kaelbling1993hierarchical,
  title={Hierarchical learning in stochastic domains: Preliminary results},
  author={Kaelbling, Leslie Pack},
  booktitle={Proceedings of the tenth international conference on machine learning},
  volume={951},
  pages={167--173},
  year={1993}
}

@inproceedings{dayan1992feudal,
 author = {Dayan, Peter and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Feudal Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Paper.pdf},
 volume = {5},
 year = {1993}
}

@inproceedings{sutton1998intra,
  title={Intra-Option Learning about Temporally Abstract Actions.},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder P},
  booktitle={ICML},
  volume={98},
  pages={556--564},
  year={1998}
}

@inproceedings{diuk2008object,
  title={An object-oriented representation for efficient reinforcement learning},
  author={Diuk, Carlos and Cohen, Andre and Littman, Michael L},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={240--247},
  year={2008}
}

@inproceedings{vezhnevets2017feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={3540--3549},
  year={2017},
  organization={PMLR}
}

@article{dietterich2000hierarchical,
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={Journal of artificial intelligence research},
  volume={13},
  pages={227--303},
  year={2000}
}

@article{kulkarni2016hierarchical,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik R and Saeedi, Ardavan and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:1604.06057},
  year={2016}
}

@inproceedings{le2018hierarchical,
  title={Hierarchical imitation and reinforcement learning},
  author={Le, Hoang and Jiang, Nan and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Yue, Yisong and Daum{\'e}, Hal},
  booktitle={International Conference on Machine Learning},
  pages={2917--2926},
  year={2018},
  organization={PMLR}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{barto2003recent,
  title={Recent advances in hierarchical reinforcement learning},
  author={Barto, Andrew G and Mahadevan, Sridhar},
  journal={Discrete event dynamic systems},
  volume={13},
  number={1},
  pages={41--77},
  year={2003},
  publisher={Springer}
}

@article{parr1998reinforcement,
  title={Reinforcement learning with hierarchies of machines},
  author={Parr, Ronald and Russell, Stuart},
  journal={Advances in neural information processing systems},
  pages={1043--1049},
  year={1998},
  publisher={MORGAN KAUFMANN PUBLISHERS}
}

@book{parr1998hierarchical,
  title={Hierarchical control and learning for Markov decision processes},
  author={Parr, Ronald Edward},
  year={1998},
  publisher={University of California, Berkeley Berkeley, CA}
}

%% Subgoals
@inproceedings{dean1995decomposition,
  title={Decomposition techniques for planning in stochastic domains},
  author={Dean, Thomas and Lin, Shieu-Hong},
  booktitle={IJCAI},
  volume={2},
  pages={3},
  year={1995},
  organization={Citeseer}
}

@article{singh1998dynamically,
  title={How to dynamically merge Markov decision processes},
  author={Singh, Satinder and Cohn, David and others},
  journal={Advances in neural information processing systems},
  number={10},
  pages={1057--1063},
  year={1998},
  publisher={Citeseer}
}

@inproceedings{meuleau1998solving,
  title={Solving very large weakly coupled Markov decision processes},
  author={Meuleau, Nicolas and Hauskrecht, Milos and Kim, Kee-Eung and Peshkin, Leonid and Kaelbling, Leslie Pack and Dean, Thomas L and Boutilier, Craig},
  booktitle={AAAI/IAAI},
  pages={165--172},
  year={1998}
}

@article{mann2015approximate,
  title={Approximate value iteration with temporally extended actions},
  author={Mann, Timothy A and Mannor, Shie and Precup, Doina},
  journal={Journal of Artificial Intelligence Research},
  volume={53},
  pages={375--438},
  year={2015}
}

@inproceedings{stolle2002learning,
  title={Learning options in reinforcement learning},
  author={Stolle, Martin and Precup, Doina},
  booktitle={International Symposium on abstraction, reformulation, and approximation},
  pages={212--223},
  year={2002},
  organization={Springer}
}

@inproceedings{simsek2008skill,
  title={Skill characterization based on betweenness},
  author={Simsek, Ozgur and Barreto, Andre S},
  booktitle={Advances in neural information processing systems},
  pages={1497--1504},
  year={2008}
}

@article{solway2014optimal,
  title={Optimal behavioral hierarchy},
  author={Solway, Alec and Diuk, Carlos and C{\'o}rdova, Natalia and Yee, Debbie and Barto, Andrew G and Niv, Yael and Botvinick, Matthew M},
  journal={PLOS Comput Biol},
  volume={10},
  number={8},
  pages={e1003779},
  year={2014},
  publisher={Public Library of Science}
}

@inproceedings{jiang2016structural,
  title={On Structural Properties of MDPs that Bound Loss Due to Shallow Planning.},
  author={Jiang, Nan and Singh, Satinder P and Tewari, Ambuj},
  booktitle={IJCAI},
  pages={1640--1647},
  year={2016}
}

@inproceedings{bakker2004hierarchical,
  title={Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization},
  author={Bakker, Bram and Schmidhuber, J{\"u}rgen and others},
  booktitle={Proc. of the 8-th Conf. on Intelligent Autonomous Systems},
  pages={438--445},
  year={2004},
  organization={Citeseer}
}

@article{mcgovern2001automatic,
  title={Automatic discovery of subgoals in reinforcement learning using diverse density},
  author={McGovern, Amy and Barto, Andrew G},
  year={2001}
}

@inproceedings{goel2003subgoal,
  title={Subgoal discovery for hierarchical reinforcement learning using learned policies},
  author={Goel, Sandeep and Huber, Manfred},
  booktitle={FLAIRS conference},
  pages={346--350},
  year={2003}
}

%% Robotics planning

@inproceedings{gopalan2017planning,
  title={Planning with abstract Markov decision processes},
  author={Gopalan, Nakul and Littman, Michael and MacGlashan, James and Squire, Shawn and Tellex, Stefanie and Winder, John and Wong, Lawson and others},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={27},
  number={1},
  year={2017}
}

@inproceedings{goel2003subgoal,
  title={Subgoal discovery for hierarchical reinforcement learning using learned policies},
  author={Goel, Sandeep and Huber, Manfred},
  booktitle={FLAIRS conference},
  pages={346--350},
  year={2003}
}

@book{lavalle2006planning,
  title={Planning algorithms},
  author={LaValle, Steven M},
  year={2006},
  publisher={Cambridge university press}
}

@book{siciliano2010robotics,
  title={Robotics: modelling, planning and control},
  author={Siciliano, Bruno and Sciavicco, Lorenzo and Villani, Luigi and Oriolo, Giuseppe},
  year={2010},
  publisher={Springer Science \& Business Media}
}

@book{russell2016artificial,
  title={Artificial Intelligence: a modern approach},
  author={Russell, Stuart J. and Norvig, Peter},
  edition={3},
  year={2009},
  publisher={Pearson}
}

%% Planning & Subgoals & MDP

@article{xu2020hierarchial,
  title={Hierarchial Reinforcement Learning in StarCraft II with Human Expertise in Subgoals Selection},
  author={Xu, Xinyi and Huang, Tiancheng and Wei, Pengfei and Narayan, Akshay and Leong, Tze-Yun},
  journal={arXiv preprint arXiv:2008.03444},
  year={2020}
}

@inproceedings{goel2003subgoal,
  title={Subgoal discovery for hierarchical reinforcement learning using learned policies},
  author={Goel, Sandeep and Huber, Manfred},
  booktitle={FLAIRS conference},
  pages={346--350},
  year={2003}
}

@inproceedings{mcgovern1998hierarchical,
  title={Hierarchical optimal control of MDPs},
  author={McGovern, Amy and Precup, Doina and Ravindran, Balaraman and Singh, Satinder and Sutton, Richard S},
  booktitle={Proceedings of the Tenth Yale Workshop on Adaptive and Learning Systems},
  pages={186--191},
  year={1998}
}

@article{hauskrecht2013hierarchical,
  title={Hierarchical solution of Markov decision processes using macro-actions},
  author={Hauskrecht, Milos and Meuleau, Nicolas and Kaelbling, Leslie Pack and Dean, Thomas L and Boutilier, Craig},
  journal={arXiv preprint arXiv:1301.7381},
  year={2013}
}

@article{precup1998multi,
  title={Multi-time models for temporally abstract planning},
  author={Precup, Doina and Sutton, Richard S and Singh, S},
  journal={Advances in neural information processing systems},
  pages={1050--1056},
  year={1998},
  publisher={MORGAN KAUFMANN PUBLISHERS}
}
%%%%%
%% Carefully designed reward function papers:
%%%%%

@article{oh2015action,
  title={Action-conditional video prediction using deep networks in atari games},
  author={Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard and Singh, Satinder},
  journal={arXiv preprint arXiv:1507.08750},
  year={2015}
}

@article{sorg2010reward,
  title={Reward design via online gradient ascent},
  author={Sorg, Jonathan and Lewis, Richard L and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  pages={2190--2198},
  year={2010},
  publisher={Citeseer}
}

@article{guo2014deep,
  title={Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning},
  author={Guo, Xiaoxiao and Singh, Satinder and Lee, Honglak and Lewis, Richard L and Wang, Xiaoshi},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={3338--3346},
  year={2014}
}

@phdthesis{guo2017deep,
  title={Deep learning and reward design for reinforcement learning},
  author={Guo, Xiaoxiao},
  year={2017}
}

@article{rudner2021outcome,
  title={Outcome-Driven Reinforcement Learning via Variational Inference},
  author={Rudner, Tim GJ and Pong, Vitchyr H and McAllister, Rowan and Gal, Yarin and Levine, Sergey},
  journal={arXiv preprint arXiv:2104.10190},
  year={2021}
}

%(The pacman)
@inproceedings{
Raileanu2020RIDE:,
title={RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments},
author={Roberta Raileanu and Tim Rocktäschel},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkg-TJBFPB}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@misc{OpenAI_dota,
      author = {OpenAI},
      title = {OpenAI Five},
      howpublished = {\url{https://blog.openai.com/openai-five/}},
      year = {2018}
}

@article{fuchs2021super,
  title={Super-human performance in gran turismo sport using deep reinforcement learning},
  author={Fuchs, Florian and Song, Yunlong and Kaufmann, Elia and Scaramuzza, Davide and D{\"u}rr, Peter},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={3},
  pages={4257--4264},
  year={2021},
  publisher={IEEE}
}

@inproceedings{ye2020towards,
 author = {Ye, Deheng and Chen, Guibin and Zhang, Wen and Chen, Sheng and Yuan, Bo and Liu, Bo and Chen, Jia and Liu, Zhao and Qiu, Fuhao and Yu, Hongsheng and Yin, Yinyuting and Shi, Bei and Wang, Liang and Shi, Tengfei and Fu, Qiang and Yang, Wei and Huang, Lanxiao and Liu, Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {621--632},
 publisher = {Curran Associates, Inc.},
 title = {Towards Playing Full MOBA Games with Deep Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/06d5ae105ea1bea4d800bc96491876e9-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{vinyals2017starcraft,
  title={Starcraft ii: A new challenge for reinforcement learning},
  author={Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:1708.04782},
  year={2017}
}

@inproceedings{racaniere2017imagination,
  title={Imagination-augmented agents for deep reinforcement learning},
  author={Racani{\`e}re, S{\'e}bastien and Weber, Th{\'e}ophane and Reichert, David P and Buesing, Lars and Guez, Arthur and Rezende, Danilo and Badia, Adria Puigdomenech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and others},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={5694--5705},
  year={2017}
}

@article{popov2017data,
  title={Data-efficient deep reinforcement learning for dexterous manipulation},
  author={Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1704.03073},
  year={2017}
}

%%%
% Negative result of ``bad'' reward design
%%%

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@inproceedings{VanSeijen2017Hybrid,
 author = {Van Seijen, Harm and Fatemi, Mehdi and Romoff, Joshua and Laroche, Romain and Barnes, Tavian and Tsang, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Hybrid Reward Architecture for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/1264a061d82a2edae1574b07249800d6-Paper.pdf},
 volume = {30},
 year = {2017}
}



%%%
% Deep Learning
%%%

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  number={2},
  year={2016},
  publisher={MIT press Cambridge}
}

@article{Krakovsky2016Reinforcement,
author = {Krakovsky, Marina},
title = {Reinforcement Renaissance},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/2949662},
doi = {10.1145/2949662},
abstract = {The power of deep neural networks has sparked renewed interest in reinforcement learning, with applications to games, robotics, and beyond.},
journal = {Commun. ACM},
month = jul,
pages = {12–14},
numpages = {3}
}

%% Mujoco

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={1329--1338},
  year={2016},
  organization={PMLR}
}

@inproceedings{nagabandi2018neural,
  title={Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author={Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7559--7566},
  year={2018},
  organization={IEEE}
}

@inproceedings{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2015},
      booktitle={International Conference on Learning Representations},
}

@misc{tieleman2012rmsprop,
  title={Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

%% Citing for JAIR
@article{kolter2011fixed,
  title={The fixed points of off-policy TD},
  author={Kolter, J},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  pages={2169--2177},
  year={2011}
}

@article{doroudi2019s,
  title={Where’s the reward?},
  author={Doroudi, Shayan and Aleven, Vincent and Brunskill, Emma},
  journal={International Journal of Artificial Intelligence in Education},
  volume={29},
  number={4},
  pages={568--620},
  year={2019},
  publisher={Springer}
}

%%% Reward Design HCI and Robotics
@article{he2021assisted,
  title={Assisted Robust Reward Design},
  author={He, Jerry Zhi-Yang and Dragan, Anca D},
  journal={arXiv preprint arXiv:2111.09884},
  year={2021}
}

%%% Goal Conditioned and Reward Design

@article{nasiriany2019planning,
  title={Planning with goal-conditioned policies},
  author={Nasiriany, Soroush and Pong, Vitchyr H and Lin, Steven and Levine, Sergey},
  journal={arXiv preprint arXiv:1911.08453},
  year={2019}
}

@inproceedings{schmidhuber1991curious,
  title={Curious model-building control systems},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. international joint conference on neural networks},
  pages={1458--1463},
  year={1991}
}

@inproceedings{portelas2020teacher,
  title={Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments},
  author={Portelas, R{\'e}my and Colas, C{\'e}dric and Hofmann, Katja and Oudeyer, Pierre-Yves},
  booktitle={Conference on Robot Learning},
  pages={835--853},
  year={2020},
  organization={PMLR}
}

@article{nair2018visual,
  title={Visual reinforcement learning with imagined goals},
  author={Nair, Ashvin V and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{eysenbach2020rewriting,
  title={Rewriting history with inverse rl: Hindsight inference for policy improvement},
  author={Eysenbach, Ben and Geng, Xinyang and Levine, Sergey and Salakhutdinov, Russ R},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={14783--14795},
  year={2020}
}

@article{mendonca2021discovering,
  title={Discovering and achieving goals via world models},
  author={Mendonca, Russell and Rybkin, Oleh and Daniilidis, Kostas and Hafner, Danijar and Pathak, Deepak},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@inproceedings{hafner2019learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International conference on machine learning},
  pages={2555--2565},
  year={2019},
  organization={PMLR}
}

@article{hafner2020mastering,
  title={Mastering atari with discrete world models},
  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal={arXiv preprint arXiv:2010.02193},
  year={2020}
}

@article{peng2018variational,
  title={Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow},
  author={Peng, Xue Bin and Kanazawa, Angjoo and Toyer, Sam and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.00821},
  year={2018}
}

@article{hafner2019dream,
  title={Dream to control: Learning behaviors by latent imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2019}
}

@inproceedings{tang2021hindsight,
  title={Hindsight expectation maximization for goal-conditioned reinforcement learning},
  author={Tang, Yunhao and Kucukelbir, Alp},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2863--2871},
  year={2021},
  organization={PMLR}
}

@article{borsa2018universal,
  title={Universal successor features approximators},
  author={Borsa, Diana and Barreto, Andr{\'e} and Quan, John and Mankowitz, Daniel and Munos, R{\'e}mi and Van Hasselt, Hado and Silver, David and Schaul, Tom},
  journal={arXiv preprint arXiv:1812.07626},
  year={2018}
}

@article{ghosh2019learning,
  title={Learning to reach goals via iterated supervised learning},
  author={Ghosh, Dibya and Gupta, Abhishek and Reddy, Ashwin and Fu, Justin and Devin, Coline and Eysenbach, Benjamin and Levine, Sergey},
  journal={arXiv preprint arXiv:1912.06088},
  year={2019}
}

@article{eysenbach2020c,
  title={C-learning: Learning to achieve goals via recursive classification},
  author={Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey},
  journal={arXiv preprint arXiv:2011.08909},
  year={2020}
}

@article{rudner2021outcome,
  title={Outcome-Driven Reinforcement Learning via Variational Inference},
  author={Rudner, Tim GJ and Pong, Vitchyr H and McAllister, Rowan and Gal, Yarin and Levine, Sergey},
  journal={arXiv preprint arXiv:2104.10190},
  year={2021}
}

@article{fu2018variational,
  title={Variational inverse control with events: A general framework for data-driven reward definition},
  author={Fu, Justin and Singh, Avi and Ghosh, Dibya and Yang, Larry and Levine, Sergey},
  journal={arXiv preprint arXiv:1805.11686},
  year={2018}
}

@article{pong2019skew,
  title={Skew-fit: State-covering self-supervised reinforcement learning},
  author={Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  journal={arXiv preprint arXiv:1903.03698},
  year={2019}
}

@inproceedings{andreas2017modular,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={166--175},
  year={2017},
  organization={PMLR}
}

@article{eysenbach2021replacing,
  title={Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification},
  author={Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2103.12656},
  year={2021}
}

@article{chebotar2021actionable,
  title={Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills},
  author={Chebotar, Yevgen and Hausman, Karol and Lu, Yao and Xiao, Ted and Kalashnikov, Dmitry and Varley, Jake and Irpan, Alex and Eysenbach, Benjamin and Julian, Ryan and Finn, Chelsea and others},
  journal={arXiv preprint arXiv:2104.07749},
  year={2021}
}

@article{khazatsky2021can,
  title={What Can I Do Here? Learning New Skills by Imagining Visual Affordances},
  author={Khazatsky, Alexander and Nair, Ashvin and Jing, Daniel and Levine, Sergey},
  journal={arXiv preprint arXiv:2106.00671},
  year={2021}
}

@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993},
  organization={Citeseer}
}

@inproceedings{sutton2011horde,
  title={Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction},
  author={Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam and Precup, Doina},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  pages={761--768},
  year={2011}
}

@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1707.01495},
  year={2017}
}

@article{pong2018temporal,
  title={Temporal difference models: Model-free deep rl for model-based control},
  author={Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.09081},
  year={2018}
}


@article{kadian2020sim2real,
  title={Sim2Real predictivity: Does evaluation in simulation predict real-world performance?},
  author={Kadian, Abhishek and Truong, Joanne and Gokaslan, Aaron and Clegg, Alexander and Wijmans, Erik and Lee, Stefan and Savva, Manolis and Chernova, Sonia and Batra, Dhruv},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={4},
  pages={6670--6677},
  year={2020},
  publisher={IEEE}
}

@inproceedings{fujita2020distributed,
  title={Distributed Reinforcement Learning of Targeted Grasping with Active Vision for Mobile Manipulators},
  author={Fujita, Yasuhiro and Uenishi, Kota and Ummadisingu, Avinash and Nagarajan, Prabhat and Masuda, Shimpei and Castro, Mario Ynocente},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={9712--9719},
  year={2020},
  organization={IEEE}
}

@article{ratner2018simplifying,
  title={Simplifying reward design through divide-and-conquer},
  author={Ratner, Ellis and Hadfield-Menell, Dylan and Dragan, Anca D},
  journal={arXiv preprint arXiv:1806.02501},
  year={2018}
}

@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={international conference on machine learning},
  pages={1311--1320},
  year={2017},
  organization={PMLR}
}

@article{parker2022evolving,
  title={Evolving Curricula with Regret-Based Environment Design},
  author={Parker-Holder, Jack and Jiang, Minqi and Dennis, Michael and Samvelyan, Mikayel and Foerster, Jakob and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:2203.01302},
  year={2022}
}

@article{fang2020adaptive,
  title={Adaptive procedural task generation for hard-exploration problems},
  author={Fang, Kuan and Zhu, Yuke and Savarese, Silvio and Fei-Fei, Li},
  journal={arXiv preprint arXiv:2007.00350},
  year={2020}
}


@article{zhang2021c,
  title={C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks},
  author={Zhang, Tianjun and Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2110.12080},
  year={2021}
}

@inproceedings{pitis2020maximum,
  title={Maximum entropy gain exploration for long horizon multi-goal reinforcement learning},
  author={Pitis, Silviu and Chan, Harris and Zhao, Stephen and Stadie, Bradly and Ba, Jimmy},
  booktitle={International Conference on Machine Learning},
  pages={7750--7761},
  year={2020},
  organization={PMLR}
}

@article{dennis2020emergent,
  title={Emergent complexity and zero-shot transfer via unsupervised environment design},
  author={Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre and Russell, Stuart and Critch, Andrew and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13049--13061},
  year={2020}
}

@article{matiisen2019teacher,
  title={Teacher--student curriculum learning},
  author={Matiisen, Tambet and Oliver, Avital and Cohen, Taco and Schulman, John},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={9},
  pages={3732--3740},
  year={2019},
  publisher={IEEE}
}


@article{warde2018unsupervised,
  title={Unsupervised control through non-parametric discriminative rewards},
  author={Warde-Farley, David and Van de Wiele, Tom and Kulkarni, Tejas and Ionescu, Catalin and Hansen, Steven and Mnih, Volodymyr},
  journal={arXiv preprint arXiv:1811.11359},
  year={2018}
}

@inproceedings{florensa2017reverse,
  title={Reverse curriculum generation for reinforcement learning},
  author={Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter},
  booktitle={Conference on robot learning},
  pages={482--495},
  year={2017},
  organization={PMLR}
}

@inproceedings{
fan2018learning,
title={Learning to Teach},
author={Yang Fan and Fei Tian and Tao Qin and Xiang-Yang Li and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HJewuJWCZ},
}

@article{wu2018learning,
  title={Learning to teach with dynamic loss functions},
  author={Wu, Lijun and Tian, Fei and Xia, Yingce and Fan, Yang and Qin, Tao and Jian-Huang, Lai and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{weinshall2018curriculum,
  title={Curriculum learning by transfer learning: Theory and experiments with deep networks},
  author={Weinshall, Daphna and Cohen, Gad and Amir, Dan},
  booktitle={International Conference on Machine Learning},
  pages={5238--5246},
  year={2018},
  organization={PMLR}
}

@inproceedings{such2020generative,
  title={Generative teaching networks: Accelerating neural architecture search by learning to generate synthetic training data},
  author={Such, Felipe Petroski and Rawal, Aditya and Lehman, Joel and Stanley, Kenneth and Clune, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={9206--9216},
  year={2020},
  organization={PMLR}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@inproceedings{ivanovic2019barc,
  title={Barc: Backward reachability curriculum for robotic reinforcement learning},
  author={Ivanovic, Boris and Harrison, James and Sharma, Apoorva and Chen, Mo and Pavone, Marco},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={15--21},
  year={2019},
  organization={IEEE}
}

@article{kim2018screenernet,
  title={Screenernet: Learning self-paced curriculum for deep neural networks},
  author={Kim, Tae-Hoon and Choi, Jonghyun},
  journal={arXiv preprint arXiv:1801.00904},
  year={2018}
}

@inproceedings{omidshafiei2019learning,
  title={Learning to teach in cooperative multiagent reinforcement learning},
  author={Omidshafiei, Shayegan and Kim, Dong-Ki and Liu, Miao and Tesauro, Gerald and Riemer, Matthew and Amato, Christopher and Campbell, Murray and How, Jonathan P},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={6128--6136},
  year={2019}
}

@inproceedings{jiang2018mentornet,
  title={Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International Conference on Machine Learning},
  pages={2304--2313},
  year={2018},
  organization={PMLR}
}

@article{ghosh2018learning,
  title={Learning actionable representations with goal-conditioned policies},
  author={Ghosh, Dibya and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:1811.07819},
  year={2018}
}

@article{sukhbaatar2018learning,
  title={Learning goal embeddings via self-play for hierarchical reinforcement learning},
  author={Sukhbaatar, Sainbayar and Denton, Emily and Szlam, Arthur and Fergus, Rob},
  journal={arXiv preprint arXiv:1811.09083},
  year={2018}
}

@inproceedings{
ghosh2021learning,
title={Learning to Reach Goals via Iterated Supervised Learning},
author={Dibya Ghosh and Abhishek Gupta and Ashwin Reddy and Justin Fu and Coline Manon Devin and Benjamin Eysenbach and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=rALA0Xo6yNJ}
}

@article{sukhbaatar2017intrinsic,
  title={Intrinsic motivation and automatic curricula via asymmetric self-play},
  author={Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
  journal={arXiv preprint arXiv:1703.05407},
  year={2017}
}

@inproceedings{florensa2018automatic,
  title={Automatic goal generation for reinforcement learning agents},
  author={Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={1515--1528},
  year={2018},
  organization={PMLR}
}

@article{openai2021asymmetric,
  title={Asymmetric self-play for automatic goal discovery in robotic manipulation},
  author={OpenAI, OpenAI and Plappert, Matthias and Sampedro, Raul and Xu, Tao and Akkaya, Ilge and Kosaraju, Vineet and Welinder, Peter and D'Sa, Ruben and Petron, Arthur and Pinto, Henrique P d O and others},
  journal={arXiv preprint arXiv:2101.04882},
  year={2021}
}

@article{ecoffet2019go,
  title={Go-explore: a new approach for hard-exploration problems},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1901.10995},
  year={2019}
}

@article{hartikainen2019dynamical,
  title={Dynamical distance learning for semi-supervised and unsupervised skill discovery},
  author={Hartikainen, Kristian and Geng, Xinyang and Haarnoja, Tuomas and Levine, Sergey},
  journal={arXiv preprint arXiv:1907.08225},
  year={2019}
}

@article{ren2019exploration,
  title={Exploration via hindsight goal generation},
  author={Ren, Zhizhou and Dong, Kefan and Zhou, Yuan and Liu, Qiang and Peng, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{klink2020self,
  title={Self-paced deep reinforcement learning},
  author={Klink, Pascal and D'Eramo, Carlo and Peters, Jan R and Pajarinen, Joni},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9216--9227},
  year={2020}
}

@inproceedings{nair2022learning,
  title={Learning language-conditioned robot behavior from offline data and crowd-sourced annotation},
  author={Nair, Suraj and Mitchell, Eric and Chen, Kevin and Savarese, Silvio and Finn, Chelsea and others},
  booktitle={Conference on Robot Learning},
  pages={1303--1315},
  year={2022},
  organization={PMLR}
}

@article{zhang2020automatic,
  title={Automatic curriculum learning through value disagreement},
  author={Zhang, Yunzhi and Abbeel, Pieter and Pinto, Lerrel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7648--7659},
  year={2020}
}

@InProceedings{pmlr-v37-schaul15,
  title = 	 {Universal Value Function Approximators},
  author = 	 {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1312--1320},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schaul15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schaul15.html},
  abstract = 	 {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}
}


@article{kalashnikov2021mt,
  title={Mt-opt: Continuous multi-task robotic reinforcement learning at scale},
  author={Kalashnikov, Dmitry and Varley, Jacob and Chebotar, Yevgen and Swanson, Benjamin and Jonschkowski, Rico and Finn, Chelsea and Levine, Sergey and Hausman, Karol},
  journal={arXiv preprint arXiv:2104.08212},
  year={2021}
}

@inproceedings{yu2020meta,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on Robot Learning},
  pages={1094--1100},
  year={2020},
  organization={PMLR}
}

@inproceedings{nair2020contextual,
  title={Contextual imagined goals for self-supervised robotic learning},
  author={Nair, Ashvin and Bahl, Shikhar and Khazatsky, Alexander and Pong, Vitchyr and Berseth, Glen and Levine, Sergey},
  booktitle={Conference on Robot Learning},
  pages={530--539},
  year={2020},
  organization={PMLR}
}

@book{allgower2012numerical,
  title={Numerical continuation methods: an introduction},
  author={Allgower, Eugene L and Georg, Kurt},
  volume={13},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

@inproceedings{li2021mural,
  title={MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning},
  author={Li, Kevin and Gupta, Abhishek and Reddy, Ashwin and Pong, Vitchyr H and Zhou, Aurick and Yu, Justin and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={6346--6356},
  year={2021},
  organization={PMLR}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{zhai2022computational,
  title={Computational Benefits of Intermediate Rewards for Goal-Reaching Policy Learning},
  author={Zhai, Yuexiang and Baek, Christina and Zhou, Zhengyuan and Jiao, Jiantao and Ma, Yi},
  journal={Journal of Artificial Intelligence Research},
  volume={73},
  pages={847--896},
  year={2022}
}

@article{janner2021offline,
  title={Offline Reinforcement Learning as One Big Sequence Modeling Problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}

@inproceedings{sodhani2021multi,
  title={Multi-task reinforcement learning with context-based representations},
  author={Sodhani, Shagun and Zhang, Amy and Pineau, Joelle},
  booktitle={International Conference on Machine Learning},
  pages={9767--9779},
  year={2021},
  organization={PMLR}
}

@inproceedings{mei2020global,
  title={On the global convergence rates of softmax policy gradient methods},
  author={Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={6820--6829},
  year={2020},
  organization={PMLR}
}


@article{nachum2017bridging,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{rechtoptimization,
  title={Optimization for Modern Data Analysis. 2019},
  author={Recht, Benjamin and Wright, Stephen J},
  journal={Preprint available at http://eecs. berkeley. edu/brecht/opt4mlbook}
}


@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}

@book{haarnoja2018acquiring,
  title={Acquiring diverse robot skills via maximum entropy deep reinforcement learning},
  author={Haarnoja, Tuomas},
  year={2018},
  publisher={University of California, Berkeley}
}

@article{ding2021beyond,
  title={Beyond Exact Gradients: Convergence of Stochastic Soft-Max Policy Gradient Methods with Entropy Regularization},
  author={Ding, Yuhao and Zhang, Junzi and Lavaei, Javad},
  journal={arXiv preprint arXiv:2110.10117},
  year={2021}
}

@article{agarwal2019reinforcement,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  year={2019}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{kumar2010self,
  title={Self-paced learning for latent variable models},
  author={Kumar, M and Packer, Benjamin and Koller, Daphne},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@inproceedings{murali2018cassl,
  title={Cassl: Curriculum accelerated self-supervised learning},
  author={Murali, Adithyavairavan and Pinto, Lerrel and Gandhi, Dhiraj and Gupta, Abhinav},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={6453--6460},
  year={2018},
  organization={IEEE}
}

@article{borsa2016learning,
  title={Learning shared representations in multi-task reinforcement learning},
  author={Borsa, Diana and Graepel, Thore and Shawe-Taylor, John},
  journal={arXiv preprint arXiv:1603.02041},
  year={2016}
}

@article{calandriello2014sparse,
  title={Sparse multi-task reinforcement learning},
  author={Calandriello, Daniele and Lazaric, Alessandro and Restelli, Marcello},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{maurer2016benefit,
  title={The benefit of multitask representation learning},
  author={Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={81},
  pages={1--32},
  year={2016}
}

@article{wu2019comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S},
  journal={arXiv preprint arXiv:1901.00596},
  year={2019}
}

@article{selsam2018learning,
  title={Learning a SAT solver from single-bit supervision},
  author={Selsam, Daniel and Lamm, Matthew and B{\"u}nz, Benedikt and Liang, Percy and de Moura, Leonardo and Dill, David L},
  journal={arXiv preprint arXiv:1802.03685},
  year={2018}
}

@article{davis1962machine,
  title={A machine program for theorem-proving},
  author={Davis, Martin and Logemann, George and Loveland, Donald},
  journal={Communications of the ACM},
  volume={5},
  number={7},
  pages={394--397},
  year={1962},
  publisher={ACM New York, NY, USA}
}

@article{bansal2019holist,
  title={HOList: An environment for machine learning of higher-order theorem proving (extended version)},
  author={Bansal, Kshitij and Loos, Sarah M and Rabe, Markus N and Szegedy, Christian and Wilcox, Stewart},
  journal={arXiv preprint arXiv:1904.03241},
  year={2019}
}


@article{wu2018understanding,
  title={Understanding short-horizon bias in stochastic meta-optimization},
  author={Wu, Yuhuai and Ren, Mengye and Liao, Renjie and Grosse, Roger},
  journal={arXiv preprint arXiv:1803.02021},
  year={2018}
}

@article{huang2018gamepad,
  title={Gamepad: A learning environment for theorem proving},
  author={Huang, Daniel and Dhariwal, Prafulla and Song, Dawn and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1806.00608},
  year={2018}
}

@article{paliwal2019graph,
  title={Graph Representations for Higher-Order Logic and Theorem Proving},
  author={Paliwal, Aditya and Loos, Sarah and Rabe, Markus and Bansal, Kshitij and Szegedy, Christian},
  journal={arXiv preprint arXiv:1905.10006},
  year={2019}
}

@inproceedings{wang2017premise,
  title={Premise selection for theorem proving by deep graph embedding},
  author={Wang, Mingzhe and Tang, Yihe and Wang, Jian and Deng, Jia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2786--2796},
  year={2017}
}

@inproceedings{schulz2013system,
  title={System description: E 1.8},
  author={Schulz, Stephan},
  booktitle={International Conference on Logic for Programming Artificial Intelligence and Reasoning},
  pages={735--743},
  year={2013},
  organization={Springer}
}

@inproceedings{selman1992new,
  title={A New Method for Solving Hard Satisfiability Problems.},
  author={Selman, Bart and Levesque, Hector J and Mitchell, David G and others},
  organization={Citeseer}
}

@inproceedings{nudelman2004understanding,
  title={Understanding random SAT: Beyond the clauses-to-variables ratio},
  author={Nudelman, Eugene and Leyton-Brown, Kevin and Hoos, Holger H and Devkar, Alex and Shoham, Yoav},
  booktitle={International Conference on Principles and Practice of Constraint Programming},
  pages={438--452},
  year={2004},
  organization={Springer}
}


@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  volume={48},
  number={1},
  pages={71--99},
  year={1993},
  publisher={Elsevier}
}

@inproceedings{ellis2018learning,
  title={Learning libraries of subroutines for neurally--guided bayesian program induction},
  author={Ellis, Kevin and Morales, Lucas and Sabl{\'e}-Meyer, Mathias and Solar-Lezama, Armando and Tenenbaum, Josh},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7805--7815},
  year={2018}
}


@article{silver2019few,
  title={Few-Shot Bayesian Imitation Learning with Logic over Programs},
  author={Silver, Tom and Allen, Kelsey R and Lew, Alex K and Kaelbling, Leslie Pack and Tenenbaum, Josh},
  journal={arXiv preprint arXiv:1904.06317},
  year={2019}
}

@article{cropper2019playgol,
  title={Playgol: learning programs through play},
  author={Cropper, Andrew},
  journal={arXiv preprint arXiv:1904.08993},
  year={2019}
}

@inproceedings{ritchie2016neurally,
  title={Neurally-guided procedural models: Amortized inference for procedural graphics programs using neural networks},
  author={Ritchie, Daniel and Thomas, Anna and Hanrahan, Pat and Goodman, Noah},
  booktitle={Advances in neural information processing systems},
  pages={622--630},
  year={2016}
}

@article{shin2019program,
  title={Program Synthesis and Semantic Parsing with Learned Code Idioms},
  author={Shin, Richard and Allamanis, Miltiadis and Brockschmidt, Marc and Polozov, Oleksandr},
  journal={arXiv preprint arXiv:1906.10816},
  year={2019}
}


@article{yang2019learning,
  title={Learning to Prove Theorems via Interacting with Proof Assistants},
  author={Yang, Kaiyu and Deng, Jia},
  journal={arXiv preprint arXiv:1905.09381},
  year={2019}
}

@inproceedings{
anonymous2020towards,
title={Towards Finding Longer Proofs},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hkeh21BKPH},
note={under review}
}

@article{nachum2018near,
  title={Near-optimal representation learning for hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.01257},
  year={2018}
}

@inproceedings{
anonymous2020learning,
title={Learning to Prove Theorems by Learning to Generate Theorems},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJxiqxSYPB},
note={under review}
}

@inproceedings{
anonymous2020adaptive,
title={{\{}ADAPTIVE{\}} {\{}GENERATION{\}} {\{}OF{\}} {\{}PROGRAMMING{\}} {\{}PUZZLES{\}}},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJeRveHKDH},
note={under review}
}

@inproceedings{
anonymous2020what,
title={What Can Neural Networks Reason About?},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJxbJeHFPS},
note={under review}
}

@inproceedings{
anonymous2020synthesizing,
title={Synthesizing Programmatic Policies that Inductively Generalize},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1l8oANFDH},
note={under review}
}

@inproceedings{
anonymous2020neuralguided,
title={Neural-Guided Symbolic Regression with Asymptotic Constraints},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1gTAp4FDB},
note={under review}
}

@article{chang2018automatically,
  title={Automatically composing representation transformations as a means for generalization},
  author={Chang, Michael B and Gupta, Abhishek and Levine, Sergey and Griffiths, Thomas L},
  journal={arXiv preprint arXiv:1807.04640},
  year={2018}
}


@incollection{NIPS2019_9241,
title = {G2SAT: Learning to Generate SAT Formulas},
author = {You, Jiaxuan and Wu, Haoze and Barrett, Clark and Ramanujan, Raghuram and Leskovec, Jure},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. dAlch\'{e}-Buc and E. Fox and R. Garnett},
pages = {10552--10563},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9241-g2sat-learning-to-generate-sat-formulas.pdf}
}

@inproceedings{burnett2019building,
  title={Building a winning self-driving car in six months},
  author={Burnett, Keenan and Schimpe, Andreas and Samavi, Sepehr and Gridseth, Mona and Liu, Chengzhi Winston and Li, Qiyang and Kroeze, Zachary and Schoellig, Angela P},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={9583--9589},
  year={2019},
  organization={IEEE}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle={Advances in neural information processing systems},
  pages={6402--6413},
  year={2017}
}

@article{mohamed2019monte,
  title={Monte carlo gradient estimation in machine learning},
  author={Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  journal={arXiv preprint arXiv:1906.10652},
  year={2019}
}

@inproceedings{een2003extensible,
    title={An extensible SAT-solver},
    author={E{\'e}n, Niklas and S{\"o}rensson, Niklas},
    booktitle={International conference on theory and applications of satisfiability testing},
    pages={502--518},
    year={2003},
    organization={Springer}
}
@article{selman1996generating,
  title={Generating hard satisfiability problems},
  author={Selman, Bart and Mitchell, David G and Levesque, Hector J},
  journal={Artificial intelligence},
  volume={81},
  number={1-2},
  pages={17--29},
  year={1996},
  publisher={Elsevier}
}

@article{cheeseman1991really,
  title={Where really hard problems are},
  author={Cheeseman, Peter and Kanefsky, Bob and Taylor, William M.},
  journal={Proc. 12th IJCAI, 1991},
  year={1991}
}
@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1902.06720},
  year={2019}
}


@inproceedings{kingma2015variational,
  title={Variational dropout and the local reparameterization trick},
  author={Kingma, Durk P and Salimans, Tim and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2575--2583},
  year={2015}
}

@article{wen2018flipout,
  title={Flipout: Efficient pseudo-independent weight perturbations on mini-batches},
  author={Wen, Yeming and Vicol, Paul and Ba, Jimmy and Tran, Dustin and Grosse, Roger},
  journal={arXiv preprint arXiv:1803.04386},
  year={2018}
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}

@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1471--1479},
  year={2016}
}

@inproceedings{cohn1995active,
  title={Active learning with statistical models},
  author={Cohn, David A and Ghahramani, Zoubin and Jordan, Michael I},
  booktitle={Advances in neural information processing systems},
  pages={705--712},
  year={1995}
}

@article{mackay1992information,
  title={Information-based objective functions for active data selection},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={4},
  pages={590--604},
  year={1992},
  publisher={MIT Press}
}


@article{pierrot2019learning,
  title={Learning Compositional Neural Programs with Recursive Tree Search and Planning},
  author={Pierrot, Thomas and Ligner, Guillaume and Reed, Scott and Sigaud, Olivier and Perrin, Nicolas and Laterre, Alexandre and Kas, David and Beguir, Karim and de Freitas, Nando},
  journal={arXiv preprint arXiv:1905.12941},
  year={2019}
}

@incollection{NIPS2019_8724,
title = {Adaptive Auxiliary Task Weighting for Reinforcement Learning},
author = {Lin, Xingyu and Baweja, Harjatin and Kantor, George and Held, David},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. dAlch\'{e}-Buc and E. Fox and R. Garnett},
pages = {4773--4784},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8724-adaptive-auxiliary-task-weighting-for-reinforcement-learning.pdf}
}


@inproceedings{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  booktitle={Advances in neural information processing systems},
  pages={2348--2356},
  year={2011}
}

@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}

@article{rissanen1978modeling,
  title={Modeling by shortest data description},
  author={Rissanen, Jorma},
  journal={Automatica},
  volume={14},
  number={5},
  pages={465--471},
  year={1978},
  publisher={Pergamon}
}

@inproceedings{you2019g2sat,
  title={G2SAT: Learning to Generate SAT Formulas},
  author={You, Jiaxuan and Wu, Haoze and Barrett, Clark and Ramanujan, Raghuram and Leskovec, Jure},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10552--10563},
  year={2019}
}


@inproceedings{spitkovsky2010baby,
  title={From baby steps to leapfrog: How less is more in unsupervised dependency parsing},
  author={Spitkovsky, Valentin I and Alshawi, Hiyan and Jurafsky, Daniel},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={751--759},
  year={2010},
  organization={Association for Computational Linguistics}
}

@article{cai2017making,
  title={Making neural programming architectures generalize via recursion},
  author={Cai, Jonathon and Shin, Richard and Song, Dawn},
  journal={arXiv preprint arXiv:1704.06611},
  year={2017}
}

@article{reed2015neural,
  title={Neural programmer-interpreters},
  author={Reed, Scott and De Freitas, Nando},
  journal={arXiv preprint arXiv:1511.06279},
  year={2015}
}

@misc{jaxrl,
  author = {Kostrikov, Ilya},
  doi = {10.5281/zenodo.5535154},
  month = {10},
  title = {{JAXRL: Implementations of Reinforcement Learning algorithms in JAX}},
  url = {https://github.com/ikostrikov/jaxrl},
  year = {2021}
}


@inproceedings{houthooft2016vime,
  title={Vime: Variational information maximizing exploration},
  author={Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1109--1117},
  year={2016}
}

@inproceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6389--6399},
  year={2018}
}

@article{wu2020curricula,
  title={When do curricula work?},
  author={Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2012.03107},
  year={2020}
}

@article{cobbe2019procgen,
  title={Leveraging Procedural Generation to Benchmark Reinforcement Learning},
  author={Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
  journal={arXiv preprint arXiv:1912.01588},
  year={2019}
}

@article{narvekar2020curriculum,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={arXiv preprint arXiv:2003.04960},
  year={2020}
}


@inproceedings{tanaka2003multitask,
  title={Multitask reinforcement learning on the distribution of MDPs},
  author={Tanaka, Fumihide and Yamamura, Masayuki},
  booktitle={Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No. 03EX694)},
  volume={3},
  pages={1108--1113},
  year={2003},
  organization={IEEE}
}

@article{rusu2015policy,
  title={Policy distillation},
  author={Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  journal={arXiv preprint arXiv:1511.06295},
  year={2015}
}

@article{rajeswaran2016epopt,
  title={Epopt: Learning robust neural network policies using model ensembles},
  author={Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.01283},
  year={2016}
}

@inproceedings{
D'Eramo2020Sharing,
title={Sharing Knowledge in Multi-Task Deep Reinforcement Learning},
author={Carlo D'Eramo and Davide Tateo and Andrea Bonarini and Marcello Restelli and Jan Peters},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgpv2VFvr}
}

@article{yu2020gradient,
  title={Gradient surgery for multi-task learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5824--5836},
  year={2020}
}

@article{bhandari2019global,
  title={Global optimality guarantees for policy gradient methods},
  author={Bhandari, Jalaj and Russo, Daniel},
  journal={arXiv preprint arXiv:1906.01786},
  year={2019}
}

@article{cen2021fast,
  title={Fast global convergence of natural policy gradient methods with entropy regularization},
  author={Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
  journal={Operations Research},
  year={2021},
  publisher={INFORMS}
}

@inproceedings{fazel2018global,
  title={Global convergence of policy gradient methods for the linear quadratic regulator},
  author={Fazel, Maryam and Ge, Rong and Kakade, Sham and Mesbahi, Mehran},
  booktitle={International Conference on Machine Learning},
  pages={1467--1476},
  year={2018},
  organization={PMLR}
}

@inproceedings{li2021softmax,
  title={Softmax policy gradient methods can take exponential time to converge},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  booktitle={Conference on Learning Theory},
  pages={3107--3110},
  year={2021},
  organization={PMLR}
}

@article{zhang2020global,
  title={Global convergence of policy gradient methods to (almost) locally optimal policies},
  author={Zhang, Kaiqing and Koppel, Alec and Zhu, Hao and Basar, Tamer},
  journal={SIAM Journal on Control and Optimization},
  volume={58},
  number={6},
  pages={3586--3612},
  year={2020},
  publisher={SIAM}
}

@article{zhang2020sample,
  title={Sample efficient reinforcement learning with REINFORCE},
  author={Zhang, Junzi and Kim, Jongho and O’Donoghue, Brendan and Boyd, Stephen},
  journal={arXiv preprint arXiv:2010.11364},
  pages={97},
  year={2020}
}

@article{andrychowicz2020learning,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{tang2017exploration,
  title={\# Exploration: A study of count-based exploration for deep reinforcement learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, OpenAI Xi and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2753--2762},
  year={2017}
}

@article{stadie2015incentivizing,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1507.00814},
  year={2015}
}

%% Contextual MDPs

@article{belogolovsky2021inverse,
  title={Inverse reinforcement learning in contextual MDPs},
  author={Belogolovsky, Stav and Korsunsky, Philip and Mannor, Shie and Tessler, Chen and Zahavy, Tom},
  journal={Machine Learning},
  volume={110},
  number={9},
  pages={2295--2334},
  year={2021},
  publisher={Springer}
}

@article{hallak2015contextual,
  title={Contextual markov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}

@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low bellman rank are pac-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@inproceedings{modi2018markov,
  title={Markov decision processes with continuous side information},
  author={Modi, Aditya and Jiang, Nan and Singh, Satinder and Tewari, Ambuj},
  booktitle={Algorithmic Learning Theory},
  pages={597--618},
  year={2018},
  organization={PMLR}
}

@article{abbasi2014online,
  title={Online learning in MDPs with side information},
  author={Abbasi-Yadkori, Yasin and Neu, Gergely},
  journal={arXiv preprint arXiv:1406.6812},
  year={2014}
}

@inproceedings{modi2020sample,
  title={Sample complexity of reinforcement learning using linearly combined model ensembles},
  author={Modi, Aditya and Jiang, Nan and Tewari, Ambuj and Singh, Satinder},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2010--2020},
  year={2020},
  organization={PMLR}
}

@inproceedings{misra2020kinematic,
  title={Kinematic state abstraction and provably efficient rich-observation reinforcement learning},
  author={Misra, Dipendra and Henaff, Mikael and Krishnamurthy, Akshay and Langford, John},
  booktitle={International conference on machine learning},
  pages={6961--6971},
  year={2020},
  organization={PMLR}
}

@inproceedings{sun2019model,
  title={Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches},
  author={Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Conference on learning theory},
  pages={2898--2933},
  year={2019},
  organization={PMLR}
}


@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{peters2008natural,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008},
  publisher={Elsevier}
}

@article{konda1999actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay and Tsitsiklis, John},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{kulkarni2016deep,
  title={Deep successor reinforcement learning},
  author={Kulkarni, Tejas D and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J},
  journal={arXiv preprint arXiv:1606.02396},
  year={2016}
}

@article{siriwardhana2019vusfa,
  title={Vusfa: Variational universal successor features approximator to improve transfer drl for target driven visual navigation},
  author={Siriwardhana, Shamane and Weerasakera, Rivindu and Matthies, Denys JC and Nanayakkara, Suranga},
  journal={arXiv preprint arXiv:1908.06376},
  year={2019}
}


@inproceedings{yuan2022general,
  title={A general sample complexity analysis of vanilla policy gradient},
  author={Yuan, Rui and Gower, Robert M and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3332--3380},
  year={2022},
  organization={PMLR}
}

@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{zhang2020almost,
  title={Almost optimal model-free reinforcement learningvia reference-advantage decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15198--15207},
  year={2020}
}

@article{li2021breaking,
  title={Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{agarwal2020pc,
  title={Pc-pg: Policy cover directed exploration for provable policy gradient learning},
  author={Agarwal, Alekh and Henaff, Mikael and Kakade, Sham and Sun, Wen},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13399--13412},
  year={2020}
}

@book{aastrom2013adaptive,
  title={Adaptive control},
  author={{\AA}str{\"o}m, Karl J and Wittenmark, Bj{\"o}rn},
  year={2013},
  publisher={Courier Corporation}
}

@book{landau2011adaptive,
  title={Adaptive control: algorithms, analysis and applications},
  author={Landau, Ioan Dor{\'e} and Lozano, Rogelio and M'Saad, Mohammed and Karimi, Alireza},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@book{tao2003adaptive,
  title={Adaptive control design and analysis},
  author={Tao, Gang},
  volume={37},
  year={2003},
  publisher={John Wiley \& Sons}
}

@book{goodwin2014adaptive,
  title={Adaptive filtering prediction and control},
  author={Goodwin, Graham C and Sin, Kwai Sang},
  year={2014},
  publisher={Courier Corporation}
}

@misc{sastry1990adaptive,
  title={Adaptive control: stability, convergence, and robustness},
  author={Sastry, Shankar and Bodson, Marc and Bartram, James F},
  year={1990},
  publisher={Acoustical Society of America}
}

@inproceedings{moskovitz2022towards,
  title={Towards an Understanding of Default Policies in Multitask Policy Optimization},
  author={Moskovitz, Ted and Arbel, Michael and Parker-Holder, Jack and Pacchiano, Aldo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={10661--10686},
  year={2022},
  organization={PMLR}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@inproceedings{du2019provably,
  title={Provably efficient RL with rich observations via latent state decoding},
  author={Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
  booktitle={International Conference on Machine Learning},
  pages={1665--1674},
  year={2019},
  organization={PMLR}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ international conference on intelligent robots and systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{gupta2022unpacking,
  title={Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity},
  author={Gupta, Abhishek and Pacchiano, Aldo and Zhai, Yuexiang and Kakade, Sham M and Levine, Sergey},
  journal={arXiv preprint arXiv:2210.09579},
  year={2022}
}

@article{liu2022revolver,
  title={REvolveR: Continuous Evolutionary Models for Robot-to-robot Policy Transfer},
  author={Liu, Xingyu and Pathak, Deepak and Kitani, Kris M},
  journal={arXiv preprint arXiv:2202.05244},
  year={2022}
}

@article{bassich2020curriculum,
  title={Curriculum learning with a progression function},
  author={Bassich, Andrea and Foglino, Francesco and Leonetti, Matteo and Kudenko, Daniel},
  journal={arXiv preprint arXiv:2008.00511},
  year={2020}
}

@inproceedings{clavera2018model,
  title={Model-based reinforcement learning via meta-policy optimization},
  author={Clavera, Ignasi and Rothfuss, Jonas and Schulman, John and Fujita, Yasuhiro and Asfour, Tamim and Abbeel, Pieter},
  booktitle={Conference on Robot Learning},
  pages={617--629},
  year={2018},
  organization={PMLR}
}

@article{agarwal2022provable,
  title={Provable Benefits of Representational Transfer in Reinforcement Learning},
  author={Agarwal, Alekh and Song, Yuda and Sun, Wen and Wang, Kaiwen and Wang, Mengdi and Zhang, Xuezhou},
  journal={arXiv preprint arXiv:2205.14571},
  year={2022}
}

@inproceedings{tian2019elf,
  title={Elf opengo: An analysis and open reimplementation of alphazero},
  author={Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, Larry},
  booktitle={International Conference on Machine Learning},
  pages={6244--6253},
  year={2019},
  organization={PMLR}
}

@article{li2022understanding,
  title={Understanding the Complexity Gains of Single-Task RL with a Curriculum},
  author={Li, Qiyang and Zhai, Yuexiang and Ma, Yi and Levine, Sergey},
  journal={arXiv preprint arXiv:2212.12809},
  year={2022}
}

@inproceedings{ostrovski2017count,
  title={Count-based exploration with neural density models},
  author={Ostrovski, Georg and Bellemare, Marc G and Oord, A{\"a}ron and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={2721--2730},
  year={2017},
  organization={PMLR}
}

@inproceedings{guo2020batch,
  title={Batch reinforcement learning through continuation method},
  author={Guo, Yijie and Feng, Shengyu and Le Roux, Nicolas and Chi, Ed and Lee, Honglak and Chen, Minmin},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{buckman2020importance,
  title={The importance of pessimism in fixed-dataset policy optimization},
  author={Buckman, Jacob and Gelada, Carles and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2009.06799},
  year={2020}
}

@article{lyu2022mildly,
  title={Mildly conservative Q-learning for offline reinforcement learning},
  author={Lyu, Jiafei and Ma, Xiaoteng and Li, Xiu and Lu, Zongqing},
  journal={arXiv preprint arXiv:2206.04745},
  year={2022}
}

@article{beeson2022improving,
  title={Improving TD3-BC: Relaxed Policy Constraint for Offline Learning and Stable Online Fine-Tuning},
  author={Beeson, Alex and Montana, Giovanni},
  journal={arXiv preprint arXiv:2211.11802},
  year={2022}
}

@inproceedings{mark2022fine,
  title={Fine-tuning Offline Policies with Optimistic Action Selection},
  author={Mark, Max Sobol and Ghadirzadeh, Ali and Chen, Xi and Finn, Chelsea},
  booktitle={Deep Reinforcement Learning Workshop NeurIPS 2022},
  year={2022}
}

@inproceedings{lee2022offline,
  title={Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble},
  author={Lee, Seunghyun and Seo, Younggyo and Lee, Kimin and Abbeel, Pieter and Shin, Jinwoo},
  booktitle={Conference on Robot Learning},
  pages={1702--1712},
  year={2022},
  organization={PMLR}
}

@article{wu2022supported,
  title={Supported Policy Optimization for Offline Reinforcement Learning},
  author={Wu, Jialong and Wu, Haixu and Qiu, Zihan and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2202.06239},
  year={2022}
}

@article{xie2021policy,
  title={Policy finetuning: Bridging sample-efficient offline and online reinforcement learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={27395--27407},
  year={2021}
}

@article{wagenmaker2022leveraging,
  title={Leveraging Offline Data in Online Reinforcement Learning},
  author={Wagenmaker, Andrew and Pacchiano, Aldo},
  journal={arXiv preprint arXiv:2211.04974},
  year={2022}
}

@inproceedings{
song2023hybrid,
title={Hybrid {RL}: Using both offline and online data can make {RL} efficient},
author={Yuda Song and Yifei Zhou and Ayush Sekhari and Drew Bagnell and Akshay Krishnamurthy and Wen Sun},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=yyBis80iUuU}
}

@inproceedings{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon and Kakade, Sham and Lee, Jason and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={2826--2836},
  year={2021},
  organization={PMLR}
}

@inproceedings{xie2020q,
  title={Q* approximation schemes for batch reinforcement learning: A theoretical comparison},
  author={Xie, Tengyang and Jiang, Nan},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={550--559},
  year={2020},
  organization={PMLR}
}

@article{zhao2022adaptive,
  title={Adaptive behavior cloning regularization for stable offline-to-online reinforcement learning},
  author={Zhao, Yi and Boney, Rinu and Ilin, Alexander and Kannala, Juho and Pajarinen, Joni},
  journal={arXiv preprint arXiv:2210.13846},
  year={2022}
}

@article{zanette2021provable,
  title={Provable benefits of actor-critic methods for offline reinforcement learning},
  author={Zanette, Andrea and Wainwright, Martin J and Brunskill, Emma},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13626--13640},
  year={2021}
}

@article{geng2022jaxcql,
  title={JaxCQL: a simple implementation of SAC and CQL in JAX},
  author={Xinyang Geng},
  year={2022},
  url={https://github.com/young-geng/JaxCQL}
}


@article{jin2021bellman,
  title={Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13406--13418},
  year={2021}
}

@article{rajeswaran2017learning,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.10087},
  year={2017}
}

@inproceedings{zhu2019dexterous,
  title={Dexterous manipulation with deep reinforcement learning: Efficient, general, and low-cost},
  author={Zhu, Henry and Gupta, Abhishek and Rajeswaran, Aravind and Levine, Sergey and Kumar, Vikash},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={3651--3657},
  year={2019},
  organization={IEEE}
}

@article{zhu2018reinforcement,
  title={Reinforcement and imitation learning for diverse visuomotor skills},
  author={Zhu, Yuke and Wang, Ziyu and Merel, Josh and Rusu, Andrei and Erez, Tom and Cabi, Serkan and Tunyasuvunakool, Saran and Kram{\'a}r, J{\'a}nos and Hadsell, Raia and de Freitas, Nando and others},
  journal={arXiv preprint arXiv:1802.09564},
  year={2018}
}

@article{peters2008reinforcement,
  title={Reinforcement learning of motor skills with policy gradients},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neural networks},
  volume={21},
  number={4},
  pages={682--697},
  year={2008},
  publisher={Elsevier}
}

@article{schaal1996learning,
  title={Learning from demonstration},
  author={Schaal, Stefan},
  journal={Advances in neural information processing systems},
  volume={9},
  year={1996}
}

@inproceedings{kang2018policy,
  title={Policy optimization with demonstrations},
  author={Kang, Bingyi and Jie, Zequn and Feng, Jiashi},
  booktitle={International conference on machine learning},
  pages={2469--2478},
  year={2018},
  organization={PMLR}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{
redq,
title={Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},
author={Xinyue Chen and Che Wang and Zijian Zhou and Keith W. Ross},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=AY8zfZm0tDd}
}

@inproceedings{
replaybarrier,
title={Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier},
author={Pierluca D'Oro and Max Schwarzer and Evgenii Nikishin and Pierre-Luc Bacon and Marc G Bellemare and Aaron Courville},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=OpC-9aBBVJe}
}

@inproceedings{
droq,
title={Dropout Q-Functions for Doubly Efficient Reinforcement Learning},
author={Takuya Hiraoka and Takahisa Imagawa and Taisei Hashimoto and Takashi Onishi and Yoshimasa Tsuruoka},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=xCVJMsPv3RT}
}

@article{rlpd,
  title={Efficient Online Reinforcement Learning with Offline Data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2302.02948},
  year={2023}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{luo2023finetuning,
  title={Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions},
  author={Luo, Yicheng and Kay, Jackie and Grefenstette, Edward and Deisenroth, Marc Peter},
  journal={arXiv preprint arXiv:2303.17396},
  year={2023}
}