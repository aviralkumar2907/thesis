\vspace{-0.15cm}
\section{Theoretical Analysis of \methodname}
\label{sec:theory}
\vspace{-0.25cm}

We will now analyze the cumulative regret attained over online fine-tuning, when the value function is pre-trained with Cal-QL, and show that enforcing calibration (Defintion~\ref{cond:calibration}) leads to a favorable regret bound during the online phase. Our analysis utilizes tools from \citet{song2023hybrid}, but studies the impact of calibration on fine-tuning. We also remark that we simplify the treatment of certain aspects (e.g., how to incorporate pessimism) as it allows us to cleanly demonstrate benefits of calibration.  

{\textbf{Notation \& terminology.}} In our analysis, we will consider an idealized version of \methodname\ for simplicity. Specifically, following prior work~\citep{song2023hybrid} under the bilinear model~\citep{du2021bilinear}, we will operate in a finite-horizon setting with a horizon $H$. We denote the learned Q-function at each learning iteration $k$ for a given $(\bs, \mathbf{a})$ pair and time-step $h$ by $Q_{\theta}^k(\bs, \mathbf{a})$. For any given policy $\pi$, let $C_\pi\geq1$ denote the concentrability coefficient such that $C_\pi:=\max_{f\in \mc C}\frac{\sum_{h=0}^{H-1}\bb E_{s,a\sim d_h^\pi}[\mc T f_{h+1}(s,a)-f_h(s,a)]}{\sqrt{\sum_{h=0}^{H-1}\bb E_{s,a\sim \nu_h}(\mc T f_{h+1}(s,a)-f_h(s,a))^2}}$,
i.e., a coefficient that quantifies the distribution shift between the policy $\pi$ and the dataset $\mathcal{D}$, in terms of the ratio of Bellman errors averaged under $\pi$ and the dataset $\mathcal{D}$. Note that $\mc C$ represents the Q-function class and we assume $\mc C$ has a bellman-bilinear rank~\citep{du2021bilinear} of $d$. We also use $C_\pi^\mu$ to denote the concentrability coefficient over a subset of {\em calibrated} Q-functions w.r.t. a reference policy $\mu$: $C^\mu_\pi:=\max_{f\in \mc C,f(s,a)\geq Q^\mu(s,a)}\frac{\sum_{h=0}^{H-1}\bb E_{s,a\sim d_h^\pi}[\mc T f_{h+1}(s,a)-f_h(s,a)]}{\sqrt{\sum_{h=0}^{H-1}\bb E_{s,a\sim \nu_h}(\mc T f_{h+1}(s,a)-f_h(s,a))^2}}$, which provides $C^\mu_\pi\leq C_\pi$. Similar to $\mc C$, let $d_\mu$ denote the bellman bilinear rank of $\mc C_\mu$ -- 
the calibrated Q-function class w.r.t. the reference policy $\mu$.
Intuitively, we have $\mc C_\mu\subset\mc C$, which implies that $d_\mu\leq d$. The formal definitions are provided in Appendix~\ref{appendix:notations}.
We will use $\pi^k$ to denote the arg-max policy induced by $Q^k_\theta$. 

% \vspace{-0.4cm}
% \subsection{Intuition} 
% \vspace{-0.4cm}

{\textbf{Intuition.}} We intuitively discuss how calibration and conservatism enable \methodname\ to attain a smaller regret compared to not imposing calibration. Our goal is to bound the cumulative regret of online fine-tuning, ${\sum_{k} \mathbb{E}_{\bs_0 \sim \rho}[V^{\pi^\star}(\bs_0) - V^{\pi^k}(\bs_0)]}$. We can decompose this expression into two terms: 
\vspace{-0.05cm}
\begin{align}
    \resizebox{.87\textwidth}{!}{$\regret(K) = \underbrace{\sum_{k=1}^K \mathbb{E}_{\bs_0 \sim \rho} \brac{V^{\star}(\bs_0) -\max_a {Q}_{\theta}^k(\bs_0,\mathbf{a})}}_{(i) ~:=~ \text{miscalibration}}
    +\underbrace{\sum_{k=1}^K \mathbb{E}_{\bs_0 \sim \rho} \brac{ \max_a {Q}_{\theta}^k(\bs_0,\mathbf{a}) - V^{\pi^{k}}(\bs_0)}}_{(ii) ~:=~ \text{overestimation}}$.}
\label{eq:regret-decomposition}
\end{align}
This decomposition of regret into terms (i) and (ii) is instructive. Term (ii) corresponds to the amount of over-estimation in the learned value function, which is expected to be small if a conservative RL algorithm is used for training. Term (i) is the difference between the ground-truth value of the optimal policy and the learned Q-function and is negative if the learned Q-function were calibrated against the optimal policy (per Definition~\ref{cond:calibration}). Of course, this is not always possible because we do not know $V^\star$ a priori. But note that when \methodname\ utilizes a reference policy $\mu$ with a high value $V^\mu$, close to $V^\star$, then the learned Q-function $Q_\theta$ is calibrated with respect to $Q^\mu$ per Condition~\ref{cond:calibration} and term (i) can still be controlled. Therefore, controlling this regret requires striking a balance between learning a calibrated (term (i)) and conservative (term (ii)) Q-function. We now formalize this intuition and defer the detailed proof to Appendix~\ref{appdendix:derivation-CalQL}. 

% \vspace{-0.2cm}
% \subsection{Theorem Statement}
% \vspace{-0.15cm}

\begin{theorem}[Informal regret bound of \methodname]
\label{thm:main-thm-informal}
    With high probability, \methodname{} obtains the following bound on total regret accumulated during online fine-tuning: \vspace{-0.15cm}
\begin{equation*}
    \begin{split}
        \regret(K) = \wt{O}\Big(\min\big\{C_{\pi^\star}^\mu H\sqrt{dK\log\paren{|\mc F|}},
        \;K\mathbb{E}_{\rho}[V^{\star}(\bs_0) -V^\mu(\bs_0)]+H\sqrt{d_\mu K\log\paren{|\mc F|}}\big\}\Big),
    \end{split}
\end{equation*}
where $\mc F$ is the functional class of the Q-function.
\end{theorem}

{\textbf{Comparison to~\citet{song2023hybrid}.}}
\citet{song2023hybrid} analyzes an online RL algorithm that utilizes offline data without imposing conservatism or calibration. We now compare Theorem~\ref{thm:main-thm-informal} to Theorem 1 of \citet{song2023hybrid} to understand the impact of these conditions on the final regret guarantee. Theorem 1 of \citet{song2023hybrid} presents a regret bound: $\regret(K) = \wt{O}\left( C_{\pi^\star} H \sqrt{d K \log\paren{|\mc F|}} \right)$ and we note some improvements in our guarantee, that we also verify via experiments in Section~\ref{subsec:diagonistic}: \textbf{(a)} for the setting where the reference policy $\mu$ contains near-optimal behavior, i.e., $V^\star - V^\mu \lesssim O(H\sqrt{d\log\paren{|\mc F|}/K})$, \methodname\ can enable a tighter regret guarantee compared to \citet{song2023hybrid}; \textbf{(b)} as we show in Appendix~\ref{subsec:CalQL-assumptions}, the concentrability coefficient $C^\mu_{\pi^\star}$ appearing in our guarantee is no larger than the one that appears in Theorem 1 of \citet{song2023hybrid}, providing another source of improvement; and \textbf{(c)} finally, in the case where the reference policy has broad coverage \emph{and} is highly sub-optimal, \methodname\ reverts back to the guarantee from \citep{song2023hybrid}, meaning that \methodname\ improves upon this prior work.


\vspace{-0.2cm}