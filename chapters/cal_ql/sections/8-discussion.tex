\vspace{-0.2cm}
\section{Discussion and Limitations}
\vspace{-0.2cm}

In this chapter, we developed \methodname\, a method for acquiring conservative offline initializations that facilitate fast online fine-tuning. \methodname\ learns conservative value functions that are constrained to be larger than the value function of a reference policy. This form of calibration allows us to avoid initial unlearning when fine-tuning with conservative methods, while also retaining the effective asymptotic performance that these methods exhibit. Our theoretical and experimental results highlight the benefit of \methodname\ in enabling fast online fine-tuning. 
% Specifically, \methodname\ improves over a number of prior approaches on 9/11 tasks that we study. 
While \methodname\ outperforms prior methods, we believe that we can develop even more effective methods by adjusting calibration and conservatism more carefully. A limitation of our work is that we do not consider fine-tuning setups where pre-training and fine-tuning tasks are different, but this is an interesting avenue for future work. Theoretically, our analysis can be improved via a more precise treatment of pessimism, and this is an avenue for future work.