\vspace{-0.2cm}
\section{Related Work}
\vspace{-0.3cm}
Several prior works suggest that online RL methods typically require a large number of samples~\cite{silver2016mastering,vinyals2019grandmaster,ye2020towards,kakade2002approximately,zhai2022computational,gupta2022unpacking,li2022understanding} to learn from scratch. We can utilize offline data to accelerate online RL algorithms. Prior works do this in a variety of ways: incorporating the offline data into the replay buffer of online RL~\cite{schaal1996learning,vecerik2017leveraging,hester2018deep,song2023hybrid}, utilizing auxiliary behavioral cloning losses with policy gradients~\cite{rajeswaran2017learning,kang2018policy,zhu2018reinforcement,zhu2019dexterous}, or extracting a high-level skill space for downstream online RL~\cite{gupta2019relay,ajay2020opal}. While these methods improve the sample efficiency of online RL from scratch, as we will also show in our results, they do not eliminate the need to actively roll out poor policies for data collection.

To address this issue, a different line of work first runs offline RL for learning a good policy and value initialization from the offline data, followed by online fine-tuning~\cite{nair2020accelerating,kostrikov2021offlineb,lyu2022mildly,beeson2022improving,wu2022supported,lee2022offline,mark2022fine}. These approaches typically employ offline RL methods based on policy constraints or pessimism~\cite{fujimoto2018off,siegel2020keep,guo2020batch,ghasemipour2021emaq,kostrikov2021offlineb,singh2020cog,lee2022offline} on the offline data, then continue training with the same method on a combination of offline and online data once fine-tuning begins~\cite{nachum2019algaedice,kidambi2020morel,yu2020mopo,kumar2020conservative,buckman2020importance}. Although pessimism is crucial for offline RL~\cite{jin2021pessimism,cheng2022adversarially}, using pessimism or constraints for fine-tuning~\cite{nair2020accelerating,kostrikov2021offlineb,lyu2022mildly} slows down fine-tuning or leads to initial unlearning, as we will show in Section~\ref{sec:calql_empirical_analysis}. In effect, these prior methods either fail to improve as fast as online RL or lose the initialization from offline RL. We aim to address this limitation by understanding some conditions on the offline initialization that enable fast fine-tuning. 

Our approach is most related to methods that utilize a pessimistic offline RL algorithm for offline training but incorporate exploration in fine-tuning~\cite{lee2022offline,mark2022fine,wu2022supported}. In contrast to these works, our method aims to learn a better offline initialization that enables standard online fine-tuning. Our approach fine-tunes na\"ively without ensembles~\cite{lee2022offline} or exploration~\cite{mark2022fine} and, as we show in our experiments, this alone is enough to outperform approaches that employ explicit optimism during data collection.