\documentclass[../thesis.tex]{subfiles}
\begin{document}
Consider looking at a photograph of a chair. We humans have the remarkable capacity of inferring properties about the 3D shape of the chair from this single photograph even if we might not have seen such a chair ever before. A more representative example of our experience though is being in the same physical space as the chair and accumulating information from various viewpoints around it to build up our hypothesis of the chairâ€™s 3D shape. How do we solve this complex 2D to 3D inference task? What kind of cues do we use? How do we seamlessly integrate information from just a few views to build up a holistic 3D model of the object?

A vast body of work in computer vision has been devoted to developing algorithms which leverage various cues from images that enable this task of 3D reconstruction. They range from monocular cues such as shading, linear perspective, size constancy \etc \cite{palmer1999vision} to binocular \cite{louis1960methods} and even multi-view stereopsis. The dominant paradigm for integrating multiple views has been to leverage stereopsis, i.e. if a point in the 3D world is viewed from multiple viewpoints, its location in 3D can be determined by triangulating its projections in the respective views. This family of algorithms has led to work on Structure from Motion (SfM)~\cite{17kruppa1913,longuet1981computer,TomasiKanade92} and Multi-view Stereo (MVS)~\cite{seitz2006comparison,furukawa2015multi} and have been used to produce city-scale 3D models~\cite{agarwal2011building} and enable rich visual experiences such as 3D flyover maps~\cite{kushal2012photo}.

While multi-view 3D reconstruction has focussed on scaling up to larger scenes with smarter optimization schemes and large scale engineering solutions, a complementary line of work in 3D inference has focussed on modelling 3D shapes of objects. A well studied example of such an object category is human faces. A key aspect of this problem is modeling the intraclass variation in object shape as demonstrated in the seminal work of Blanz and Vetter \cite{blanz1999morphable}. Recent advances \cite{kemelmacher2011internet,movingFace:eccv14} in 3D face modelling have resulted in real-time systems for facial animation~\cite{thies2016face} and face-based authentication. 

Perhaps in contrast to 3D reconstruction, recognition systems in computer vision (object classification, detection, segmentation \etc) have improved leaps and bounds in recent times - primarily driven by the success of powerful deep neural networks~\cite{Lecun89} in modelling highly complex data distributions. As a result, we now have ready access to cues for 3D perception such as object silhouettes, keypoints, poses as well as the opportunity to borrow ideas from these recognition systems to model rich shape distributions. It is precisely these avenues for 3D reconstruction that this thesis explores - how can recognition help 3D reconstruction, specifically for the task of 3D object reconstruction. 

This thesis makes the following contributions in the area of learning-based 3D object reconstruction:
\begin{itemize}
    \item Building statistical models of 3D shapes for diverse object categories beyond faces and using them in conjunction with recognition techniques for fully automatic single-view 3D reconstruction.
    \item Enriching the output of object detection systems with real world heights of objects by predicting their amodal extent in scenes.
    \item Unifying single and multi-view 3D object reconstruction by incorporating geometric constraints in state-of-the-art recognition systems (CNNs)
\end{itemize} 

We begin in Chapter \ref{chapter:CategoryShapes} by addressing the problem of fully automatic object localization and reconstruction from a single image. This is both a very challenging and very important problem which has received limited attention due to difficulties in segmenting objects and predicting their poses. We leverage advances in learning convolutional networks for object detection~\cite{girshick2013rich}, instance segmentation~\cite{BharathCVPR2015} and camera viewpoint prediction~\cite{ShubhamPose}. These predictors, while very powerful, are still not perfect given the stringent requirements of 3D shape reconstruction. Thus, we introduce a new class of deformable 3D models that can be robustly fitted to images based on noisy pose and silhouette estimates computed upstream and that can be learned directly from 2D annotations available in existing object detection datasets. These deformable shape models capture top-down information about the major modes of shape variation within a class providing a ``low-frequency'' estimate of 3D shape. In order to capture fine instance-specific shape details, we fuse it with a high-frequency component recovered from bottom-up shading shading cues.

Armed with shape models for objects, we work towards coherently assembling 3D objects in a scene in Chapter \ref{chapter:Amodal}. More specifically, we look at the task of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges involved here, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. Here we propose to tackle these issues by building upon advances in object recognition using large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the objects in an image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completions to infer veridical sizes of objects in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scale ambiguities and demonstrate qualitative results on challenging real-world scenes.

In Chapter \ref{chapter:LSM}, we move beyond single-view 3D reconstruction and propose a unified framework for single and multi-view object reconstruction with calibrated camera poses within an end-to-end learnt framework. End-to-end learning allows us to implicitly model complex shape distributions using powerful CNNs while conforming to geometric constraints imposed by the camera poses. We show large improvements over learning systems that treat 3D reconstruction as a regression problem and classical multi-view stereo systems on low number of views.

We conclude with a discussion on the limitations of current systems and promising future directions in recognition-based 3D reconstruction.

%  With the advent of deep neural networks and their immense power in modelling visual data, the focus has recently shifted to modelling monocular cues implicitly with a CNN and predicting 3D from a single image as depth/surface orientation maps or 3D voxel grids.

% Talk about 3D reconstruction. How it has been approached traditionally. Brief history of 3D reconstruction. From blocks world to sfm to mvs. Mention faces as a special case of an object category. Talk about faces as a well studied object category. blanz and vetter. kemelmacher-schlizerman etc. Been traditionally hard for generic object categories due to greater variation in shapes etc.

% Recognition systems have improved leaps and bounds in recent times. We can detect objects in images, segment out the pixels for objects etc. Such outputs provide essential cues to 3D perception as well. For example, the silhouette provides information about the 3D shape. The relative locations of keypoints in a 2D image inform the global 3D pose of the object. 

% Gaps in literature: models for complex objects, low shot object reconstruction, a stable framework for integrating semantic reasoning into reconstruction.
% Contributions of this thesis:
% Use statistical machine learning techniques to reason about 3D perception cues from diverse image collections. Using recognition systems in the loop for informing object reconstruction. Adapting 
% \begin{itemize}
% \item Building statistical models for 3D shapes for diverse object categories beyond faces and using them in conjunction with recognition techniques for fully automatic single-view 3D reconstruction.
% \item Enriching the output of object detection systems with real world heights of objects by predicting their amodal extent in scenes.
% \item Unifying single and multi-view 3D object reconstruction by incorporating geometric constraints in state-of-the-art recognition systems (CNNs)
% \end{itemize} 

% Brief summary of the thesis. In chapter 2, we talk about our system to learn category specific deformable 3D models..... In chapter 3, we presnet amodal.... Finally in chapter 4, we present LSMs, where we learn in a data driven manner with CNNs while incorporating epipolar geometry constraints. 
\end{document}