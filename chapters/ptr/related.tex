\vspace{0.1cm}
\section{Related Work}
\label{sec:related}
\vspace{0.05cm}

A number of prior works have proposed algorithms for offline RL~\citep{fujimoto2018off,kumar2019stabilizing,kumar2020conservative,kostrikov2021offline,kostrikov2021iql,wu2019behavior,jaques2019way,fujimoto2021minimalist,siegel2020keep}. In particular, many prior works study offline RL with multi-task data and devise techniques that perform parameter sharing\citep{wilson2007multi, parisotto2015actor, teh2017distral, espeholt2018impala, hessel2019multi}, or perform data sharing or relabeling~\citep{yu2021conservative,andrychowicz2017hindsight,yu2022leverage,kalashnikov2021mt,xie2021lifelong}. In this paper, our goal is not to develop new offline RL algorithms, but to show that these offline RL algorithms can be an effective tool to pre-train from prior data and then fine-tune on new tasks. We show that a few simple but important design decisions are essential for making offline RL pre-training scalable, and provide detailed experiments on fine-tuning these pre-trained models to new tasks.

Going beyond methods that only perform fine-tuning from a learned initialization with online interaction~\citep{nair2020accelerating,kostrikov2021iql,lee2022offline}, we consider two independent fine-tuning settings: (1) the setting where we do not use any online interaction and fine-tune the pre-trained policy entirely offline, (2) the setting where a limited amount of online interaction is allowed to autonomously acquire the skills to solve the task from a challenging initial condition.
This resembles the problem setting considered by offline meta-RL methods~\citep{li2019multi, dorfman2020offline, mitchell2021offline, pong2021offline,lin2022model}. {However, our approach is simpler as we fine-tune the very same offline RL algorithm that we use for pre-training. In our experiments, we observe that our method, PTR, outperforms the meta-RL method of \citet{mitchell2021offline}. 

Some other prior approaches that attempt to leverage large, diverse datasets via representation learning~\citep{mandlekar2020iris,yang2021representation,yang2021trail,nair2022r3m,he2111masked,xiao2022masked,ma2022vip},
as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures~\citep{shafiullah2022behavior}.
We compare to some of these methods~\citep{xiao2022masked,nair2022r3m} in our experiments and find that PTR outperforms these methods. We also perform an empirical study to identify the design decisions behind the improved performance of RL-based PTR on demonstration data compared to BC, and find that the gains largely come from the ability of the value function in identifying the most ``critical'' decisions in a trajectory. While some prior works~\citep{mandlekar2021what} shows results that suggest that offline RL underperforms imitation learning when provided with human demonstration data, our results show that offline RL can perform better than BC even with demonstrations, supporting the analysis in \citet{kumar2022should}.


The most closely related to our work are prior methods that run model-free offline RL on diverse real-world data and then fine-tune on new tasks~\citep{singh2020cog,kalashnikov2021mt,julian2020never,chebotar2021actionable,lee2022spend}.
These prior methods typically only consider the setting of \emph{online} fine-tuning, whereas in our experiments, we demonstrate the efficacy of PTR for offline fine-tuning (where we must acquire a good policy for the downstream task using 10-15 demonstrations) \emph{as well as} online fine-tuning considered in these prior works, where we must acquire a new task entirely via autonomous interaction in the real world. 