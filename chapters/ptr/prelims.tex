\vspace{-0.2cm}
\section{Problem Statement and Definitions}
\vspace{-0.2cm}
% An RL algorithm aims to learn a policy in a Markov decision process (MDP), which is a tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \transitions, r, \mu_0, \gamma)$, where $\mathcal{S}, \mathcal{A}$ denote the state and action spaces, and
% $\transitions(\bs' | \bs, \mathbf{a})$, $r(\bs,\mathbf{a})$ represent the dynamics and reward function respectively. 
% $\mu_0(\bs)$ denotes the initial state distribution, and $\gamma \in (0,1)$ denotes the discount factor. The policy $\pi(\mathbf{a}|\bs)$ learned by RL agents must optimize the long-term cumulative reward, $\max_\policy J(\policy) := \E_{(\bs_t, \mathbf{a}_t) \sim \pi}[\sum_{t} \gamma^t r(\bs_t, \mathbf{a}_t)].$ 

\textbf{Problem statement.} Our goal is to learn general-purpose initializations from a broad, multi-task offline dataset and then fine-tune these initializations to specific downstream tasks. Following the notation from Chapter~\ref{chapter:cds_uds}, we denote the general-purpose offline dataset by $\mathcal{D}$, which is partitioned into $k$ chunks. Each chunk contains several transition typles for a given robotic task (e.g., picking and placing a given object) collected in a given domain (e.g., a particular kitchen). See \autoref{fig:system_overview} for an illustration. 
Formally, the dataset can be represented as $\mathcal{D} = \cup_{i=1}^k \left(i, \mathcal{D}_i \right)$, where we denote the set of training tasks concisely as $\mathcal{T}_{\text{train}} = [k]$. 
% Chunk $\mathcal{D}_i$ consists of data for a given task identifier $i$, and consists of a collection of transition tuples, $\mathcal{D}_i = \{(\bs^i_j, \mathbf{a}^i_j, r^i_j, \bs'^i_j)\}_{j=1}^n$ collected by a demonstrator on task $i$. 
% Each task has a different reward function.
Our goal is to utilize this multi-task dataset to help train a policy for one or multiple target tasks (denoted without loss of generality as task $\mathcal{T}_{\text{target}} = \{k+1, \cdots, n\}$). 

While the diverse prior dataset $\mathcal{D}$ does not contain any experience for the target tasks, in the offline fine-tuning setting, we are provided with a very small dataset of demonstrations $\mathcal{D}^* := \{\mathcal{D}_{k+1}^*, \mathcal{D}^*_{k+1}, \cdots, \mathcal{D}^*_n\}$ corresponding to each of the target tasks. In our experiments, we use only 10 to 15 demonstrations for each target task, making it impossible to learn the target task from this data alone, such that a method that effectively maximizes performance for the target tasks $\mathcal{T}_\text{target}$ must leverage the prior data $\mathcal{D}$. We also study the setting where we aim to quickly fine-tune the policy learned via offline pre-training and offline fine-tuning using limited amounts of autonomously collected data via online real-world interaction. More details about this are in Section~\ref{sec:experiments_online}.
 
%%AK: maybe problem statement and background should be different sections?
% \textbf{Background and preliminaries.} The Q-value of a given state-action tuple $Q^\pi(\bs, \mathbf{a})$ for a policy $\pi$ is the long-term discounted reward attained by executing action $\mathbf{a}$ at state $\bs$ and following policy $\pi$ thereafter. The Q-function satisfies the Bellman equation $Q^\pi(\bs, \mathbf{a}) = r(\bs, \mathbf{a}) + \gamma \mathbb{E}_{\bs', \mathbf{a}'}[Q^\pi(\bs', \mathbf{a}')]$. Typical model-free offline RL methods~\citep{fujimoto2018off,kumar2019stabilizing,kumar2020conservative} alternate between estimating the Q-function of a fixed policy $\pi$ using the offline dataset $\mathcal{D}$ and then improving the policy $\pi$ to maximize the learned Q-function. Our system, \ptrmethodname, utilizes one such model-free offline-RL method, conservative Q-learning (CQL)~\citep{kumar2020conservative}. We discuss how we adapt CQL for pre-training on diverse data followed by single-task fine-tuning in Section~\ref{sec:method}.

\textbf{Tasks and domains}. We use the Bridge Dataset~\cite{ebert2021bridge} as the source of our pre-training tasks, which we augment with a few additional tasks as discussed in Section~\ref{sec:result}. Our terminology for ``task'' and ``domain'' follows \citet{ebert2021bridge}: a task is a skill-object pair, such as ``put potato in pot'' and a domain corresponds to an environment, which in the case of the Bridge Dataset consists of different toy kitchens, potentially with different viewpoints and robot placements. We assume the new tasks and environments come from the same training distribution, but are not seen in the prior data.