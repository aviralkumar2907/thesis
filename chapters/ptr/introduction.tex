\vspace{-0.2cm}
\section{Introduction}
\label{sec:ptr_intro}
\vspace{-0.2cm}

% Robotic learning methods based on reinforcement learning (RL) or imitation learning (IL) have led to impressive results~\citep{levine2016end,kalashnikov2018qtopt, young2020visual, kalashnikov2021mt,ahn2022can}, but the generalization abilities of policies learned this way are typically limited by the quantity and breadth of training data available. In practice, the cost of real-world data collection for each task means that such methods often use smaller datasets, which leads to more limited generalization. A natural way to circumvent this limitation is to incorporate existing diverse robotic datasets into the training pipeline of a robot learning algorithm, analogously to how pre-training on diverse prior datasets has enabled rapid fine-tuning in supervised learning. How can we devise methods that enable effective pre-training for robotic RL?

% In most cases, answering this question requires devising a method that can pre-train on existing data from a wide range of tasks and domains, and then provide a good starting point for efficiently learning a \emph{new} task in a \emph{new} domain. Prior approaches utilize such existing data by running imitation learning (IL)~\citep{young2020visual,ebert2021bridge,shafiullah2022behavior} or by using representation learning~\citep{nair2022r3m} methods for pre-training and then fine-tuning with imitation learning. However, this may not necessarily lead to representations that can reason about the consequences of their actions. In contrast, end-to-end RL can offer a more general paradigm, that can be effective for both pre-training and fine-tuning, and is applicable even when assumptions in prior work are violated. Hence we ask, can we devise a simple and unified framework where \emph{both} the pre-training and fine-tuning process uses RL? This presents significant challenges pertaining to leveraging large amounts of offline multi-task datasets, which would require high capacity models and this can be very challenging \citep{bjorck2021towards}.

In this chapter, we show that multi-task offline RL pre-training on diverse multi-task demonstration data followed by offline RL fine-tuning on a very small number of trajectories (as few as 10 trials, maximum 15) or online fine-tuning on autonomously collected data, can indeed be made into an effective robotic learning strategy that can significantly outperform methods based on imitation learning as well as RL-based methods that do not employ pre-training. This is surprising and significant, since prior work~\citep{mandlekar2021what} has suggested that imitation learning methods are superior to offline RL when provided with human demonstrations. Moreover prior RL-based pre-training and fine-tuning methods typically require thousands of trials~\citep{singh2020cog,kalashnikov2021mt,julian2020never,chebotar2021actionable,lee2022spend}. Our framework, which we call \ptrmethodname (pre-training for robots), is based on the CQL algorithm discussed previously, but introduces a number of design decisions, that we show are critical for good performance and enable large-scale pre-training. These choices include a specific choice of architecture for providing high capacity while preserving spatial information, the use of group normalization, and an approach for feeding actions into the model that ensures that actions are used properly for value prediction. We experimentally validate these design decisions and show that PTR benefits from increasing the network capacity, even with large ResNet-50 architectures, which have never been previously shown to work with offline RL. Our experiments utilize the Bridge Dataset~\citep{ebert2021bridge}, which is an extensive dataset consisting of thousands of trials for a very large number of robotic manipulation tasks in multiple environments. A schematic of PTR is shown in Figure~\ref{fig:system_overview}. 

% The main contribution of this chapter is a demonstration that PTR can enable offline RL pre-training on diverse real-world robotic data, and that these pre-trained policies can be fine-tuned to learn new tasks with just 10-15 demonstrations or with autonomously collected online interaction data in the real world. This is a significant improvement . We present a detailed analysis of the design decisions that enable offline RL to provide an effective pre-training framework, and show empirically that these design decisions are crucial for good performance. {Although these decisions are based on prior work, we show that the novel combination of these components in PTR is important to make offline RL into a viable pre-training tool that can outperform other approaches.}
