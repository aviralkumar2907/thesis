\section{Sampling Distributions}
\label{sec:sampling_distributions}

%As alluded to in Section~\ref{sec:analysis_nonstationarity}, the choice of sampling distribution $\mu$ is an important design decision can have a large impact on performance. Indeed, it is not immediately clear which distribution is ideal for Q-learning. In this section, we hope to shed some light on this issue.

%\subsection{Technical Background}
Off-policy data has been cited as one of the ``deadly triads'' for Q-learning~\citep{suttonrlbook}, which has potential to cause instabilities in learning. On-policy distributions~\citep{Tsitsiklis1997} and fixed behavior distributions~\citep{Sutton09a,Maei2010} have often been targeted for theoretical convergence analysis, and many works use importance sampling to correct for off-policyness~\citep{precup2001offpol, munos2016safe}
However, to our knowledge, there is relatively little guidance which compares how different weighting distributions compare in terms of convergence rate and final solutions.

Nevertheless, several works give hypotheses on good choices for weighting distributions.~\citep{munos2005erroravi} provides an error bound which suggests that ``more uniform'' distributions can guarantee better worst-case performance.~\citep{NIPS2017_6913} suggests that when the state-distribution is fixed, the action distribution should be weighted by the optimal policy for residual Bellman errors. In deep RL, prioritized replay~\citep{Schaul2015}, and mixing replay buffer with on-policy data~\citep{hausknecht2016policy,zhang2018deeper} have been found to be effective. %We aim to empirically analyze multiple choices for weighting distributions to determine which are the most effective.

%\vspace{-10pt}
\subsection{What Are the Best Weighting Distributions in Absence of Sampling Error?}
\label{subsec:dist_shift_exact}

\begin{figure*}[ttt!]
\begin{minipage}[t]{0.32\linewidth}
\caption{\label{fig:distribution_shift_vs_returns} Average distribution shift across time for different weighting distributions, plotted against returns for a 256x256 model. We find that distribution shift does not have strong correlation with returns.}
\includegraphics[width=0.99\columnwidth]{chapters/diagnosing_q/images/returns_vs_shift}
% generated by plot_distribution_shift.py
% data_dir = east1//2019-02-12-exact-weighting-distr-shift
\vspace{-0.2in}
\end{minipage}
~\vline~
\begin{minipage}[t]{0.32\linewidth}
\caption{\label{fig:weighting_schemes} Weighting distribution versus architecture in Exact-FQI. Replay(s, a) consistently provides the highest performance. Note that Adversarial Feature Matching is comparable to Replay(s, a), but surprisingly better for small networks. }
\includegraphics[width=0.99\columnwidth]{chapters/diagnosing_q/images/exact_fqi_schemes.pdf}
% generated by plot_exact_weighting.py
% data_dir = east1//2019-01-18-newenv-exact-weighting
\vspace{-0.3in}
\end{minipage}
~\vline~
\begin{minipage}[t]{0.32\linewidth}
\caption{\label{fig:weighting_entropy_vs_returns} Normalized returns plotted against normalized entropy for different weighting distributions. All experiments use Exact-FQI with a 256x256 network. We see a general trend that high-entropy distributions lead to greater performance.}
\includegraphics[width=0.99\columnwidth]{chapters/diagnosing_q/images/returns_vs_entropy}
% generated by plot_distribution_shift.py
% data_dir = east1//2019-02-12-exact-weighting-distr-shift
\end{minipage}
\end{figure*}


We begin by studying the effect of weighting distributions when disentangled from sampling error. We run Exact-FQI with an ablation over architectures and weighting distributions and report our results in Fig.~\ref{fig:weighting_schemes}. $\text{Unif}(s,a)$ $Replay(s,a)$, and $\text{Prioritized}(s,a)$ consistently result in the highest returns across all architectures.
We believe that these results are in favor of the \textit{uniformity} hypothesis: the best distributions spread weight across larger support of the state-action space. For example, a replay buffer contains state-action tuples from many policies, and therefore would be expected to have wider support than the state-action distribution of a single policy. We can see this general trend in Fig.~\ref{fig:weighting_entropy_vs_returns}. 
%These distributions generally result in the tightest contraction rates, and allow the Q-function to focus on high-error regions. 
%In the sampled setting, this observation motivates exploration algorithms that maximize state coverage (for example, ~\citet{hazan2018} solve an exploration objective which maximizes state-space entropy).
%However, note that in this particular experiment is distinct from exploration, as there is no sampling involved. 
%All states are observed, just with different weights, thus isolating the issue of distributions from the issue of sampling.

\subsection{Designing a Better Off-Policy Distribution: Adversarial Feature Matching}
\label{sec:afm}

In our final study, we attempt to design a better weighting distribution using insights from previous sections that can be integrated into deep RL methods. We refer to this method as adversarial feature-matching (AFM). We draw upon three specific insights outlined in previous analysis. First, the function approximator should be incentivized to maximize its ability to distinguish states to minimize function approximation bias (Section~\ref{sec:function_approx}). Second, the weighting distribution should emphasize areas where the Q-function incurs high Bellman error, to minimize the discrepancy between $\ltwonorm$ norm error and $\linfnorm$ norm error. Third, high-entropy weighting distributions tend to be higher performant. The first insight was also demonstrated in \cite{martha2018sparse} where enforcing sparsity in the Q-function was found to provide locality in the Q-function which prevented catastrophic interference and provided better values for bootstrapping.

We propose to model our problem as a minimax game, where the weighting distribution is a parameterized adversary $p_\phi(s, a)$ which tries to \emph{maximize} the Bellman error, while the Q-function ($Q_\theta(s, a)$) tries to minimize it. 
Note that in the unconstrained setting, this game is equivalent to minimizing the $\linfnorm$ norm error in its dual-norm representation. However, in practical settings where minimizing stochastic approximations of the $\linfnorm$ norm can be difficult for neural networks, it is crucial to introduce constraints to weaken the adversary. These constraints also make the adversary closer to the uniform distribution while allowing it to be sufficiently different at specific state-action pairs.

We use a feature matching constraint which enforces the expected feature vectors, $\mathbb{E}[\Phi(s)]$, under $p_\phi(s, a)$ to \emph{roughly} match the expected feature vector under uniform sampling from the replay buffer. We can express the output of a neural network Q-function as $Q_\theta(s, a) = w_{a}^T \Phi_\theta(s)$ or, in the continuous case, as $Q_\theta(s, a) = w^T \Phi_\theta(s, a)$,  where the feature vector $\Phi_\theta(s), \Phi_\theta(s, a)$ represent the the output of all but the final layer.
Intuitively, this constraint restricts the adversary to distributing probability mass among states that are perceptually similar to the Q-function. The objective is
\begin{multline*}
    \min_{\theta, w} \max_{\phi} \mathbb{E}_{p_\phi(s, a)} [(Q_{w, \theta} (s, a) - y(s, a))^2]\\
s.t.~~ \vert\vert \mathbb{E}_{p_\phi(s, a)}[\Phi(s)] - \frac{\sum_i \Phi(s_i)}{N} \vert\vert \leq \varepsilon
\end{multline*}
Note that $\Phi(s)$ is a function of $\theta$ but, while solving the maximization, $\theta$ is assumed to be a constant. This is equivalent to solving only the inner maximization with a constraint, and empirically provides better stability. Implementation details for AFM are provided in \textbf{Appendix \ref{app:adversarial}}. The $\frac{\sum_{i} \Phi(s_i)}{N}$ denotes an estimator for the true expectation under some sampling distribution, such as a uniform distribution over all states and actions (in exact FQI) or the replay buffer distribution. So, $\frac{\sum_{i} \Phi(s_i)}{N} \approx E_{p_{rb}}[\Phi]$ holds when using a replay buffer.

In tabular domains with Exact-FQI, we find that AFM performs at par with the top performing weighting distributions, such as $\text{Unif}(s, a)$ and \textit{better} than $\text{Prioritized}(s, a)$ (Fig.~\ref{fig:weighting_schemes}). This confirms that adaptive prioritization works better than Prioritized($s, a$). Another benefit of AFM is its robustness to function approximation and the performance gains in the case of small architectures (say, $(4, 4)$) are particularly noticeable. (Fig.~\ref{fig:weighting_schemes})

In tabular domains with Replay-FQI (Table~\ref{table:final}), we also compare AFM to prioritized replay (PER)~\citep{Schaul2015}, where AFM and PER perform similarly in terms of normalized returns. We also evaluate a variant of AFM (AFM+Sampling in Table~\ref{table:final}) which changes which samples instead of reweighting. We further evaluate AFM on MuJoCo tasks with the TD3 algorithm~\citep{pmlr-v80-fujimoto18a} and the entropy constrained SAC algorithm~\citep{Haarnoja18}. The results are presented in the appendix. We find AFM improving performance of the algorithm with three MuJoCo tasks (Ant, Hopper and Cheetah) and two algorithms (TD3 and SAC).

\begin{table}
    \centering
    \small{
    \begin{tabular}{|c|r|r|}
    \hline
    \textbf{Sampling distribution} & \textbf{Norm. Returns} & \textbf{Norm. Returns} \\
     & \textbf{(16, 16)} & \textbf{(64, 64)} \\
     \hline\hline
    None &  0.18 & 0.23 \\
    \hline
    Uniform(s, a) &  0.19 & 0.25 \\
    \hline
    $\pi(s, a)$ &  \textbf{0.45} & 0.39 \\
    \hline
    $\pi^*(s, a)$ & 0.30 & 0.21 \\
    \hline
    Prioritized(s, a) & 0.17 & 0.33 \\
    \hline
    PER~\cite{Schaul2015} & 0.42 & \textbf{0.49}\\
    \hline
    \textbf{AFM} (Ours) & 0.41 & \textbf{0.48} \\
    \hline
    \textbf{AFM + Sampling} (Ours) & 0.43 & \textbf{0.51} \\
    \hline
     \end{tabular}}
    \caption{\label{table:final}Average Performance of various sampling distributions for (16, 16) and (64, 64) neural nets in the setting with replay buffers averaged across 5 random seeds. PER, our AFM and on-policy sampling perform roughly at par on benchmark tasks in expectation when using (16, 16) architectures. However, note that  $\pi(s,a)$ is generally computationally intractable.}
    \vspace{-0.2in}
    \vspace{-10pt}
\end{table}
