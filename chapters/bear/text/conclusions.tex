\vspace{-0.2cm}
\section{Discussion and Limitations}
\vspace{-0.2cm}

In this chapter, we theoretically and empirically analyzed how error propagates in offline RL due to the use of out-of-distribution actions for computing the target values in the Bellman backup. Our experiments suggest that this source of error is one of the primary issues afflicting off-policy RL: increasing the number of samples does not appear to mitigate the degradation issue (Figure~\ref{fig:divergence}), and training with na\"{i}ve RL on data from a random policy, where there are no out-of-distribution actions, shows much less degradation than training on data from more focused policies (Figure~\ref{fig:optimal_random}). Armed with this insight, we develop a method for mitigating the effect of out-of-distribution actions, which we call BEAR. BEAR constrains the backup to use actions that have non-negligible support under the data distribution, but without being overly conservative in constraining the learned policy. We observe experimentally that BEAR achieves good performance across a range of tasks, and across a range of dataset compositions, learning well on random, medium-quality, and expert data.

\iffalse
% \vspace{-0.15in}
\begin{wrapfigure}{r}{0.51\textwidth}
        \includegraphics[width=0.48\linewidth]{chapters/bear/images/kl_vs_mmd_ablation_final.pdf}
       ~
        \includegraphics[width=0.48\linewidth]{chapters/bear/images/num_samples_ablation.pdf}
        % ~
        % \includegraphics[width=0.31\linewidth]{images/ensembles_ablation_final.pdf}
      \caption{\footnotesize Average return (averaged Hopper-v2 and Walker2d-v2) as a function of train steps for ablation studies from Section~\ref{subsec:ablations}. (a) MMD constrained optimization is more stable and leads to better returns, (b) 4 sample MMD is more performant than 10.}
    %   and (c) Ensemble variance has mixed benefit.}
      \label{fig:ablations}
\vspace{-10pt}
\end{wrapfigure}
\fi

% One limitation of our current BEAR is that, although the learned policies are more performant than those acquired with na\"{i}ve RL, performance sometimes still tends to degrade for long learning runs. An exciting direction for future work would be to develop an early stopping condition for RL, perhaps by generalizing the notion of validation error to reinforcement learning. 
\niparagraph{Limitations.} A limitation of BEAR, and also other approaches that perform constrained-action selection, is that they can be overly conservative compared to methods that aim to constrain state-distributions directly and, even, methods that utilize conservative value estimation, especially with datasets collected from mixtures of sub-optimal policies. In the next chapter, we will present a principle for conservative value-estimation and develop model-based and model-free offline RL algorithms based on this principle, which we find leads to better performance across a broader range of tasks.  

Theoretically, while our analysis demonstrates that support-matching based on distributional backups can perform better than policy-constrained backups, unfortunately, developing an efficient practical method for support matching in high-dimensional continuous action spaces is still an open question. Additionally, there is also room to formally characterize the gap between support-matching and policy-constrained backups. We make progress towards this open question in a follow-up manuscript~\citep{singh2022offline}.  
% Perhaps methods from outside RL, predominantly used in domain adaptation, such as using asymmetric f-divergences~\citep{wu19domain} can be used for support restriction.} 
% Another promising future direction is to examine how well BEAR-QL can work on large-scale off-policy learning problems, of the sort that are likely to arise in domains such as robotics, autonomous driving, operations research, and commerce. If RL algorithms can learn effectively from large-scale off-policy datasets, reinforcement learning can become a truly data-driven discipline, benefiting from the same advantage in generalization that has been seen in recent years in supervised learning fields, where large datasets have enabled rapid progress in terms of accuracy and generalization~\cite{imagenet_cvpr09}.

\section*{Acknowledgements and Funding}
We thank Kristian Hartikainen for sharing implementations of RL algorithms and for help in debugging certain issues. We thank Matthew Soh for help in setting up environments. We thank Aurick Zhou, Chelsea Finn, Abhishek Gupta, Kelvin Xu and Rishabh Agarwal for informative discussions. We thank Ofir Nachum for comments on an earlier draft of this paper. We thank Google, NVIDIA, and Amazon for providing computational resources. This research was supported by Berkeley DeepDrive, JPMorgan Chase \& Co., NSF IIS-1651843 and IIS-1614653, the DARPA Assured Autonomy program, and ARL DCIST CRA W911NF-17-2-0181.