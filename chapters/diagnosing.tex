\documentclass[../thesis.tex]{subfiles}

\usepackage{wrapfig}
\usepackage{cite}
% \usepackage{natbib}
% \usepackage[round]{natbib}

\begin{document}

% \blfootnote{This chapter is based on \cite{fu2019diagnosing}, published at ICML 2019, which is joint work with Justin Fu, Matthew Soh, and Sergey Levine.}

% \input{chapters/bear/text/abstract.tex}

% \input{chapters/bear/text/introduction.tex}
% \input{chapters/bear/text/related.tex}
% \input{chapters/bear/text/background.tex}
% \input{chapters/bear/text/method.tex}
% \input{chapters/bear/text/experiments.tex}
% \input{chapters/bear/text/conclusions.tex}

% Q-learning methods are a common class of algorithms used in reinforcement learning (RL). However, their behavior with function approximation, especially with neural networks, is poorly understood theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a "unit testing" framework where we can utilize oracles to disentangle sources of error. 
% Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with deep RL methods.
% We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains. 

\vspace{-0.4cm}
\begin{AIbox}{\large{\textbf{Abstract}}}
\vspace{4mm}
In this chapter, we seek to understand challenges in the offline RL problem setting. To this end, we empirically study the behavior of off-policy RL methods when only training on a static, offline dataset. In principle, off-policy RL methods such as those based on approximate dynamic programming should succeed at leveraging experience collected from prior policies for sample-efficient learning. However, as we illustrate in this chapter, in practice, commonly used off-policy methods based on Q-learning and actor-critic methods are incredibly sensitive to \emph{both} the dataset distribution and quantity. Building on the insights from our empirical observations, we identify two main issues with learning value functions in the offline setting: \emph{distributional shift} and \emph{sampling error}. Later in the thesis, we also demonstrate that these challenges also inhibit performance for other classes of off-policy RL methods such as model-based approaches. Finally, in the subsequent chapters of this dissertation, we develop techniques to handle these challenges, progressing towards reliable and easy-to-use offline RL algorithms. 
% Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. 
% In this chapter, we will present a paradigm called \emph{conservative value function estimation}, which aims to address these limitations by learning a value-function such that the expected value of a policy under this value-function lower-bounds its true value. We discuss two concrete variants of this paradigm: a model-free variant, called conservative Q-learning (CQL)~\citep{kumar2020conservative}, and a model-based variant, called conservative offline model-based policy optimization (COMBO)~\citep{yu2021conservative}. Both of these variants can be implemented by augmenting the standard temporal-difference error objective with a Q-value regularizer, on top of model-based and model-free Q-learning and actor-critic algorithms. We will also theoretically show that this paradigm produces conservative value functions, that lower bound the ground-truth value of the current policy, and policies, that enjoy robust policy improvement.
\vspace{2mm}
% CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with safe improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of standard Q-learning and actor-critic implementations. 
% On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.
\end{AIbox}
    
\input{chapters/diagnosing_q/sections/1_introduction.tex}
% \input{chapters/diagnosing_q/sections/3_background.tex}
\input{chapters/diagnosing_q/sections/4_setup.tex}
% \input{chapters/diagnosing_q/sections/5_approx.tex}
\input{chapters/diagnosing_q/sections/6_sampling.tex}
% \input{chapters/diagnosing_q/sections/7_nonstationarity.tex}
\input{chapters/diagnosing_q/sections/8_weighting_distributions.tex}
\input{chapters/diagnosing_q/sections/conclusion.tex}


\end{document}