\documentclass[../thesis.tex]{subfiles}
\begin{document}

Over the course of various chapters in this dissertation, we have attempted to develop a ``tool-box'' for utilizing static datasets in reinforcement learning. In Part I of this thesis, we began by understanding the challenges in the offline RL setting by running diagnostic experiments in a ``unit-testing'' framework, followed by developing an initial offline RL approach, BEAR, which restricted policy learning to the support of the offline data in Part II. Refining this approach, we then developed the paradigm of conservative value estimation and instantiated it into two algorithms: model-free CQL and model-based COMBO. We found that CQL and COMBO attain state-of-the-art empirical results. The efficacy of this paradigm is reflected in the use of CQL by external practitioners to tackle a variety of real-world decision-making problems. 

Building on the foundations provided by the principle of conservative value estimation, in Part III of this dissertation we developed approaches that make it possible to utilize offline RL effectively with high-capacity models and diverse datasets. We saw how the implicit regularization of stochastic gradient descent, which is widely believed to aid generalization of over-parameterized deep neural networks in supervised learning, can hurt when learning value functions in the offline setting. Building on this insight, we proposed the explicit DR3 regularizer that alleviates pathologies due to implicit regularization in offline RL and showed that in conjunction with the ResNet architectures, DR3 enables offline Q-learning to scale well to high-capacity models. Then, we turned our attention to the problem of maximally utilizing diverse data and proposed the CDS approach for intelligently relabeling data in multi-task offline RL problems and extended it via the UDS approach to deal with scenarios when reward annotations are hard to obtain during data sharing. We also developed Cal-QL, a method that extends CQL to make it effective at fast online fine-tuning, after offline training. Finally, in Part IV of this dissertation, we utilized the aforementioned algorithmic advancements to tackle decision-making problems in two application domains.   

\paragraph{Open Problems.} Zooming out, the tools discussed in this dissertation and several other works in the field of RL with static datasets provide an effective starting point for \textbf{building broadly deployable decision-making systems}, but there are still several questions that I believe are important to address towards attaining this goal. Here, I discuss some questions that I especially find the most compelling to address. 
% For instance, in many problems, large, problem-specific datasets are not available and we need to develop techniques to instead incorporate multi-modal data sources that do not fit into the traditional RL paradigm; more analysis and methodological work is needed in developing reliable and easy to tune RL methods; algorithms that benefit from offline datasets but are also able to perform targeted and safe online interaction to perceive and adapt to the real world need to be developed. 

{\textbf{Foundation models for decision-making that use multi-modal data.}} 
The mantra in modern ML is that larger and more diverse the dataset, the better the performance. However, problem-specific datasets for many sequential decision-making problems (e.g., protein design, human-AI interaction) are much smaller and narrow as well. We can attempt to tackle this issue by incorporating multi-modal datasets from other sources (e.g., language annotations of proteins for protein design). But this is tricky as it requires a departure from the standard RL paradigm since multi-modal datasets often do not satisfy several pre-conditions needed for RL, e.g., no transitions, no rewards, no sequential structure. It is also unclear if ways to incorporate multi-modal data from supervised learning alone are sufficient for decision-making. Some of our initial works~\citep{yu2021conservative,yu2022leverage,ajay2020opal}, including UDS from Chapter~\ref{chapter:cds_uds} take initial steps towards this goal, but nonetheless, it is important for future work to study questions pertaining to extracting rewards, representations and structures from multi-modal data using insights from self-supervised learning and pre-training. 
Perhaps a particularly interesting initial step in the current research atmosphere is developing reinforcement learning methods that can utilize initializations provided by multi-modal pre-trained foundation models.  

{\textbf{Understanding and building reliable and easy-to-tune RL methods.}} One of the major reasons behind the widespread adoption of supervised and unsupervised ML in real-world systems is that these methods behave consistently and are easy to tune, greatly reducing the need to deploy imperfect models. To enable a broad real-world deployment of RL, we need to ensure RL enjoys similar benefits. This entails a continued effort on understanding and analyzing the failures and learning dynamics of offline RL building on our insights in Chapter~\ref{chapter:scaling}; designing theoretically sound recipes for tuning design decisions in offline RL, equivalently to cross-validation in standard machine learning; understanding which RL algorithms must be used when the underlying problem satisfies specific certain structural conditions; to list a few. Some of my work~\citep{kumar2021workflow,kumar2022should} tackles questions of this sort using tools from optimization and statistical machine learning, but a complete solution is still missing and a promising avenuew for future work. I believe that the next generation of reinforcement learning algorithms will be methods that come equipped with protocols or ``workflows'' for practitioners to use them effectively. Ultimately, success towards this goal comes down to the usability of techniques in the real world, and hence we believe that it would be especially important to study these questions while being grounded in real-world applications.

{\textbf{Robustness and adaptability in the rapidly-changing real world.}} Upon deployment, RL algorithms that train on static datasets must not just run the learned policy, but they must also perceive and adapt to the current state of the real world (for e.g., understand the intent of users around them) while exhibiting safe and robust behavior. They also need to tackle implicit biases and heterogeneity in experience they will gather. For example, a drug discovery method must learn to neutralize a new pathogen in the least amount of trials while being safe for consumption at every step in the process; a recommender system should not promote unethical content and amplify biases in society just to attain a higher click-through-rate. This presents a challenge as we need to develop offline RL algorithms that not only leverage static datasets to learn policies, but also to learn strategies that are adaptive and exhibit safe behavior. One potential avenue to study these questions could be to build on my initial work~\citep{bharadhwaj2020conservative,ajay2020opal} and utilize insights from areas such as robustness, adaptation, and meta learning.

{\textbf{Advancing real-world applications.}} An important, but overlooked factor in RL algorithms research is iterating on real-world problems alongside algorithmic research. In general, the RL problem is often intractable, and indeed, this is evident from a number of theoretical results showing hardness of RL. This also means that making long-term progress in RL needs to be grounded on a set of real-world problems, with representative properties. Some promising avenues for future work includes problems in \textbf{computer architecture}~\citep{fawzi2022discovering}, \textbf{robotic manipulation}, and also problems understudied in the context of RL such as \textbf{optimizing drug designs}, which requires building easy-to-tune algorithms capable of learning from multi-modal datasets. 
Another interesting problem in modern-day context is that of fine-tuning deep learning models to make them amenable for safe interaction with humans (\textbf{human-AI interaction}), which requires easy-to-use RL algorithms that can leverage static data while also exhibiting rapid adaptation. Another example is the problem of making decisions in \textbf{electrical systems} (e.g., power grids, HVAC systems~\citep{wong2022optimizing}) to control climate change, using historical data, safe and robustly deployable methods. It would also be very valuable to bring the insights from these domains back to the algorithms community by building benchmarks that serve as milestones to guide research in the years to come.  
% I also plan to bring the insights back to the algorithm community by building benchmarks that serve as milestones to guide research in years to come.

\end{document}