\vspace{-0.2cm}
\subsection{Related Work}
\label{sec:uds_related}
\vspace{-0.2cm}
% \textbf{Offline RL.} Offline RL~\citep{ernst2005tree,riedmiller2005neural,LangeGR12,levine2020offline} considers the problem of learning a policy from a static dataset without interacting with the environment, which has shown promises in many practical applications such as robotic control~\citep{kalashnikov2018scalable,Rafailov2020LOMPO}, NLP~\citep{jaques2019way}, 
% and healthcare~\citep{shortreed2011informing, Wang2018SupervisedRL}. 
% The main challenge of offline RL is the distributional shift between the learned policy and the behavior policy~\citep{fujimoto2018off}, which can cause erroneous value backups. To address this issue, prior methods have constrained the learned policy to be close to the behavior policy via policy regularization~\citep{liu2020provably,wu2019behavior,kumar2019stabilizing, zhou2020plas,ghasemipour2021emaq,fujimoto2021minimalist}, conservative value functions~\citep{kumar2020conservative}, 
% and model-based training with conservative penalties~\citep{yu2020mopo,kidambi2020morel,swazinna2020overcoming,lee2021representation,yu2021combo}. Unlike these prior works, we study how unlabeled data can be incorporated into the offline RL framework.

\textbf{RL with unlabeled data.} Prior works tackle the problem of learning from data without reward labels via either directly imitating expert trajectories~\citep{pomerleau1988alvinn,RossB12,GAIL2016Ho}, learning reward functions from expert data using inverse RL~\citep{abbeel2004apprenticeship,ng2000irl,ziebart2008maximum,finn2016guided,AIRLFu2018,konyushkova2020semi}, or learning a reward / value classifier that discriminates successes and failures~\citep{xie2018few,fu2018variational,singh2019end,eysenbach2021replacing} in standard online RL settings. 
{Unlike these prior works, the goal of our paper is not to propose a sophisticated new algorithm for reward learning,} but rather to study when the simple solution of using zero as the reward label can work well from offline data. {While} \citet{singh2020cog} also considers relabeling prior data with zero reward in the standard, single-task offline RL setting, {the key distinction is that the unlabeled data in \citet{singh2020cog} cannot solve the target task and \emph{actually} gets zero rewards and is thus labeled with the true reward. Unlike \citet{singh2020cog}, we show that UDS can surprisingly also be effective even when the unlabeled data is incorrectly labeled with zero reward, and discuss how it can be improved (Section~\ref{sec:rebalancing}).}
Additionally, we consider both single-task and multi-task offline RL settings.

\textbf{Data sharing.} 
% Sharing data across different tasks has been found to be effective in multi-task~\citep{eysenbach2020rewriting,kalashnikov2021mt,yu2021conservative} and meta-RL~\citep{dorfman2020offline,mitchell2021offline} and it improves performance significantly in multi-task offline RL. 
As discussed in Section~\ref{sec:cds_method}, prior works share data based on learned Q-values~\citep{eysenbach2020rewriting,li2020generalized} (including our CDS approach), domain knowledge~\citep{kalashnikov2021mt}, and other structural quantities such as distance to goals in goal-conditioned settings~\citep{andrychowicz2017hindsight,liu2019competitive,sun2019policy,lin2019reinforcement,chebotar2021actionable}, or the learned distance with robust inference in the offline meta-RL setting~\citep{li2019multi}. However, all of these either require access to the functional form of the reward for relabling or are limited to goal-conditioned settings. In contrast, we attempt to tackle a more general problem where reward labels are not provided for a subset of the data, and find that a simple approach for relabeling data from other tasks with the constant zero (or the minimal possible reward) can work well. 
