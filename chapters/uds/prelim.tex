\section{Preliminaries}
\vspace{-0.15cm}
\label{sec:prelim}

\textbf{Offline RL.} Standard RL considers a Markov decision process (MDP), $\mdp =(\states, \actions, P, \gamma, R)$, where $\states$ and $\actions$ denote the state and action spaces respectively, $P(\bs' | \bs, \mathbf{a})$ denotes the dynamics, $\gamma \in [0, 1)$ is the discount factor, and $R$ correspond to the reward function. Offline RL tackles the problem of learning a policy $\pi(\mathbf{a}|\bs)$ from a static dataset with $\mathcal{D}$, generated by a behavior policy $\pi_\beta(\mathbf{a}|\bs)$.


\textbf{Data sharing in offline RL.} Data sharing has been considered in the multi-task offline RL setting where there is a static multi-task dataset with $\mathcal{D} = \cup_{i=1}^N \mathcal{D}_i$ where $N$ is the number of tasks. Prior works~\citep{kalashnikov2021mt,eysenbach2020rewriting,yu2021conservative} show that sharing data from different tasks to task $i$ to be conducive. To do so, these prior methods assume access to the functional form of the reward $r_i$. This is a strong assumption in practice, as it necessitates access to a functional (programmatic) form for the reward function. In offline RL, it might be desirable to simply label the reward function by hand, but then the algorithm does not have access to the functional form of the reward, and all unlabeled data also needs to be labeled by hand for use with such methods. Our aim in this paper is to utilize unlabeled data without any reward labels at all.
If however functional access to the reward \emph{is} available, a simple strategy is to na\"ively share data across all tasks, which we refer to as Sharing All. Formally, Sharing All defines the dataset of transitions relabeled from task $j$ to task $i$ as $\mathcal{D}_{j \rightarrow i}$ and the method can be then defined as
    $\mathcal{D}^\mathrm{eff}_i := \mathcal{D}_i \cup ( \cup_{j \neq i} \mathcal{D}_{j \rightarrow i})$,
where $\mathcal{D}^\mathrm{eff}_i$ denotes the effective dataset for task $i$. Therefore, the policy optimization objective in Sharing All can be written as follows:
\begin{equation*}
     \forall i \in [N], ~~\pi^*(\mathbf{a}|\bs, i) := \arg \max_{\pi}~~ J_{\mathcal{D}^\mathrm{eff}_i}(\pi) - \alpha D(\pi, \pi^\mathrm{eff}_\beta),
\end{equation*}
where $\pi_\beta^\mathrm{eff}(\mathbf{a}|\bs, i)$ is the effective behavior policy for task $i$ denoted as $\pi_\beta^\mathrm{eff}(\mathbf{a}|\bs, i) := |\mathcal{D}^\mathrm{eff}_i(\bs, \mathbf{a})| / |\mathcal{D}^\mathrm{eff}_i(\bs)|$, $J_{\mathcal{D}^\mathrm{eff}_i}(\pi)$ denotes the average return of policy $\pi$ in the empirical MDP induced by the effective dataset, and $D(\pi, \pi^\mathrm{eff}_\beta)$ denotes a divergence measure (e.g., KL-divergence~\citep{jaques2019way,wu2019behavior}, fisher divergence~\citep{kostrikov2021offline}, MMD distance~\citep{kumar2019stabilizing} or $D_{\text{CQL}}$ from conservative Q-values~\citep{kumar2020conservative}) between the learned policy $\pi$ and the effective behavior policy $\pi_\beta^\mathrm{eff}$. Note that conservative Q-values refer to the Q-value for a given policy corresponding to a modified reward function $r(\bs, \mathbf{a}) - \alpha \pi(\mathbf{a}|\bs) \cdot (\pi(\mathbf{a}|\bs) / \pi_\beta(\mathbf{a}|\bs) - 1)$, computed on the empirical MDP. We also note that Sharing All can be easily adapted to the single-task setting where there is only one target task with labeled data $\mathcal{D}_\text{L}$ and unlabeled prior data $\mathcal{D}_\text{U}$. While data sharing tends to show promising results, it requires the assumption of the access to the functional form of the reward function. We instead focus on the data sharing problem where we do not make such an assumption and instead, only have the reward labels for originally commanded task, which we will discuss in the following section.


