\section{Related Work}
\label{sec:related}



Offline RL algorithms~\citep{ernst2005tree, riedmiller2005neural, LangeGR12, levine2020offline} address the problem of learning a policy from a static dataset without any interaction with the environment. 
%This formulation can be more practical in real-world applications, where online data collection can be expensive (e.g. in robotics, NLP and healthcare) and dangerous (e.g. in healthcare and autonomous driving).
%%CF: Why do we need this sentence above? Seems redundant with the intro and doesn't have any citations. I'm going to comment it out.
Recent works have shown successes in applying offline RL to real-world robotic manipulation  with high-dimensional visual inputs~\citep{ kalashnikov2018scalable, mandlekar2020iris, Rafailov2020LOMPO,singh2020cog}, NLP~\citep{jaques2019way,jaques2020human} and healthcare~\citep{shortreed2011informing, Wang2018SupervisedRL}.
%%SL.1.23: These citations are a bit weird, not clear how they support the statement. I also wouldn't suggest putting forward "sample complexity" as the main benefit of offline RL. There is a somewhat nuanced argument for why that is true, but I suspect many reviewers won't quite get it.
%%TY.1.26: I removed the citations above and added some examples with references to real-world robotics and NLP applications. Are there good references that can back up the other examples?
%%CF: I removed Pinto et al because it's not doing RL, just classification. That said, many of the citations above are conflicted with us / are self-cites.
%%TY.2.3: I added Mandlekar et al. to the list.
We now briefly discuss two main categories of offline RL methods and compare those works with our approach.

%%SL.1.23: would be good to cite older batch RL work (see my tutorial article), as well as add some citations to more applied work that uses offline/batch RL.
%%TY.1.26: I added those citations in the above paragraph.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.215\textwidth]{walker_task.png}
    \includegraphics[width=0.215\textwidth]{dooropen_task.png}
    \vspace{-0.2cm}
    \caption{Visualization of our image-based test environments. Observations consist of $64\times 64$ and $128\times 128$ raw pixel images for the \texttt{walker-walk} and \texttt{sawyer-door} tasks respectively. The \texttt{sawyer-door-close} environment used in our generalization experiments in Section~\ref{sec:generalization_exps} also uses the same environment as \texttt{sawyer-door}.}
    \label{fig:visual}
    \vspace{-0.7cm}
\end{figure}



\textbf{Model-free offline RL.} As noted in prior works, model-free offline RL methods suffer from the issue of action distribution shift~\citep{kumar2019stabilizing, levine2020offline}.
To tackle this challenge, prior model-free offline RL algorithms are designed to regularize the learned policy to be ``close`` to the behavioral policy either implicitly via regularized variants of importance sampling based algorithms~\citep{precup2001off, sutton2016emphatic, LiuSAB19, SwaminathanJ15, nachum2019algaedice}, offline actor-critic methods~\citep{siegel2020keep, peng2019advantage}, applying uncertainty quantification to the predictions of the Q-values~\citep{agarwal2020optimistic, kumar2019stabilizing, wu2019behavior, levine2020offline}, and learning conservative Q-values~\citep{kumar2020conservative} or explicitly measured by direct action constraints~\cite{fujimoto2018off},
KL divergence~\citep{jaques2019way,wu2019behavior, zhou2020plas}, Wasserstein distance, and MMD~\citep{kumar2019stabilizing}. 
%%CF: This paper is also missing: https://arxiv.org/abs/2011.07213
%%TY.2.3: added it to the family of using KL as the policy constraint.
While these methods provide a number of ways to regularize the policy or value function to mitigate distributional shift, they typically require the learned policy to stay close to the data~\citep{levine2020offline} and could be too conservative when the dataset is diverse and contains substantial suboptimal examples.
%%SL.1.23: This seems like it's tarring a wide range of prior work with a very broad brush... maybe we can tone this down a bit. I think reviewers are likely to object to some of these statements and we wouldn't really have the evidence to back it up
%%TY.1.26: I toned it down a bit.
%%CF: I think it's still pretty aggressive. Can you instead be positive about the potential of our approach in contrast to these? (e.g. Unlike these prior methods, we ...")
%%SL.1.31: I actually think this discuss more generally is hard to parse -- but I also don't think we entirely need it. I would suggest just deleting the above sentence, and replacing it instead with a discussion of conservatism. Maybe Aviral can suggest some citations, but in terms of phrasing, we can say something like this: While these methods provide a number of ways to regularize the policy or value function to mitigate distributional shift, they typically require the learned policy to stay close to the data.
%%TY.2.3: I took Sergey's suggested phrasing with references to the offline RL tutorial (Aviral can correct this if it's not appropriate) and added one more sentence to emphasize that model-free methods do not optimize generalization abilities.
Different from the prior model-free algorithms, COMBO leverages a learned dynamics model, which enables us to augment the dataset with model rollouts and allows the method to behave less conservatively in the setting mentioned above both theoretically (under regularity conditions) as shown in Section~\ref{sec:theory} and also empirically as shown in Section~\ref{sec:generalization_exps}.
%%SL.1.31: I'm torn about this conservatism thing. I do believe it's true, but I'm not sure we have evidence to back this up. In the absence of evidence, we could consider toning down these conservatism claims. The reason I'm concerned about it is that it's very easy for a reviewer to say: all this model-based stuff relies on the data just like model-free, so there is no reason to expect it to generalize better.
%%TY.2.3: I think our theory section shows that COMBO achieves a tighter lower bound and thus is less conservative and the generalization experiments also show that COMBO generalizes better than CQL. I included references to those two sections.

\textbf{Model-based offline RL.} Model-based offline RL methods~\citep{finn2017deep, ebert2018visual, kahn2018composable, kidambi2020morel, yu2020mopo, matsushima2020deployment, argenson2020model, swazinna2020overcoming,Rafailov2020LOMPO, lee2021representation} provide an alternative approach, leveraging supervised learning when learning the dynamics model and allowing the methods to utilize datasets with diverse behaviors more effectively.
%%SL.1.23: I don't think this is a statement that we can really back up, it's not clear why (or even if) model-based methods would be any better for generalizing beyond the support of the data. It would be nice to work in some older citations too (see the offline RL tutorial), otherwise it kind of creates the impression that running model-based RL on logged data was invented in 2020, which it most definitely wasn't. 
%%TY.1.26: I changed the benefit of MBRL to the advantage of using supervised learning for learning the models. Also added some older works.
%%CF: This still comes across as excessively positive about offline model-based method. I changed the phrasing a bit, but I still agree with Sergey that the last part is not something that we can back up.
%%TY.2.3: I think the current argument that MBRL can better utilize diverse datasets leveraging the power of supervised learning is intuitive and reasonable? We could even cite MOPO to show this, but there might be better citations.
%%CF: Also, the Oh et al and the Kaiser et al papers don't do offline learning. Oh et al doesn't do RL at all, while Kaiser et al is off-policy. I removed them. Can you work in the older citations that Sergey mentioned?
%%TY.2.3: I added Greg's off-policy robot navigation paper to the list.
%%CF: There are also other offline MBRL papers that you are missing in this paragraph. For example, there's the model-based offline planning paper from DeepMind, the LOMPO paper, and I think there are more papers. You can check for papers that cite MOPO. 
%%TY.2.3: added more citations.
However, model-based approaches suffer from distributional shift between the states generated by the learned model and the ground truth states. To mitigate this distributional shift, recent offline model-based works have used models to optimize policies in a pessimistic~\citep{kidambi2020morel} or uncertainty-penalized~\citep{yu2020mopo, Rafailov2020LOMPO, lee2021representation} MDP, or regularize the policy used to generate model rollouts to stay ``close`` to the behavioral policy~\citep{matsushima2020deployment, swazinna2020overcoming}. However, these methods either require manually designed uncertainty metrics, such as disagreement of model predictions across an ensemble of models~\citep{chua2018deep}
%%CF: Chua is not an offline RL method.
%%TY.2.3: I cited Chua et al. for the ensemble part.
for measuring pessimism~\citep{kidambi2020morel, yu2020mopo}, which could be inaccurate and computationally expensive, or explicitly constrain the learned policy similar to model-free offline RL approaches~\citep{matsushima2020deployment}.
%%CF: The last part of the sentence also seems repetitive with the description of the method above.
%%TY.2.3: I removed the "over-conservatism" part.
In contrast, our method explicitly minimizes the value function on states generated by the model while maximizing the values on states from the offline dataset, removing the requirement of designing uncertainty heuristics in model-based offline RL, which we have discussed in Section~\ref{sec:combo}.
%%SL.1.23: Is the statement about being less conservative something we will be able to formally back up? If so, then this is fine. But if not, maybe it would be better in related work to take the perspective that we remove the need for ad-hoc uncertainty estimation heuristics in model-based offline RL, and leave out strong statements about how model-based compares to model-free beyond the empirical results?
%%TY.1.26: I think our theory should be able to show this.
%%CF: I feel like it would be better to break this sentence into two and put it at the end of each of the two main paragraphs
%%TY.2.3: I removed the model-free part since I think we already discussed this at the end of the model-free paragraph.