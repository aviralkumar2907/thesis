\documentclass[../thesis.tex]{subfiles}

\usepackage{wrapfig}
\usepackage{cite}
% \usepackage{natbib}
% \usepackage[round]{natbib}

\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{amssymb}
\usetikzlibrary{positioning}
\usepackage{mathtools}
\usepackage{fleqn, tabularx}
\usepackage{multirow}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
% \newtheorem{corollary}{Corollary}[proposition]
\newtheorem{definition1}{Definition}[section]

% \include{chapters/cql/defs}
\definecolor{Gray}{gray}{0.9}

\makeatletter
\newcommand{\mybox}{%
    \collectbox{%
        \setlength{\fboxsep}{1pt}%
        \fbox{\BOXCONTENT}%
    }%
}
\makeatother
\newcommand{\CC}{\cellcolor{Gray}}

\newcommand{\rebuttal}[1] {{\color{black} #1}}
\newcommand{\final}[1] {{\color{black} #1}}


\begin{document}


% \blfootnote{This chapter is based on \cite{yu2021conervative}, published at NeurIPS 2020, which is joint work with Aurick Zhou, George Tucker, and Sergey Levine.}

\vspace{-0.4cm}
\begin{AIbox}{\large{\textbf{Abstract}}}
\vspace{4mm}
A natural use case of offline RL is in settings where we can pool large amounts of data collected in various scenarios for solving different tasks, and utilize all of this data to learn behaviors for all the tasks more effectively rather than training each one in isolation. We presented one approach for this in Chapter ??, via \emph{parameter sharing}, i.e., training a single multi-task policy and value function for solving all scenarios or tasks together. A complementary question in multi-task RL is that of \emph{data sharing}: when multiple tasks in a multi-task offline RL problem share the underlying dynamics, can we use data corresponding to one task to improve performance on the other? It has been noted that data sharing performs surprisingly poorly in practice. In this chapter, we analyze data sharing empirically and find that sharing data can actually exacerbate the distributional shift between the learned policy and the dataset, which in turn can lead to divergence of the learned policy and poor performance.

~

To address this challenge, we develop a simple technique for data-sharing in multi-task offline RL that routes data based on the improvement over the task-specific data. We call this approach conservative data sharing (\cdsmethodname), and show it can be applied with multiple single-task offline RL methods. We further extend \cdsmethodname\ to the setting when the reward labels for different tasks cannot be computed during rerouting and show that it is nonetheless effective in utilizing data across tasks. 
\vspace{2mm}
\end{AIbox}


% Offline reinforcement learning (RL) algorithms have shown promising results in domains where abundant pre-collected data is available. However, prior methods focus on solving individual problems from scratch with an offline dataset without considering how an offline RL agent can acquire multiple skills. \arxiv{We argue that a natural use case of offline RL is in settings where we can pool large amounts of data collected in various scenarios for solving different tasks, and utilize all of this data to learn behaviors for all the tasks more effectively rather than training each one in isolation. However, sharing data across all tasks in multi-task offline RL performs surprisingly poorly in practice. Thorough empirical analysis, we find that sharing data can actually exacerbate the distributional shift between the learned policy and the dataset, which in turn can lead to divergence of the learned policy and poor performance.} To address this challenge, we develop a simple technique for data-sharing in multi-task offline RL that routes data based on the improvement over the task-specific data. We call this approach conservative data sharing (\cdsmethodname), and it can be applied with multiple single-task offline RL methods. On a range of challenging multi-task locomotion, navigation, and vision-based robotic manipulation problems, \cdsmethodname\ achieves the best or comparable performance compared to prior offline multi-task RL methods and previous data sharing approaches.  

\vspace{-0.2cm}
\section{Introduction}
\vspace{-0.2cm}

Many realistic settings where we might want to apply offline RL are inherently \emph{multi-task} problems, where we want to solve multiple tasks using all of the data available. For example, if our goal is to enable robots to acquire a range of different behaviors, it is more practical to collect a modest amount of data for each desired behavior, resulting in a large but heterogeneous dataset, rather than requiring a large dataset for every individual skill. Indeed, many existing datasets in robotics~\citep{finn2017deep,dasari2020robonet,sharma2018multiple} and offline RL~\citep{fu2020d4rl} include data collected in precisely this way. Unfortunately, leveraging such heterogeneous datasets leaves us with two unenviable choices. We could train each task only on data collected for that task, but such small datasets may be inadequate for good performance. Alternatively, we could combine all of the data together and use data relabeled from other tasks to improve offline training, but this presents two crucial challenges.

First, utilizing relabeled data from other tasks into standard offline RL algorithms would require labeling large datasets with rewards, which is costly especially if these rewards must be provided by human labelers. Second, even if the functional form of the reward function for the task of interest were known beforehand, and data from other tasks were labeled with accurate reward annotations, na\"{i}vely using all of this data for training can actually often degrade performance over simple single-task training in practice~\citep{kalashnikov2021mt}. To address these issues, in this chapter, we aim to study data sharing with and without reward annotations. Concretely, we aim to understand how data sharing affects RL performance in the offline RL setting and develop a reliable and effective method for selectively sharing data across tasks (Section~\ref{sec:cds_section}). Then, we aim to address the problem with unlabeled data and develop an approach which still allows us to benefit from diverse, unlabeled datasets, even when reward annotations cannot be inferred correctly (Section~\ref{sec:uds_section}). 

% Moreover, it is often unclear how to even use all of this data together if the reward annotations (i.e., the supervision signal) is known beforehand 


% A number of prior works have studied multi-task RL in the \emph{online} setting, confirming that multi-tasking can often lead to performance that is worse than training tasks individually~\cite{parisotto2015actor,rusu2015policy,yu2020metaworld}. These prior works focus on mitigating optimization challenges that are aggravated by the online data generation process~\cite{schaul2019ray,yu2020gradient,yang2020multi}. As we will find in Section~\ref{sec:analysis}, multi-task RL remains a challenging problem in the offline setting when sharing data across tasks, even when exploration is not an issue. While prior works have developed heuristic methods for reweighting and relabeling data~\citep{andrychowicz2017hindsight,eysenbach2020rewriting, li2020generalized,kalashnikov2021mt}, they do not yet provide a principled explanation for why data sharing can hurt performance in the offline setting, nor do they provide a robust and general approach for selective data sharing that alleviates these issues while preserving the efficiency benefits of sharing experience across tasks.

\iffalse

In this paper, we hypothesize that data sharing can be harmful or brittle in the offline setting because it can exacerbate the distribution shift between the policy represented in the data and the policy being learned. 
We analyze the effect of data sharing in the offline multi-task RL setting, and present evidence to support this hypothesis.
Based on this analysis, we then propose an approach for selective data sharing that aims to minimize distributional shift, by sharing only data that is particularly relevant to each task. Instantiating a method based on this principle requires some care, since we do not know a priori which data is most relevant for a given task before we've learned a good policy for that task.
\arxiv{To provide a practical instantiation, we propose the conservative data sharing (CDS) algorithm. CDS reduces distributional shift by sharing data based on a learned conservative estimate of the Q-values that penalizes Q-values on out-of-distribution actions. Specifically, CDS relabels transitions when the conservative Q-value of the added transitions exceeds the expected conservative Q-values on the target task data. We visualize how \cdsmethodname\ works in Figure~\ref{fig:teaser}.}

\fi


The primary contributions of Section~\ref{sec:cds_section} are an analysis of data sharing in offline multi-task RL and a new algorithm, \textit{conservative data sharing} (\cdsmethodname), for multi-task offline RL problems. {\cdsmethodname\ relabels a transition into a given task only when it is expected to improve performance based on a conservative estimate of the Q-function.} 
After data sharing, similarly to prior offline RL methods, \cdsmethodname\ applies a standard conservative offline RL algorithm, such as CQL~\citep{kumar2020conservative} or BRAC~\citep{wu2019behavior}, a policy-constraint offline RL algorithm. Further, we theoretically analyze \cdsmethodname\ and characterize scenarios under which it provides safe policy improvement guarantees. Finally, we conduct extensive empirical analysis of \cdsmethodname\ on multi-task locomotion, multi-task robotic manipulation with sparse rewards, multi-task navigation, and multi-task imaged-based robotic manipulation. We compare CDS to vanilla offline multi-task RL without sharing data, to na\"{i}vely sharing data for all tasks, and to existing data relabeling schemes for multi-task RL. \cdsmethodname\ is the only method to attain good performance across all of these benchmarks, often significantly outperforming the best \textit{domain-specific} method, improving over the next best method on each domain by \textbf{17.5\%} on average.

The primary contribution of Section~\ref{sec:uds_section} is the demonstration that simply labeling unlabeled data with a reward of zero for use in multi-task offline RL is surprisingly effective in many cases. We perform extensive theoretical and empirical analysis to study conditions under which this simple approach, \emph{unlabeled data sharing} (UDS), would either excel or fail, and we study how reweighting the relabeled data with CDS (while still using zero reward as the label) can substantially increase the range of settings when UDS is successful. Our empirical evaluation, conducted over various multi-task offline RL scenarios such as locomotion, robotic manipulation from visual inputs, and ant-maze navigation shows that this simple zero reward strategy leads to improved performance, especially when combined with the CDS approach, even compared to methods that use more sophisticated learning and label propagation strategies to infer rewards. Overall, we demonstrate that an effective way to perform data sharing with unlabeled multi-task datasets in offline RL is to use a combination of CDS and UDS approaches.



% Offline reinforcement learning (RL) can learn control policies from static datasets but, like standard RL methods, it requires reward annotations for every transition. In many cases, labeling large datasets with rewards may be costly, especially if those rewards must be provided by human labelers, while collecting diverse unlabeled data might be comparatively inexpensive. How can we best leverage such unlabeled data in offline RL? One natural solution is to learn a reward function from the labeled data and use it to label the unlabeled data. In this paper, we find that, perhaps surprisingly, a much simpler method that simply applies zero rewards to unlabeled data leads to effective data sharing both in theory and in practice, without learning any reward model at all. While this approach might seem strange (and incorrect) at first, we provide extensive theoretical and empirical analysis that illustrates how it trades off reward bias, sample complexity and distributional shift, often leading to good results. We characterize conditions under which this simple strategy is effective, and further show that extending it with a simple reweighting approach can further alleviate the bias introduced by using incorrect reward labels. Our empirical evaluation confirms these findings in simulated robotic locomotion, navigation, and manipulation settings.

\section{Data Sharing for Multi-Task Offline Reinforcement Learning}    
\label{sec:cds_section}


% \input{chapters/cds/intro}

\input{chapters/cds/related_work}

% \input{chapters/cds/prelim}

\input{chapters/cds/method}

\input{chapters/cds/experiments_new}
    
% \section{Discussion and Perspectives}
% \label{sec:conclusion}
% In this paper, we study the multi-task offline RL setting, focusing on the problem of sharing offline data across tasks for better multi-task learning. Through empirical analysis, we identify that na\"{i}vely sharing data across tasks generally helps learning but can significantly hurt performance in scenarios where excessive distribution shift is introduced. To address this challenge, we present conservative data sharing (CDS), which relabels data to a task when the conservative Q-value of the given transition is better than the expected conservative Q-value of the target task. On multitask locomotion, manipulation, navigation, and vision-based manipulation domains, CDS consistently outperforms or achieves comparable performance to existing data sharing approaches. While CDS attains superior results, it is not able to handle data sharing in settings where dynamics vary across tasks and requires functional forms of rewards. We leave these as future work. 
% Another limitation of CDS is that it requires access to the functional form of the reward. Though this is a common assumption as discussed in Section~\ref{sec:prelims}, an interesting future direction would be removing this dependence and replacing it with either a learned reward predictor or other supervision signal like language instructions.
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       UDS      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multi-Task Offline RL With Unlabeled Data}
\label{sec:uds_section}

% \section{Introduction}
% Offline reinforcement learning (RL) provides the promise of a fully data-driven framework for learning performant policies. To avoid costly active data collection and exploration, offline RL methods utilize a previously collected dataset to extract the best possible behavior, making it feasible to use RL to solve real-world problems where active exploration is expensive, dangerous, or otherwise infeasible~\citep{zhan2021deepthermal, de2021discovering, Wang2018SupervisedRL, kalashnikov2018scalable}. 
% However, this concept is only viable when a significant amount of data for the target task is available in advance. A more realistic scenario might allow for a much smaller amount of task-specific data, combined with a large amount of task-agnostic data, that is not labeled with task rewards and some of which may not be relevant. For example, if our goal is to train a robot to perform a new manipulation task (e.g., cutting an onion), we might have some data of the robot (suboptimally) attempting that task, perhaps collected under human teleoperation and manually labeled with rewards, combined with background data, some of which might be structurally related (e.g., picking up an onion, or cutting a carrot). {This scenario presents several questions: How do we decide which prior data should be included when learning the new task? And how do we determine reward labels to use for this prior data?}

% Prior methods have attempted to answer these two questions, typically in isolation. For the first question, it has been noted that a na\"{i}ve strategy of sharing all of the prior data can be highly suboptimal~\citep{kalashnikov2021mt}, even when it is annotated with reward labels, and some works have proposed both manual~\citep{kalashnikov2021mt} and automated~\citep{yu2021conservative,eysenbach2020rewriting} data sharing strategies that prioritize the most structurally similar prior data. 
% However, these methods assume that shared data can automatically be relabeled with the reward function for the task,
% but the assumption that we have access to the functional form of this reward is a strong one: for example, in many real-world settings, computing the reward might require human labeling~\citep{cabi2019scaling,finn2016deep}. 
% Prior works use learned classifiers for reward labeling~\citep{VICEFu2018,xie2018few,singh2019end}, or other automated mechanisms~\citep{konyushkova2020semi}. But these mechanisms themselves add complexity and potential brittleness to the pipeline. Thus, we aim to study the possibility of tackling this problem with a simple method that determines which rewards to use for shared data, with minimal supervision and no additional modeling and learning.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{chapters/uds/teaser_new.png}
    \vspace{-0.6cm}
    \caption{\footnotesize  Overview of various methods dealing with unlabeled offline data. UDS is a simple method compared to the complex reward learning approaches yet achieves effective results.}
    \vspace{-0.55cm}
    \label{fig:uds_teaser}
\end{figure}

% In this paper, we make the potentially surprising observation that prior data such as data from other tasks can be utilized with na\"{i}ve constant reward labels in offline RL, which corresponds to zero reward or whatever is the minimum reward for the task (which we can set to zero via rescaling, without loss of generality). It may at first seem that such an approach would lead to incorrect solutions due to using the wrong reward values. However, we show that this simple reward relabeling strategy, which we refer to as unlabeled data sharing (UDS), can outperform more sophisticated methods that separately learn a reward model and lead to good results in a range of settings. We visualize UDS along with more complex methods in Figure~\ref{fig:uds_teaser}. Na\"{i}ve UDS does not work in all cases, but we further show that carefully reweighting the unlabeled transitions (to modify their distribution) can significantly increase the applicability of this approach and reduce the effects of reward bias, while still preserving many of the benefits. {We analyze this theoretically and show that, in practice, a method based on conservative data sharing~\citep{yu2021conservative}, which reweights unlabeled data to minimize distributional shift (originally designed for use with ground truth reward labels) can be particularly effective in this role.}

% Our main contribution is the finding that simply labeling unlabeled data with a reward of zero for use in offline RL is surprisingly effective in many cases. We perform extensive theoretical and empirical analysis to study conditions under which this simple approach, UDS, would either excel or fail, and we analyze how reweighting the relabeled data (while still using zero reward as the label) can increase the range of settings when UDS is successful. Our empirical evaluation, conducted over various single-task and multi-task offline RL scenarios such as locomotion, robotic manipulation from visual inputs, and ant-maze navigation shows that this simple zero reward strategy leads to improved performance, even compared to methods that use more sophisticated learning and label propagation strategies to infer rewards. This further supports our hypothesis that relabeling unlabeled data with zero reward is an effective approach for utilizing unlabeled prior data in offline RL.
    
    
\input{chapters/uds/related_work}

% \input{chapters/uds/prelim}

\input{chapters/uds/method}

\input{chapters/uds/experiments}
    
\section{Discussion and Limitations}

In this chapter, we studied the problem of \emph{data sharing} in multi-task offline RL, when presented with diverse, unlabeled data. We developed approaches to tackle two facets of this problem: (i) learning to selectively reweight multi-task data to guarantee a performance improvement, and (ii) learning in the presence of no reward annotations. Our approaches CDS and UDS are theoretically-motivated and lead to significant boost in performance in several multi-task problem scenarios, where we must learn from diverse datasets. We provide a theoretical and empirical analysis in simplified setups to understand the conditions under which our approaches can work and find that, in general, when reward annotations are not present, combining CDS and UDS together can be performant. We believe that this chapter justifies the effectiveness of simple methods for using unlabeled and multi-task data in offline RL and shed light on directions for future work that can better control the reward bias and enjoy better policy performance.    

While CDS and UDS provide an initial way to incorporate diverse, multi-task datasets in offline RL, these approaches do not tackle several other challenges that often come with real datasets. For instance, a running assumption in this chapter is that the state and action spaces for transitions in the task-specific data and the diverse data are identical, but this assumption is not specified in many practical problems, an example of which is incorporating diverse human videos for downstream robotic control. Theoretically, takeaways from our analyis can be improved to show policy optimality guarantees. We leave these as avenues for future work.       

% leveraging unlabeled data in offline RL where we find that a simple method UDS that relabels unlabeled data with zero rewards is surprisingly effective in various single-task and multi-task offline RL domains. We provide both theoretical and empirical analysis of UDS to study conditions where it works and does not work. Furthermore, we show that by utilizing the optimized reweighting strategy, reward bias introduced in UDS can be reduced and the policy performance bound is improved in our theoretical analysis, which is also verified empirically. We believe that our analysis justifies the effectiveness of such simple methods for using unlabeled data in offline RL and shed light on directions for future work that can better control the reward bias and enjoy better policy performance.
    
    
\section*{Acknowledgements}
We thank Kanishka Rao, Julian Ibarz, Xinyang Geng, Avi Singh, members of IRIS at Stanford, RAIL at UC Berkeley and Robotics at Google and Google Research for valuable and constructive feedback on an early version of this manuscript. This research was funded in part by Google, ONR grants N00014-21-1-2685, N00014-20-1-2675, and N00014-21-1-2838, Intel Corporation and the DARPA Assured Autonomy Program. CF is a CIFAR Fellow in the Learning in Machines and Brains program. 



\end{document}
