
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\input{iclr2023/defs}

\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage[skins,theorems]{tcolorbox}

\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{url}
\usepackage{wrapfig}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor}         % colors
% \usepackage{fleqn, tabularx}
\usepackage{multirow}
% \usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsthm}
% \newtheorem{proposition}{Proposition}[section]
% \newtheorem{lemma}{Lemma}[section]
% \usepackage{amsfonts} 
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
morekeywords={self, clip, exp, mse_loss, uniform_sample, concatenate, logsumexp},              % Add keywords here
keywordstyle=\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=single,                         % Any extra options here
showstringspaces=false
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\title{Offline Q-Learning on Diverse Multi-Task\\ Data Both Scales And Generalizes}

\iclrfinalcopy

\author{
\qquad \qquad \qquad {\qquad \qquad Aviral Kumar$^{1, 2}$ ~~~ Rishabh Agarwal$^{1}$}  \vspace{0.1cm}\\
 \qquad \quad ~~~~~~~~~~~~ \textbf{Xinyang Geng$^{2}$ ~~~ George Tucker$^{*, 1}$~~~ Sergey Levine$^{*, 1, 2}$}\vspace{0.1cm}\\
\qquad \qquad \qquad ~~~~ {$^1$ Google Research, Brain Team ~~~ $^2$ UC Berkeley} ~~~~~~~ \vspace{0.1cm}\\
\qquad ~~~~~\small{{\{aviralk, young.geng, svlevine\}@eecs.berkeley.edu, \{rishabhagarwal, gjt\}@google.com}}}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
\blfootnote{$^*$ Co-senior authors.}
The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: \emph{ResNets,
cross-entropy based distributional backups, and feature normalization}, offline Q-learning algorithms exhibit strong performance that scales with model capacity.
Using multi-task Atari as a test-bed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly suboptimal dataset~(51\% human-level performance).
Compared to return-conditioned supervised approaches, offline Q-learning scales similarly with model capacity and has better performance, especially when the dataset is suboptimal. Finally, we show that offline Q-learning with a diverse dataset is sufficient to learn powerful representations that facilitate rapid transfer to novel games and fast online learning on new variations of a training game, improving over existing state-of-the-art representation learning approaches.  
\end{abstract}

\input{iclr2023/intro}

\input{iclr2023/related_work}

\input{iclr2023/prelim}

\input{iclr2023/method}

\input{iclr2023/experiments}


\input{iclr2023/discussion}

% \subsubsection*{Acknowledgments}


\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage

\appendix

\input{appendix}

\end{document}
