\vspace{-0.2cm}
% \subsection{Problem Setup}
% \label{sec:prelims}
% \vspace{-0.2cm}

% We consider sequential-decision making problems~\citep{SuttonBook} where on each timestep, an agent observes a state $\bs$, produces an action $\ba$, and receives a reward $r$. The goal of a learning algorithm is to maximize the sum of discounted rewards. Our approach is based on conservative Q-learning (CQL)~\citep{kumar2020conservative}, an offline Q-learning algorithm. CQL uses a sum of two loss functions to combat value overestimation on unseen actions: \textbf{(i)} standard TD-error that enforces Bellman consistency, and \textbf{(ii)} a regularizer that minimizes the Q-values for unseen actions at a given state, while maximizing the Q-value at the dataset action to counteract excessive underestimation. Denoting $Q_\theta(\bs, \ba)$ as the learned Q-function, the training objective for CQL is given by:
% \begin{align}
% \label{eqn:cql_training}
%     \min_{\theta}~ \alpha\!\left( \mathbb{E}_{\bs \sim \mathcal{D}} \left[\log \left(\sum_{\ba'} \exp(Q_\theta(\bs, \ba')) \right) \right]\! -\! \mathbb{E}_{\bs, \ba \sim \mathcal{D}}\left[Q_\theta(\bs, \ba)\right]  \right) + \mathsf{TDError}(\theta; \mathcal{D}),
% \end{align}
% where $\alpha$ is the regularizer weight, which we fix to $\alpha=0.05$ based on preliminary experiments unless noted otherwise. \citet{kumar2020conservative} utilized a distributional $\mathsf{TDError}(\theta; \mathcal{D})$ from C51~\citep{bellemare2017distributional}, whereas \citep{kumar2021dr3} showed that similar results could be attained with the standard mean-squared TD-error. \citet{lee2022multi} use the distributional formulation of CQL and found that it underperforms alternatives and performance does not improve with model capacity. In general, there is no consensus on which formulation of TD-error must be utilized in Equation~\ref{eqn:cql_training}, and we will study this choice in our scaling experiments.


Our goal in this section is to learn a single policy that is effective at multiple Atari games and can be fine-tuned to new games. For training, we utilize the set of 40 Atari games used by \citet{lee2022multi}, and for each game, we utilize the experience collected in the DQN-Replay dataset~\citep{agarwal2019optimistic} as our offline dataset. We consider two different dataset compositions: 
% \vspace{-0.1cm}
\begin{enumerate}
    \item \textbf{Sub-optimal} dataset consisting of the initial 20\% of the trajectories (10M transitions) from DQN-Replay for each game, containing 400 million transitions overall with average human-normalized interquartile-mean~(IQM)~\citep{agarwal2021deep} score of 51\%. Since this dataset does not contain optimal trajectories, we do not expect methods that simply copy behaviors in this dataset to perform well. Instead, we would expect methods that can combine useful segments of sub-optimal trajectories to perform well. 
    \item  \textbf{Near-optimal} dataset, used by \citet{lee2022multi}, consisting of all the experience~(50M transitions) encountered during training of a DQN agent including human-level trajectories, containing 2 billion transitions with average human-normalized IQM score of 93.5\%. 
\end{enumerate}

\textbf{Evaluation}. We evaluate our method in a variety of settings as we discuss in our experiments in Section~\ref{sec:scaling_exps}. Due to excessive computational requirements of running huge models, we are only able to run our main experiments with one seed. Prior work~\citep{lee2022multi} that also studied offline multi-game Atari evaluated models with only one seed. That said, to ensure that our evaluations are reliable, for reporting performance, we follow the recommendations by \citet{agarwal2021deep}. Specifically, we report interquartile mean~(IQM) normalized scores, which is the average scores across middle 50\% of the games, as well as performance profiles for qualitative summarization. 

