\vspace{-0.2cm}
\subsection{Related Work}
\vspace{-0.2cm}

Prior works have sought to train a single generalist policy to play multiple Atari games simultaneously from environment interactions, either using off-policy RL with online data collection~\citep{espeholt2018impala, hessel2019multi, song2019v}, or policy distillation~\citep{teh2017distral, rusu2015policy} from single-task policies. While our work also focuses on learning such a generalist multi-task policy, it investigates whether we can do so by scaling offline Q-learning on suboptimal offline data, analogous to how supervised learning can be scaled to large, diverse datasets. Furthermore, prior attempts to apply transfer learning using RL-learned policies in ALE~\citep{rusu2015policy, parisotto2015actor, mittel2019visual} are restricted to a dozen games that tend to be similar and generally require an “expert”, instead of learning how to play all games concurrently. 

Closely related to our work, recent work train Transformers~\citep{vaswani2017attention} on purely offline data for learning such a generalist policy using supervised learning~(SL) approaches, namely, behavioral cloning~\citep{reed2022generalist} or return-conditioned behavioral cloning~\citep{lee2022multi}. While these works focus on large datasets containing expert or near-human performance trajectories, our work focuses on the regime when we only have access to highly diverse but sub-optimal datasets. We find that these SL approaches perform poorly with such datasets, while offline Q-learning is able to substantially extrapolate beyond dataset performance~(Figure~\ref{fig:suboptimal_offline}). Even with near-optimal data, we observe that scaling up offline Q-learning outperforms  SL approaches with 200 million parameters using as few as half the number of network parameters~(Figure~\ref{fig:scaling}).


% There has been a recent surge of offline RL algorithms that focus on mitigating distribution shift in single task settings~\citep{fujimoto2018off,kumar2019stabilizing,liu2020provably,wu2019behavior,fujimoto2021minimalist, siegel2020keep,peng2019advantage,nair2020accelerating, LiuSAB19, SwaminathanJ15, nachum2019algaedice, kumar2020conservative,kostrikov2021offline,kidambi2020morel,yu2020mopo,yu2021combo}. Complementary to such work, our work investigates scaling offline RL on the more diverse and challenging multi-task Atari setting with data from 40 different games~\citep{agarwal2019optimistic, lee2022multi}. To do so, we use CQL~\citep{kumar2020conservative}, due to its simplicity as well as its efficacy on offline RL datasets with high-dimensional observations. 
